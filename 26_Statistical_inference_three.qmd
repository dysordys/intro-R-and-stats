# Statistical inference in qualitative variables {#sec-inference-qualitative}

The previous chapters have focused on quantitative variables, except for the proportion or binomial tests of a specific category in a qualitative variable. If we want to compare groups of a  qualitative variables we are in this part of the book limited in the methods available. 

## $\chi^2$-test for independence
The *$\chi^2$-test for independence* (pronounced chi-square) is used to determine if there is a significant association between two categorical variables. It compares the observed frequencies in each category to the frequencies we would expect if the variables were independent. For example, you might want to test whether the distribution of plant species is different across habitats or whether treatment groups in a study differ in their responses.

Formally we state the hypotheses as:
$$
\begin{aligned}
&H_0: \text{The variables are independent}\\
&H_1: \text{The variables are dependent}
\end{aligned}
$$

The process calculates the difference in observed frequencies with the expected frequencies under the null hypothesis.

$$
\begin{aligned}
\chi^2 = \sum_{\text{cells}} \frac{(O_i - E_i)^2}{E_i}
\end{aligned}
$$
where:

- $O_i$ is the observed frequency in cell $i$,
- $E_i$ is the expected frequency in cell $i$ under the assumption of independence. 

The expected frequency is calculated as:
$$
\begin{aligned}
E_i = \frac{(\text{row total}) \times (\text{column total})}{\text{grand total}}
\end{aligned}
$$

The assumptions for the test is that no more than 20% of the expected frequencies are lower than 5, and none are lower than 1. If this assumption does not hold, we cannot in earnest believe the test results. 

Suppose you are studying whether different plant species are distributed differently across two habitat types ("Forest" and "Grassland"). The observed counts are as follows:

- Species A: 30 in Forest, 20 in Grassland
- Species B: 25 in Forest, 15 in Grassland
- Species C: 10 in Forest, 5 in Grassland

You can create a contingency table and run the chi-square test:

```{r}
# Create a contingency table
observed <- matrix(c(30, 20, 
                     25, 15, 
                     10, 5), 
                   nrow = 3, byrow = TRUE)
rownames(observed) <- c("Species A", "Species B", "Species C")
colnames(observed) <- c("Forest", "Grassland")

# Run the chi-square test using the custom chi.sq() function
chi_result <- chisq.test(observed)

# Display the test results
print(chi_result)

```

The output shows a p-value which can be used to determine whether we can reject the $H_0$ or not.

## $\chi^2$-test for goodness-of-fit
The **$\chi^2$ goodness-of-fit test** is used to determine if a sample data matches a population with a specific distribution. It compares the observed frequencies in each category to the expected frequencies based on the specified distribution. 

$$
\begin{aligned}
&H_0: \text{X follows a binomial distribution with probability p}\\
&H_1: \text{X does not follow a binomial distribution with probability p}
\end{aligned}
$$

For example when using the binomial distribution as the null hypothesis, we are checking if a dichotomous outcome (such as “defective” vs. “non‐defective” or “success” vs. “failure”) occurs with a specified probability, $p$. We assume that in $n$ independent trials, the probability of success is given by $p$. Under this hypothesis, the expected number of successes and failures is computed as follows:

$$ \begin{aligned}
E_{\text{success}} = n \cdot \hat{p} \quad \text{and} \quad E_{\text{failure}} = n \cdot (1-\hat{p}).
\end{aligned} $$

Suppose we have observed counts $O_{\text{success}}$ and $O_{\text{failure}}$. The $\chi^2$ test statistic is then calculated by comparing these observed counts with the expected counts:

$$ \begin{aligned}
\chi^2 = \frac{(O_{\text{success}} - E_{\text{success}})^2}{E_{\text{success}}} + \frac{(O_{\text{failure}} - E_{\text{failure}})^2}{E_{\text{failure}}}.
\end{aligned} $$


Since we have two categories (success and failure), the degrees of freedom for this test is given by:

$$ \begin{aligned}
\text{df} = k - 1,
\end{aligned} $$


where \( k = 2 \). Therefore, in our scenario,

$$ \begin{aligned}
\text{df} = 2 - 1 = 1.
\end{aligned} $$

If the p-value associated with the computed $\chi^2$ statistic (evaluated using a chi-square distribution with 1 degree of freedom) is less than the chosen significance level (typically $\alpha = 0.05$), the null hypothesis is rejected. This result indicates that the observed frequencies significantly deviate from those expected under the assumed binomial distribution.

This use of the $\chi^2$-test is in practice very similar to the test for independence, but the main difference is what $H_0$ represents --- a specific distribution instead of independence --- and how the expected values are calculated.

## Exercises
The material to be analyzed in these exercises is a collection of phosphorus levels in 4189 lakes from different parts of Sweden. The phosphorus levels have been divided into five different condition classes. The data set can be found via [`lake-phosphorus.csv`](https://raw.githubusercontent.com/dysordys/intro-R-and-stats/main/data/lake-phosphorus.csv).

This task aims to analyze the collected data from the Swedish lakes. What we want to investigate is whether there is a dependency between condition class and region.

1. Formulate hypotheses that describe what you want to investigate and set the level of significance.

2. Show the expected values and check that the assumptions for the test has been fulfilled. 

3. Conduct the full test and interpret the results of the output. Does the data provide evidence that there is a dependency between the region of Sweden and the amount of phosphorus in the lakes?
