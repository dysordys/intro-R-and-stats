[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to data analysis and visualization with R",
    "section": "",
    "text": "Welcome\nDo you have any comments, suggestions for improvement, or errata? Feel free to send them to me at dysordys@protonmail.com."
  },
  {
    "objectID": "Intro_R_RStudio.html#overview",
    "href": "Intro_R_RStudio.html#overview",
    "title": "1  Introduction to R and RStudio",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nThis chapter introduces R and RStudio. R is a free and open-source programming language for statistics, graphing, and modeling, originally developed by statisticians. In recent years, R has become extremely popular among biologists, and you will almost certainly encounter it as part of real-world research projects. In this book we will be learning some of the ways in which R can be used for efficient data analysis and visualization.\nRStudio is an “integrated development environment” (IDE) for R, which means it is a software application that lets you write, run, and interact graphically with programs. RStudio integrates a text editor, the R console (where you run R commands), package management, plotting, help, and more."
  },
  {
    "objectID": "Intro_R_RStudio.html#installing-r-and-rstudio",
    "href": "Intro_R_RStudio.html#installing-r-and-rstudio",
    "title": "1  Introduction to R and RStudio",
    "section": "1.2 Installing R and RStudio",
    "text": "1.2 Installing R and RStudio\nYou can download the most up-to-date R distribution for free here:\nhttp://www.r-project.org\nRun the installer as directed and you should be set to go. We will not interact with the installed R application directly, but the R software components you install will be used by RStudio under the hood.\nTo install RStudio on your computer, download it from here:\nhttps://www.rstudio.com/products/rstudio/download/\nOn a Mac, open the downloaded disk image and drag the RStudio application into your Applications folder. On Windows, run the installer you downloaded."
  },
  {
    "objectID": "Intro_R_RStudio.html#getting-around-rstudio",
    "href": "Intro_R_RStudio.html#getting-around-rstudio",
    "title": "1  Introduction to R and RStudio",
    "section": "1.3 Getting Around RStudio",
    "text": "1.3 Getting Around RStudio\nRStudio should be available in the usual places: the Applications folder (on a Mac) or the Start menu (on Windows). When you start it up, you will see four sections of the screen. The most important items found in each are:\n\nUpper-left: an area for viewing and editing text files\nLower-left: Console, where you send commands to R\nUpper-right: Environment, for loading, saving, and examining data\nLower-right:\n\nFiles: a list of files in the “working directory” (more on this later)\nPlots: where the plots (graphs) you draw show up\nPackages: an area for managing installed R packages\nHelp: access to all the official documentation for R\n\n\n\n\n\nRStudio starting screen\n\n\n\n1.3.1 A Simple Calculation\nEven if you don’t know R, you can start by typing some simple calculations into the console. The &gt; symbol indicates that R is waiting for you to type something. Click on the console, type 2 + 2, and hit Enter (or Return on a Mac). You should see that R produces the right answer:\n\n&gt; 2 + 2\n[1] 4\n&gt;\n\nNow, press the Up arrow on the keyboard. You will notice that the 2 + 2 you typed before shows up. You can use the Up and Down arrows to go back and forth through past commands you have typed, which can save a lot of repetitive typing when you are trying things out. You can change the text in these historical commands: change the 2 to a 3 and press Enter (Return, on a Mac) again:\n\n&gt; 2 + 3\n[1] 5\n&gt;\n\n(The [1] at the beginning of the result just means that the following number is at position 1 of a vector. In this case, the vector only has one element, but when R needs to print out a long vector, it splits it into multiple lines tells you at the beginning of each line what position you are at.)\nBefore going deeper into R programming, we need to discuss a few things to enable you to get around R and RStudio more easily.\n\n\n1.3.2 Writing R scripts\nThe upper left part of RStudio is a simple text editor where you can write R code. But, instead of having to enter it one line at a time as we did in the console above, you can string long sequences of R instructions together that build on one another. You can save such a text file (Ctrl-S on Windows; Cmd-S on a Mac), giving it an appropriate name. It is then known as an R script, a text file containing R code that can be run from within R.\nAs an example, enter the following code. Do not worry about how or why it works just yet. It is a simple simulation and visualization of regulated (logistic) population growth:\n\ntime &lt;- 1:15\ngrowthFun &lt;- function(x, y, lambda = 1.8) lambda * x * (1 - x)\npop &lt;- Reduce(growthFun, rep(0.01, times = length(time)), accumulate = TRUE)\nplot(time, pop, xlab = \"time\", ylab = \"population density\", type = \"b\")\n\nAfter having typed this, highlight all lines. You can do this either with a mouse, or by pressing Ctrl-A on Windows / Cmd-A on a Mac, or by using the arrow keys while holding the Shift key down. Then, to send these instructions to R for processing, press Ctrl-Enter (Cmd-Return on a Mac). If all went well, the lower right screen section should have jumped to the Plots panel, showing the following graph:\n\n\n\n\n\n\n\n1.3.3 Setting the Working Directory\nWhen you ask R to run a program or load data from a file, it needs to know where to find the file. Unless you specify the complete path to the file on your machine, it assumes that file names are relative to what is called the “working directory”.\nThe first thing you should do when starting a project is to create a directory (folder) on your computer to store all the files related to the project, and then tell R to set the working directory to that location.\nThe R function setwd(\"/path/to/directory\") is used to set the working directory, where you substitute in the actual path and directory name in place of path/to/directory. In turn, getwd() tells you what the current working directory is. The path can be found using File Explorer on a Windows PC, but you will need to change backslashes to forward slashes (\\ to /). On a Mac, you can find the path by selecting a folder and choosing File &gt; Get Info (the path is under “Where:”).\nThere is also a convenient graphical way to set the working directory in RStudio. In the Files panel, you can navigate around the computer’s file space. You can do this either in the panel itself, or using the ellipsis (…) to bring up the system-standard file browser. Looking around there does not immediately set the working directory, but you can set it by clicking the “Session” menu point at the top menu bar, hovering over “Set Working Directory” with the mouse, and choosing “To Files Pane Location” from the list of sub-options when they appear.\n\n\n\n\n\n\nWarning\n\n\n\nIt is worth repeating: finding the appropriate directory in the Files panel is not enough. It will not set the working directory automatically. You need to actually click the “Session” menu and then click on “To Files Pane Location” under “Set Working Directory”.\n\n\nYou may have also noticed that you don’t actually need to type getwd(): the RStudio Console panel shows the current working directory below the word “Console”.\n\n\n1.3.4 Packages\nOne of the primary reasons ecologists use R is the availability of hundreds of free, user-contributed pieces of software, called packages. Packages are generally created by people who wanted to solve a particular problem for their own research and then realized that other people might find their code useful. Take a moment to browse the packages available on the main R site:\nhttp://cran.r-project.org/web/packages/\nTo install a package, you take its name, put it in quotes, and put it in between the parentheses of install.packages(). For example, to install the package tidyverse (which we will be relying on later), you type:\n\ninstall.packages(\"tidyverse\")\n\nand press Enter (Return, on a Mac). Note that some packages can take quite a while to install. If you are installing tidyverse for instance, it could take anywhere between five minutes to an hour (!) depending on your computer setup. This is normal, and the good news is that you only need to do this once on a computer. Once the package is installed, it will stick around.\n\n\n\n\n\n\nNote\n\n\n\nIt is possible for the installation of packages to fail. The most common reason is that the package relies on some external software (e.g., curl, ffmpeg, a C++ compiler, etc.) which is not installed on your computer. In such cases, make sure the required software is installed first, then try installing the package again.\n\n\nTo actually use a previously installed package in an R session, you need to load it from your disk directly into the computer’s memory. That can be done like this:\n\nlibrary(tidyverse)\n\nNote the lack of quotation marks when loading a package.\nRStudio also makes package management a bit easier. In the Packages panel (top line of lower right portion of the screen) you can see a list of all installed packages. You can also load and unload packages simply by checking a checkbox, and you can install new packages using a graphical interface (although you will still need to know the name of the package you want to install)."
  },
  {
    "objectID": "Intro_R_RStudio.html#additional-reading",
    "href": "Intro_R_RStudio.html#additional-reading",
    "title": "1  Introduction to R and RStudio",
    "section": "1.4 Additional Reading",
    "text": "1.4 Additional Reading\nR:\n\nR website\nCRAN package index\n\nRStudio:\n\nRStudio documentation"
  },
  {
    "objectID": "Intro_R_RStudio.html#sec-intro-exercises",
    "href": "Intro_R_RStudio.html#sec-intro-exercises",
    "title": "1  Introduction to R and RStudio",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\n\nCreate a folder named data-with-R on your computer and set it as the working directory in R. Make certain that the working directory has indeed been set properly.\nUse the RStudio file browser to set the working directory somewhere else on your hard drive, and then set it back to the data-with-R folder you created earlier. Make sure it is being set properly at each step.\nInstall an R package called vegan, using install.packages as discussed in Section 1.3.4. (The vegan package contains various utilities for community ecology.)\nLoad the vegan package invoking library(vegan). Afterwards, try unloading and then loading the vegan package again, using the Packages panel in RStudio this time.\nCreate a simple text file (you can do this via File \\(\\blacktriangleright\\) New File \\(\\blacktriangleright\\) Text File from the main menu bar at the top) and put the following in it:\nA dozen, a gross, and a score\nPlus three times the square root of four\n  Divided by seven\n  Plus five times eleven\nIs nine squared and not a bit more.\nNow save this file as limerick.txt in the data-with-R folder you created earlier.\nCreate a new R script file (File \\(\\blacktriangleright\\) New File \\(\\blacktriangleright\\) R Script) and enter the following:\n\nreadLines(\"limerick.txt\") |&gt;\n  paste(collapse = \"\\n\") |&gt;\n  writeLines()\n\nSave the file as read-limerick.R in the same folder (data-with-R). Run it and see what happens. If all goes well, it should display the contents of the file on the screen. (Hint: if you run into trouble, you most likely will need to set the working directory appropriately.)"
  },
  {
    "objectID": "R_programming_basics.html#using-r-as-a-calculator",
    "href": "R_programming_basics.html#using-r-as-a-calculator",
    "title": "2  R programming basics",
    "section": "2.1 Using R as a calculator",
    "text": "2.1 Using R as a calculator\nAs we have seen in Section 1.3.1, R can be used as a glorified pocket calculator. Elementary operations work as expected: + and - are symbols for addition and subtraction, while * and / are multiplication and division. Thus, we can enter things such as 3 * 4 - 6 / 2 + 1 n the console, and press Enter (Return, on a Mac) to get the result:\n\n3 * 4 - 6 / 2 + 1\n\n[1] 10\n\n\nOne even has exponentiation, denoted by the symbol ^. To raise 2 to the 5th power, we enter\n\n2^5\n\n[1] 32\n\n\nFurthermore, one is not restricted to integers. It is possible to calculate with fractional numbers:\n\n1.62 * 34.56\n\n[1] 55.9872\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn line with Anglo-Saxon tradition, R uses decimal points instead of commas. A common mistake for people coming from other traditions is to type 1,62 * 34,56. This will throw an error.\n\n\nR also has many basic mathematical functions built into it. For example, sqrt is the square root function; cos is the cosine function, log is the (natural) logarithm, exp is the exponential function, and so on. The following tables summarize the symbols for various arithmetic operations and basic mathematical functions built into R:\n\n\n\nSymbol\nMeaning\nExample\nForm in R\n\n\n\n\n+\naddition\n\\(1 + 3\\)\n1 + 3\n\n\n-\nsubtraction\n\\(5 - 1\\)\n5 - 1\n\n\n*\nmultiplication\n\\(2 \\cdot 2\\)\n2 * 2\n\n\n/\ndivision\n\\(8 / 2\\)\n8 / 2\n\n\n^\nraise to power\n\\(2^2\\)\n2 ^ 2\n\n\n\n\n\n\nFunction\nMeaning\nExample\nForm in R\n\n\n\n\nlog\nnatural log\n\\(\\log(4)\\)\nlog(4)\n\n\nexp\nexponential\n\\(\\text{e}^4\\)\nexp(4)\n\n\nsqrt\nsquare root\n\\(\\sqrt{4}\\)\nsqrt(4)\n\n\nlog2\nbase-2 log\n\\(\\log_2(4)\\)\nlog2(4)\n\n\nlog10\nbase-10 log\n\\(\\log_{10}(4)\\)\nlog10(4)\n\n\nsin\nsine (radians!)\n\\(\\sin(4)\\)\nsin(4)\n\n\nabs\nabsolute value\n\\(|-4|\\)\nabs(-4)\n\n\n\nExpressions built from these basic blocks can be freely combined. Try to calculate \\(3^{\\log(4)} - \\sin(\\text{e}^2)\\) for instance. To do so, we simply type the following and press Enter to get the result:\n\n3^log(4) - sin(exp(2))\n\n[1] 3.692108\n\n\nNow obtain \\(\\text{e}^{1.3} (4 - \\sin(\\pi / 3))\\). Notice the parentheses enclosing \\(4 - \\sin(\\pi /3)\\). This means, as usual, that this expression is evaluated first, before any of the other computations. It can be implemented in R the same way, by using parentheses:\n\nexp(1.3) * (4 - sin(3.14159 / 3))\n\n[1] 11.49948\n\n\nNote also that you do need to indicate the symbol for multiplication between closing and opening parentheses: omitting this results in an error. Try it: entering exp(1.3)(4 - sin(3.14159/3)) instead of exp(1.3)*(4 - sin(3.14159/3)) throws an error message. Also, be mindful that exp(1.3)*(4 - sin(3.14159/3)) is not the same as exp(1.3)*4 - sin(3.14159/3). This is because multiplication takes precedence over addition and subtraction, meaning that multiplications and divisions are performed first, and additions/subtractions get executed only afterwards—unless, of course, we override this behaviour with parentheses. In general, whenever you are uncertain about the order of execution of operations, it can be useful to explicitly use parentheses, even if it turns out they aren’t really necessary. For instance, you might be uncertain whether 3 * 6 + 2 first multiplies 3 by 6 and then adds 2 to the result, or if it first adds 2 to 6 and then multiplies that by 3. In that case, if you want to be absolutely sure that you perform the multiplication first, just write (3 * 6) + 2, explicitly indicating with the parentheses that the multiplication should be performed first—even though doing so would not be strictly necessary in this case.\nIncidentally, you do not need to type out 3.14159 to approximate \\(\\pi\\) in the mathematical expressions above. R has a built-in constant, pi, that you can use instead. Therefore, exp(1.3)*(4 - sin(pi/3)) produces the same result as our earlier exp(1.3)*(4 - sin(3.14159/3)).\nAnother thing to note is that the number of spaces between various operations is irrelevant. 4*(9-6) is the same as 4*(9 - 6), or 4 * (9 - 6), or, for that matter, 4   * (9-    6). To the machine, they are all the same—it is only us, the human users, who might get confused by that last form…\nIt is possible to get help on any function from the system itself. Type either help(asin) or the shorter ?asin in the console to get information on the function asin, for instance. Whenever you are not sure how to use a certain function, just ask the computer."
  },
  {
    "objectID": "R_programming_basics.html#variables-and-types",
    "href": "R_programming_basics.html#variables-and-types",
    "title": "2  R programming basics",
    "section": "2.2 Variables and types",
    "text": "2.2 Variables and types\n\n2.2.1 Numerical variables and variable names\nYou can assign a value to a named variable, and then whenever you call on that variable, the assigned value will be substituted. For instance, to obtain the square root of 9, you can simply type sqrt(9); or you can assign the value 9 to a variable first:\n\nx &lt;- 9\nsqrt(x)\n\n[1] 3\n\n\nThis will calculate the square root of x, and since x was defined as 9, we get sqrt(9), or 3. The assignment symbol &lt;- consists of a less-than symbol &lt; and a minus sign - written next to each other. Together, they look like an arrow pointing towards the left, meaning that we assign the value on the right to the variable on the left of the arrow. A keyboard shortcut in RStudio to create this symbol is to press Alt and - together.\nThe name for a variable can be almost anything, but a few restrictions apply. First, the name must consist only of letters, numbers, the period (.), and the underscore (_) character. Second, the variable’s name cannot start with a number or an underscore. So one_result or one.result are fine variable names, but 1_result or _one_result are not. Similarly, the name crowns to $ is not valid because of the spaces and the dollar ($) symbol, neither of which are numbers, letters, period, or the underscore.\nAdditionally, there are a few reserved words which have a special meaning in R, and therefore cannot be used as variable names. Examples are: if, NA, TRUE, FALSE, NULL, and function. You can see the complete list by typing ?Reserved.\nHowever, one can override all these rules and give absolutely any name to a variable by enclosing it in backward tick marks (` `). So while crowns to $ and function are not valid variable names, `crowns to $` and `function` are. For instance, you could type\n\n`crowns to $` &lt;- 0.09 # Approximate SEK-to-USD exchange rate\nmy_money &lt;- 123 # Assumed to be given in Swedish crowns\nmy_money_in_USD &lt;- my_money * `crowns to $`\nprint(my_money_in_USD)\n\n[1] 11.07\n\n\nto get our money’s worth in US dollars. Note that the freedom of naming our variables whatever we wish comes at the price of having to always include them between back ticks to refer to them. It is entirely up to you whether you would like to use this feature or avoid it; however, be sure to recognize what it means when looking at R code written by others.\nNotice also that the above chunk of code includes comments, prefaced by the hash (#) symbol. Anything that comes after the hash symbol on a line is ignored by R; it is only there for other humans to read.\n\n\n\n\n\n\nWarning\n\n\n\nThe variable my_money_in_USD above was defined in terms of the two variables my_money and `crowns to $`. You might be wondering: if we change my_money to a different value by executing my_money &lt;- 1000 (say), does my_money_in_USD also get automatically updated? The answer is no: the value of my_money_in_USD will remain unchanged. In other words, variables are not automatically recalculated the way Excel formula cells are. To recompute my_money_in_USD, you will need to execute my_money_in_USD &lt;- my_money * `crowns to $` again. This leads to a recurring theme in programming: while assigning variables is convenient, it also carries some dangers, in case we forget to appropriately update them. In this book we will be emphasizing a style of programming which avoids relying on (re-)assigning variables as much as possible.\n\n\n\n\n2.2.2 Strings\nSo far we have worked with numerical data. R can also work with textual information. In computer science, these are called character strings, or just strings for short. To assign a string to a variable, one has to enclose the text in quotes. For instance,\n\ns &lt;- \"Hello World!\"\n\nassigns the literal text Hello World! to the variable s. You can print it to screen either by just typing s at the console and pressing Enter, or typing print(s) and pressing Enter.\nOne useful function that works on strings is paste, which makes a single string out of several ones (in computer lingo, this is known as string concatenation). For example, try\n\ns1 &lt;- \"Hello\"\ns2 &lt;- \"World!\"\nmessage &lt;- paste(s1, s2)\nprint(message)\n\n[1] \"Hello World!\"\n\n\nThe component strings are separated by a space, but this can be changed with the optional sep argument to the paste function:\n\nmessage &lt;- paste(s1, s2, sep = \"\")\nprint(message)\n\n[1] \"HelloWorld!\"\n\n\nThis results in message becoming HelloWorld!, without the space in between. Between the quotes, you can put any character (including nothing, like above), which will be used as a separator when merging the strings s1 and s2. So specifying sep = \"-\" would have set message equal to Hello-World! (try it out and see how it works).\nIt is important to remember that quotes distinguish information to be treated as text from information to be treated as numbers. Consider the following two variable assignments:\n\na &lt;- 6.7\nb &lt;- \"6.7\"\n\nAlthough they look superficially similar, a is the number 6.7 while b is the string “6.7”, and the two are not equal! For instance, executing 2 * a results in 13.4, but 2 * b throws an error, because it does not make sense to multiply a bunch of text by 2.\n\n\n2.2.3 Logical values\nLet us type the following into the console, and press Enter:\n\n2 &gt; 1\n\n[1] TRUE\n\n\nWe are asking the computer whether 2 is larger than 1. And it returns the answer: TRUE. By contrast, if we ask whether two is less than one, we get FALSE:\n\n2 &lt; 1\n\n[1] FALSE\n\n\nSimilar to “greater than” and “less than”, there are other logical operations as well, such as “greater than or equal to”, “equal to”, “not equal to”, and others. The table below lists the most common options.\n\n\n\nSymbol\nMeaning\nExample in R\nResult\n\n\n\n\n&lt;\nless than\n1 &lt; 2\nTRUE\n\n\n&gt;\ngreater than\n1 &gt; 2\nFALSE\n\n\n&lt;=\nless than or equal\n2 &lt;= 5.3\nTRUE\n\n\n&gt;=\ngreater than or equal\n4.2 &gt;= 3.6\nTRUE\n\n\n==\nequal to\n5 == 6\nFALSE\n\n\n!=\nnot equal to\n5 != 6\nTRUE\n\n\n%in%\nis element of set\n2 %in% c(1, 2, 3)\nTRUE\n\n\n!\nlogical NOT\n!(1 &gt; 2)\nTRUE\n\n\n&\nlogical AND\n(1 &gt; 2) & (1 &lt; 2)\nFALSE\n\n\n|\nlogical OR\n(1 &gt; 2) | (1 &lt; 2)\nTRUE\n\n\n\nThe == and != operators can also be used with strings: \"Hello World\" == \"Hello World!\" returns FALSE, because the two strings are not exactly identical, differing in the final exclamation mark. Similarly, \"Hello World\" != \"Hello World!\" returns TRUE, because it is indeed true that the two strings are unequal.\nLogical values can either be TRUE or FALSE, with no other options.1 This is in contrast with numbers and character strings, which can take on a myriad different values. Note that TRUE and FALSE must be capitalized: true, False, or anything other than the fully capitalized forms will result in an error. Just like in the case of strings and numbers, logical values can be assigned to variables:\n\nlgl &lt;- 3 &gt; 4 # Since 3 &gt; 4 is FALSE, lgl will be assigned FALSE\nprint(!lgl) # lgl is FALSE, so !lgl (\"not lgl\") will be TRUE\n\n[1] TRUE\n\n\nThe function ifelse takes advantage of logical values, doing different things depending on whether some condition is TRUE or FALSE (“if the condition is true then do something, else do some other thing”). It takes three arguments: the first is a condition, the second is the expression that gets executed only if the condition is true, and the third is the expression that executes only if the condition is false. To illustrate its use, we can apply it in a program that simulates a coin toss. R will generate n random numbers between 0 and 1 by invoking runif(n). Here runif is a shorthand for “random-uniform”, randomly generated numbers from a uniform distribution between 0 and 1. The function call runif(1) therefore produces a single random number, and we can interpret values less than 0.5 as having tossed heads, and other values as having tossed tails. The following lines implement this:\n\ntoss &lt;- runif(1)\ncoin &lt;- ifelse(toss &lt; 0.5, \"heads\", \"tails\")\nprint(coin)\n\n[1] \"heads\"\n\n\nThis time we happened to have tossed heads, but try re-running the above three lines over and over again, to see that the results keep coming up at random.\n\n\n\n\n\n\nTip\n\n\n\nIn scientific applications, one often wants to create a sequence of random values that are repeatable. That is, while the sequence of \"heads\" and \"tails\" is random, the exact same sequence of values will be generated by any user who runs the R script. One can force such repeatability by setting the random generator seed. This seed is an integer value, and every choice will generate a different (but perfectly repeatable) random sequence. For instance, the random number generator can be seeded with the value 71 by typing set.seed(71).\nTo illustrate the difference between not seeding versus seeding the random number generator, here we run the coin-tossing program again, this time with eight tosses:\n\ntoss &lt;- runif(8)\ncoin &lt;- ifelse(toss &lt; 0.5, \"heads\", \"tails\")\nprint(coin)\n\n[1] \"heads\" \"heads\" \"tails\" \"tails\" \"heads\" \"heads\" \"heads\" \"heads\"\n\n\nIf we now execute the same three lines again, we will naturally get a different result because the coin tosses are, after all, random:\n\ntoss &lt;- runif(8)\ncoin &lt;- ifelse(toss &lt; 0.5, \"heads\", \"tails\")\nprint(coin)\n\n[1] \"heads\" \"tails\" \"tails\" \"tails\" \"tails\" \"tails\" \"tails\" \"heads\"\n\n\nTo ensure repeatability, we have to set the random number generator seed:\n\nset.seed(71) # Set the random number generator seed\ntoss &lt;- runif(8)\ncoin &lt;- ifelse(toss &lt; 0.5, \"heads\", \"tails\")\nprint(coin)\n\n[1] \"heads\" \"tails\" \"heads\" \"heads\" \"heads\" \"tails\" \"tails\" \"tails\"\n\n\nIf we run the above four lines together again, we get the exact same sequence of tosses as before:\n\nset.seed(71) # Set the random number generator seed\ntoss &lt;- runif(8)\ncoin &lt;- ifelse(toss &lt; 0.5, \"heads\", \"tails\")\nprint(coin)\n\n[1] \"heads\" \"tails\" \"heads\" \"heads\" \"heads\" \"tails\" \"tails\" \"tails\""
  },
  {
    "objectID": "R_programming_basics.html#vectors",
    "href": "R_programming_basics.html#vectors",
    "title": "2  R programming basics",
    "section": "2.3 Vectors",
    "text": "2.3 Vectors\nA vector is simply a sequence of variables of the same type. That is, the sequence may consist of numbers or strings or logical values, but one cannot intermix them. The c function will create a vector in the following way:\n\nx &lt;- c(2, 5, 1, 6, 4, 4, 3, 3, 2, 5)\n\nThis is a vector of numbers. If, after entering this line, you type x or print(x) and press Enter, all the values in the vector will appear on screen:\n\nx\n\n [1] 2 5 1 6 4 4 3 3 2 5\n\n\nWhat can you do if you want to display only the third entry? The way to do this is by applying brackets:\n\nx[3]\n\n[1] 1\n\n\nNever forget that vectors and its elements are simply variables! To show this, calculate the value of x[1] * (x[2] + x[3]), but before pressing Enter, guess what the result will be. Then check if you were correct. You can also try typing x * 2:\n\nx * 2\n\n [1]  4 10  2 12  8  8  6  6  4 10\n\n\nWhat happened? Now you performed an operation on the vector as a whole, i.e., you multiplied each element of the vector by two. Remember: you can perform all the elementary operations on vectors as well, and then the result will be obtained by applying the operation on each element separately.\nCertain functions are specific to vectors. Try mean(x) and var(x) for instance (if you are not sure what these do, just ask by typing ?mean or ?var). Some others to try: max, min, length, sum, and prod.\nOne can quickly generate vectors of sequences of values, using one of two ways. First, the notation 1:10 generates a vector of integers ranging from 1 to 10 (inclusive), in steps of 1. Similarly, 2:7 generates the same vector as c(2, 3, 4, 5, 6, 7), and so on. Second, the function seq generates sequences, starting with the first argument, ending with the second, in steps defined with the by argument. So calling\n\nseq(0, 3, by = 0.2)\n\n [1] 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0\n\n\ncreates a vector of numbers ranging from 0 to 3, in steps of 0.2.\nJust as one can create a vector of numerical values, it is also possible to create a vector of character strings or of logical values. For example:\n\nstrVec &lt;- c(\"I am the first string\", \"I am the second\", \"And I am the 3rd\")\n\nNow strVec[1] is simply equal to the string \"I am the first string\", strVec[2] is equal to \"I am the second\", and so on. Similarly, defining\n\nlogicVec &lt;- c(TRUE, FALSE, TRUE, TRUE)\n\ngives a vector whose second entry, logicVec[2], is equal to FALSE, and its other three entries are TRUE."
  },
  {
    "objectID": "R_programming_basics.html#exercises",
    "href": "R_programming_basics.html#exercises",
    "title": "2  R programming basics",
    "section": "2.4 Exercises",
    "text": "2.4 Exercises\n\nWhich of the variable names below are valid, and why?\n\nfirst.result.of_computation\n2nd.result.of_computation\ndsaqwerty\ndsaq werty\n`dsaq werty`\nbreak\nis this valid?...\n`is this valid?...`\nis_this_valid?...\n\nIn a single line of R code, calculate the number of seconds in a leap year.\nLoad and display the limerick that you saved in Exercise 5 of Section 1.5. Follow its instructions and verify that the result is indeed “nine squared and not a bit more”. (Hint: one dozen is 12, one gross is a dozen dozens, or 144, and one score is 20.)\nThe Hardy-Weinberg law of population genetics (e.g., Gillespie 2004) establishes a connection between the frequency of alleles and that of genotypes within a population.2 Assume there are two alleles of a gene, A and a, in a human population. People with the AA genotype have brown eyes; those with Aa have green eyes, and those with aa have blue eyes. What fraction of people will have these three genotypes, assuming that the frequency of A is 18%? How about 37%? And 57%?\nApplying the same rule backwards: what are the frequencies of the alleles A and a if 49% of the population has brown eyes, 21% have green eyes, and 9% have blue eyes?\nThe breeder’s equation states that the response to selection (how much the average trait of a population changes in one generation) is equal to the selection differential (mean difference of the selected parents’ traits from the population average) times the heritability of the trait (a number between 0 and 1, documenting the fraction of the variation in the trait due to genetic vs. other, non-heritable factors). In terms of an equation: \\(R = S \\cdot h^2\\), where \\(R\\) is the response to selection, \\(S\\) is the selection differential, and \\(h^2\\) is the heritability. Assume now that a constant selection differential of \\(S = 0.5\\) is applied to wing length in a population of crickets, where the mean wing length is initially 8 millimeters. The heritability is \\(h^2 = 0.74\\). What will be the mean wing length in the population after one generation? After two generations? After three generations?\nAs populations are selected for some trait, it is common for the available genetic variance to eventually diminish (since certain genotypes are being discarded from the population). This leads to a reduction in heritability over time. Repeat the above calculations, but assuming that the heritability \\(h^2\\) is reduced by 10% after every round of selective breeding.\nCreate a vector called z, with entries 1.2, 5, 3, 13.7, 6.66, and 4.2 (in that order). Then, by applying functions to this vector, obtain:\n\nIts smallest entry.\nIts largest entry.\nThe sum of all its entries.\nThe number of entries in the vector.\nThe vector’s entries sorted in increasing order (Hint: look up the function sort).\nThe vector’s entries sorted in decreasing order.\nThe product of the fourth entry with the difference of the third and sixth entries. Then take the absolute value of the result.\n\nDefine a vector of strings, called s, with the three entries \"the fat cat\", \"sat on\", and \"the mat\".\n\nCombine these three strings into a single string, and print it on the screen. (Hint: look up the help page for the paste function, in particular its collapse argument.)\nReverse the entries of s, so they come in the order \"the mat\", \"sat on\", and \"the fat cat\" (hint: check out the rev function). Then merge the three strings again into a single one, and print it on the screen.\n\n\n\n\n\n\nGillespie, John H. 2004. Population Genetics: A Concise Guide. Baltimore, MD, USA: Johns Hopkins University Press."
  },
  {
    "objectID": "R_programming_basics.html#footnotes",
    "href": "R_programming_basics.html#footnotes",
    "title": "2  R programming basics",
    "section": "",
    "text": "Technically, there is a third option: a logical value could be equal to NA, indicating missing data. Numerical and string variables can also be NA to show that their values are missing.↩︎\nThe law assumes random mating and the absence of selection, genetic drift, or gene flow into the population. This “law” is really just an application of elementary logic: if a gene has two alleles (call them A and a) and their proportions in the population are \\(p\\) and \\(q\\) respectively, then random mating means that the proportion of AA genotypes is \\(p^2\\) (i.e., the probability that an allele A meets another allele A), the proportion of Aa genotypes is \\(pq\\) by a similar logic, and that of aa genotypes is \\(q^2\\). Naturally, since every allele is either A or a, we have \\(p + q = 1\\).↩︎"
  },
  {
    "objectID": "Functions.html#user-defined-functions",
    "href": "Functions.html#user-defined-functions",
    "title": "3  Functions",
    "section": "3.1 User-defined functions",
    "text": "3.1 User-defined functions\n\n3.1.1 How to define functions\nThus far, we have been using many built-in functions in R, such as exp, log, sqrt, seq, paste, and others. However, it is also possible to define our own functions, which can then be used just like any built-in function. The way to do this is to first give a name to the function. The naming rules for functions are exactly the same as the naming rules for variables (Section 2.2.1). Then we assign to this name the function keyword, followed by the function’s arguments in parentheses, and then the R code comprising the function’s body enclosed in curly braces {}. For example, here is a function which calculates the area of a circle with given radius:\n\ncircleArea &lt;- function(radius) {\n  area &lt;- radius^2 * pi\n  return(area)\n}\n\nThe function implements the formula that the area of a circle is equal to \\(\\pi\\) times its radius squared. The return keyword determines what result the function will output when it finishes executing. In this case, the function returns the value of area that is created within the function. After running the above lines, the computer now knows and remembers the function. Calling circleArea(3) will, for example, calculate the area of a circle with radius 3, which is approximately 28.27433.\nOne very important property of functions is that any variables defined within them (such as area above) are local to that function. This means that they are not visible from outside: even after calling the function, the variable area will not be accessible to the rest of the program, despite the fact that it was declared in the function. This helps us create programs with modular structure, where functions operate as black boxes: we can use them without looking inside.\nTwo things are good to be aware of. First, the return keyword is optional in functions. If omitted, the final expression evaluated within the body of the function automatically becomes the return value. Second, the curly braces used in defining functions are not in fact specific to functions per se. Instead, the rule is as follows: an arbitrary block of code between curly braces, potentially made up of several expressions, is always treated as if it was one single expression. Its value is then by definition the final evaluated expression within the braces. This also means that the curly braces are superfluous whenever they contain only a single expression.\nThis means that the definition of circleArea could be written in alternative forms as well, some being shorter than the one above. For starters, since a variable evaluates to the value it contains (naturally), one could simply remove return and have the same effect:\n\ncircleArea &lt;- function(radius) {\n  area &lt;- radius^2 * pi\n  area\n}\n\nBut then, one may also think that it is not worth defining area just for the sake of being able to return it. One might as well simply return radius^2 * pi to begin with:\n\ncircleArea &lt;- function(radius) {\n  radius^2 * pi # Or, equivalently: return(radius^2 * pi)\n}\n\nWhile the previous version of the function has better self-documentation (because by having named the intermediate variable area, we clarified its intended meaning and purpose), the latest one is more concise. Often, it is a question of taste which version one prefers. But there is an opportunity to compress even further. Since the purpose of the curly braces is to pretend that several expressions are just one single expression (its value being the last evaluated expression inside the braces), but the above function consists of just one expression anyway, the braces can be omitted:\n\ncircleArea &lt;- function(radius) radius^2 * pi\n\nThe four versions of circleArea are all equivalent from the point of view of the computer.\n\n\n3.1.2 Functions with more than one argument\nOne can define functions with more than one argument. For instance, here is a function that calculates the volume of a cylinder with given radius and height:\n\ncylinderVol &lt;- function(radius, height) {\n  baseArea &lt;- circleArea(radius)\n  volume &lt;- baseArea * height\n  return(volume)\n}\n\nHere we used the fact that the volume of a cylinder is the area of its base circle, times its height. Notice also that we made use of our earlier circleArea function within the body of cylinderVol. While this was not a necessity and we could have simply written volume &lt;- radius^2 * pi * height above, this is generally speaking good practice: by constructing functions to solve smaller problems, one can write slightly more complicated functions which make use of those simpler ones. Then, one will be able to write even more complex functions using the slightly more complex ones in turn—and so on. We will discuss this principle in more detail below, in Section 3.2.\nWhen calling a function with multiple arguments, the default convention is that their order should follow the same pattern as in the function’s definition. The function call cylinderVol(2, 3) means that radius will be set to 2 and height to 3, because that is the order in which the arguments were defined in function(radius, height). But one can override this default ordering by explicitly naming arguments, as explained below.\nIt is optional but possible to name the arguments explicitly. This means that calling circleArea(3) is the same as calling circleArea(radius = 3), and calling cylinderVol(2, 3) is the same as calling cylinderVol(radius = 2, height = 3). Even more is true: since naming the arguments removes any ambiguity about which argument is which, one may even call cylinderVol(height = 3, radius = 2), with the arguments in reverse order, and this will still be equivalent to cylinderVol(2, 3).\nWhile naming arguments this way is optional, doing so can increase the clarity of our programs. To give an example from a built-in function in R, take log(5, 3). Does this function compute the base-5 logarithm of 3, or the base-3 logarithm of 5? While reading the documentation reveals that it is the latter, one can clarify this easily, because the second argument of log is called base, as seen from reading the help after typing ?log. We can then write log(5, base = 3), which is now easy to interpret: it is the base-3 logarithm of 5.\n\nlog(5, base = 3)\n\n[1] 1.464974\n\n\nIt is also possible to define default values for one or more of the arguments to any function. If defaults are given, the user does not have to specify the value for that argument. It will then automatically be set to the default value instead. For example, one could rewrite the cylinderVol function to specify default values for radius and height. Making these defaults be 1 means we can write:\n\ncylinderVol &lt;- function(radius = 1, height = 1) {\n  baseArea &lt;- circleArea(radius)\n  volume &lt;- baseArea * height\n  return(volume)\n}\n\nIf we now call cylinderVol() without specifying arguments, the defaults will be substituted for radius and height. Since both are equal to 1, the cylinder volume will simply be \\(\\pi\\) (about 3.14159), which is the result we get back. Alternatively, if we call cylinderVol(radius = 2), then the function returns \\(4\\pi\\) (approximately 12.56637), because the default value of 1 is substituted in place of the unspecified height argument. Importantly, if we don’t define default values and yet omit to specify one or more of those parameters, we get back an error message. For example, our earlier circleArea function had no default value for its argument radius, so leaving it unspecified throws an error:\n\ncircleArea()\n\nError in circleArea(): argument \"radius\" is missing, with no default"
  },
  {
    "objectID": "Functions.html#sec-funccomp",
    "href": "Functions.html#sec-funccomp",
    "title": "3  Functions",
    "section": "3.2 Function composition",
    "text": "3.2 Function composition\nA function is like a vending machine: we give it some input(s), and it produces some output. The output itself may then be fed as input to another function—which in turn produces an output, which can be fed to yet another function, and so on. Chaining functions together in this manner is called the composition of functions. For example, we might need to take the square root of a number, then calculate the logarithm of the output, and finally, obtain the cosine of the result. This is as simple as writing cos(log(sqrt(9))), if the number we start with is 9. More generally, one might even define a new function (let us call it cls, after the starting letters of cos, log, and sqrt) like this:\n\ncls &lt;- function(x) {\n  return(cos(log(sqrt(x))))\n}\n\nA remarkable property of composition is that the composed function (in this case, cls) is in many ways just like its constituents: it is also a black box which takes a single number as input and produces another number as its output. Putting it differently, if one did not know that the function cls was defined manually as the composition of three more “elementary” functions, and instead claimed it was just another elementary built-in function in R, there would be no way to tell the difference just based on the behaviour of the function itself. The composition of functions thus has the important property of self-similarity: if we manage to solve a problem through the composition of functions, then that solution itself will behave like an “elementary” function, and so can be used to solve even more complex problems via composition—and so on.\nIf we conceive of a program written in R as a large lego building, then one can think of functions as the lego blocks out of which the whole construction is made up. Lego pieces are designed to fit well together, one can always combine them in various ways. Furthermore, any combination of lego pieces itself behaves like a more elementary lego piece: it can be fitted together with other pieces in much the same way. The composition of functions is analogous to building larger lego blocks out of simpler ones. Remarkably, just as the size of a lego block does not hamper our ability to stick them together, the composability of functions is retained regardless of how many more elementary pieces each of them consist of. Thus, the composition of functions is an excellent way of keeping the complexity of larger programs in hand."
  },
  {
    "objectID": "Functions.html#sec-pipes",
    "href": "Functions.html#sec-pipes",
    "title": "3  Functions",
    "section": "3.3 Function piping",
    "text": "3.3 Function piping\nOne problem with composing many functions together is that the order of application must be read backwards. An expression such as sqrt(sin(cos(log(1)))) means: “take the square root of the sine of the cosine of the natural logarithm of 1”. But it is more convenient for the human brain to think of it the other way round: we first take the log of 1, then the cosine of the result, then the sine of what we got, and finally the square root. The problem of interpreting composed functions gets more difficult when the functions have more than one argument. Even something as relatively simple as\n\nexp(mean(log(seq(-3, 11, by = 2)), na.rm = TRUE))\n\n[1] 4.671655\n\n\nmay cause one to stop and have to think about what this expression actually does—and it only involves the composition of four simple functions. One can imagine the difficulties of having to parse the composition of dozens of functions in this style.\nThe expression exp(mean(log(seq(-3, 11, by = 2)), na.rm = TRUE)) generates the numeric sequence -3, -1, 1, …, 11 (jumping in steps of 2), and computes their geometric mean. To do so, it takes the logarithms of each value, takes their mean, and finally, exponentiates the result back. The problem is that the logarithm of a negative number does not exist,1 and therefore, log(-3) and log(-1) both produce undefined results. Thus, when taking the mean of the logarithms, we must remove any such undefined values. This can be accomplished via an extra argument to mean, called na.rm (“NA-remove”). By default, this is set to FALSE, but by changing it to TRUE, undefined values are simply ignored when computing the mean. For example mean(c(1, 2, 3, NA)) returns NA, because of the undefined entry in the vector; but mean(c(1, 2, 3, NA), na.rm = TRUE) returns 2, the result one gets after discarding the NA entry.\nBut all this is quite difficult to see when looking at the expression\n\nexp(mean(log(seq(-3, 11, by = 2)), na.rm = TRUE))\n\nPart of the reason is the awkward backwards order of function applications which makes it hard to see which function the argument na.rm = TRUE belongs to. Fortunately, there is a simple operator in R called a pipe (written |&gt;), which allows one to write the same code in a more streamlined way. The pipe allows one to write function application in reverse order (first the argument and then the function), making the code more transparent. Formally, x |&gt; f() is equivalent to f(x) for any function f. For example, sqrt(9) can also be written 9 |&gt; sqrt(). Thus, sqrt(sin(cos(log(1)))) can be written as 1 |&gt; log() |&gt; cos() |&gt; sin() |&gt; sqrt(), which reads straightforwardly as “start with the number 1; then take its log; then take the cosine of the result; then take the sine of that result; and then, finally, take the square root to obtain the final output”. In general, it helps to pronounce |&gt; as “then”.2\nThe pipe also works for functions with multiple arguments. In that case, x |&gt; f(y, ...) is equivalent to f(x, y, ...). That is, the pipe refers to the function’s first argument (though, as we will see in Section 10.4, it is possible to override this). Instead of the awkward and hard-to-read mean(log(seq(-3, 11, by = 2)), na.rm = TRUE), we can therefore write:\n\nseq(-3, 11, by = 2) |&gt;\n  log() |&gt;\n  mean(na.rm = TRUE) |&gt;\n  exp()\n\n[1] 4.671655\n\n\nThis is fully equivalent to the traditional form, but is much more readable, because the functions are written in the order in which they actually get applied. Moreover, even though the program is built only from the composition of functions, it reads straightforwardly as if it was a sequence of imperative instructions: we start from the vector of integers c(-3, -1, 1, 3, 5, 7, 9, 11); then we take the logarithm of each; then we take their average, discarding any invalid entries (produced in this case by taking the logarithm of negative numbers); and then, finally, we exponentiate the result back to obtain the geometric mean.\n\n\n\n\n\n\nTip\n\n\n\nOne can create the pipe symbol |&gt; with the keyboard shortcut Ctrl+Shift+M (Cmd+Shift+M on a Mac). In case you get the symbol %&gt;% instead, do the following: in RStudio, click on the Tools menu at the top, and select Global options. A new menu will pop up. Click on Code in the panel on the left, and tick the box Use native pipe operator, |&gt; (requires R 4.1+). (In case you cannot see this option, then you are likely using a version of R earlier than 4.1; you should upgrade your R installation.)"
  },
  {
    "objectID": "Functions.html#exercises",
    "href": "Functions.html#exercises",
    "title": "3  Functions",
    "section": "3.4 Exercises",
    "text": "3.4 Exercises\n\nAssume you have a population of some organism in which one given allele of some gene is the only one available in the gene pool. If a new mutant organism with a different, selectively advantageous allele appears, it would be reasonable to conclude that the new allele will fix in the population and eliminate the original one over time. This, however, is not necessarily true, because a very rare allele might succumb to being eliminated by chance, regardless of how advantageous it is. According to the famous formula of Motoo Kimura (1924-1994), the probability of such a new allele eventually fixing in the population is given as: \\[ P = \\frac{1 - \\text{e}^{-s}}{1 - \\text{e}^{-2Ns}} \\] (e.g., Gillespie 2004). Here P is the probability of eventual fixation, s is the selection differential (the degree to which the new allele is advantageous over the original one), and N is the effective population size.\n\nWrite a function that implements this formula. It should take the selection differential s and the effective population size N as parameters, and return the fixation probability as its result.\nA selection differential of 0.5 is very strong (though not unheard of). What is the likelihood that an allele with that level of advantage will fix in a population of 1000 individuals? Interpret the result.\n\nA text is palindromic if it reads backwards the same as it reads forwards. For example, “deified”, “racecar”, and “step on no pets” are palindromic texts. Assume that you are given some text in all lowercase, broken up by characters. For instance, you could be given the vector c(\"n\", \"o\", \"o\", \"n\") (a palindrome) or c(\"h\", \"e\", \"l\", \"l\", \"o\") (not a palindrome).\n\nWrite a function which checks whether the vector encodes a palindromic text. The function should return TRUE if the text is a palindrome, and FALSE otherwise. (Hint: reverse the text, collapse both the original and the reversed vectors into single strings, and then compare them using logical equality.)\nModify the function to allow for both upper- and lowercase text, treating case as irrelevant (i.e., \"A\" is treated to be equal to \"a\" when evaluating whether the text is palindromic). One simple way to do this is to convert each character of the text into lowercase, and use this standardized text for reversing and comparing with. Look up the function tolower, and implement this improvement in your palindrome checker function.\nIf you haven’t done so already: try to rewrite your function to rely as much on function composition as possible.\n\nWrite a function called contrary which takes a string as input, and prepends it with the string \"un\". That is, contrary(\"satisfactory\") should return \"unsatisfactory\", contrary(\"kind\") should return \"unkind\", and so on.\nModify the function contrary you wrote above. In general, it should prepend \"un\" to the input as before. However, if the input string already starts with \"un\", it should return the string unchanged. That is, contrary(\"disclosed\") should still return \"undisclosed\", but contrary(\"undisclosed\") should return \"undisclosed\" instead of \"unundisclosed\". (Hint: look up the substr function, and use ifelse.)\n\n\n\n\n\nGillespie, John H. 2004. Population Genetics: A Concise Guide. Baltimore, MD, USA: Johns Hopkins University Press."
  },
  {
    "objectID": "Functions.html#footnotes",
    "href": "Functions.html#footnotes",
    "title": "3  Functions",
    "section": "",
    "text": "More precisely, it is not a real number. Just as with square roots, negative numbers have complex logarithms. In particular, \\(\\log(-1) = \\log(\\text{e}^{\\text{i}\\pi}) = \\text{i}\\pi\\) and \\(\\sqrt{-1} = \\text{i}\\). Both are rigorously provable propositions, and they have profound if strange-looking consequences. For instance, \\(\\log(-1)\\) divided by \\(\\sqrt{-1}\\) is exactly \\(\\pi\\). This has prompted the 19th-century English mathematician Augustus De Morgan (1806-1871) to exclaim: “Imagine a person with a gift of ridicule [who might say:] first that a negative quantity has no logarithm; secondly that a negative quantity has no square root; thirdly that the first non-existent is to the second as the circumference of a circle is to the diameter.” (Note: it is possible to compute with complex numbers in R. Try executing log(-1 + 0i) / sqrt(-1 + 0i) to verify De Morgan’s result.)↩︎\nThis built-in pipe |&gt; has only been available since R 4.1. Upgrade your R version if necessary. Alternatively, you can use another pipe operator that is provided by the magrittr package. It is written %&gt;% instead of |&gt;, but otherwise works identically (at least at this level; we will be mentioning some of the subtler differences in Section 10.4). To use the magrittr pipe, install the package first with install.packages(\"magrittr\") and then load it with library(magrittr). Now you can use %&gt;%. Incidentally, the package name magrittr is an allusion to Belgian surrealist artist René Magritte (1898-1967) because of his famous painting La trahison des images.↩︎"
  },
  {
    "objectID": "Data_reading.html#the-tidyverse-package-suite",
    "href": "Data_reading.html#the-tidyverse-package-suite",
    "title": "4  Reading tabular data from disk",
    "section": "4.1 The tidyverse package suite",
    "text": "4.1 The tidyverse package suite\nA suite of R packages, sharing the same design philosophy, are collected under the name tidyverse. The tidyverse describes itself as “an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.” The packages in this ecosystem work well together, making it easy to combine their elements to perform a wide range of data analysis tasks. In case it is not yet installed on your computer, type\n\ninstall.packages(\"tidyverse\")\n\nat the R console and press Enter. After the package is installed, we can load it via the function call\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nLooking at the message generated by executing the above line, we see that nine packages are now loaded.1 They are called ggplot2, tibble, and so on. We will get to know these in more detail throughout the book.\nThere are even more packages that are part of the tidyverse. Typing and executing tidyverse_packages() will show all such packages. Of all these, only eight are loaded by default when invoking library(tidyverse). The others must be loaded separately. For example, readxl is a tidyverse package for loading Excel files in R. To use it, run library(readxl).\n\n\n\n\n\n\nTip\n\n\n\nIn general, it is a good idea to load all necessary packages at the top of your R script, instead of loading them wherever the need to use them first arises. There are two reasons for this. First, if you close and then later reopen RStudio, the packages do not get automatically reloaded—one must execute the calls to library all over again. Second, often other users will run the scripts you write on their own computers, and they will not be able to do so unless the proper packages are loaded first. It is then helpful to others if the necessary packages are all listed right at the top, showing what is needed to run your program."
  },
  {
    "objectID": "Data_reading.html#data-file-formats",
    "href": "Data_reading.html#data-file-formats",
    "title": "4  Reading tabular data from disk",
    "section": "4.2 Data file formats",
    "text": "4.2 Data file formats\nOne of the packages loaded by default with the tidyverse is called readr. This package contains tools for loading data files and writing them to disk. We will explore how it works using an example dataset. Before turning to the data however, it is worth mentioning a few things about data file formats.\nTabular data (i.e., data which can be organized into a table with rows and named columns) may be stored in many different forms. By far the most popular choice is to use Excel spreadsheets. While these may have many good properties, they are decidedly not recommended for scientific use. Instead, in science we strive to rely on plain text files to store our data. Plain text files have several advantages. First, they can be read by any machine without the need for special or proprietary software (such as Excel). Second, they will never go obsolete: one will always be able to open plain text files on any machine. By contrast, there is no guarantee that an Excel file saved today will still open ten years from now in future versions of Excel. And third, file formats such as Excel’s .xls and .xlsx contain a lot of metadata about formatting, font types, locale, etc. that are obscured from the user, but which have the potential to be in the way when performing scientific analyses. By contrast, plain text files have nothing up their sleeve: what you see they contain is exactly what you get; no more, no less. For these reasons, this book will emphasize workflows that rely on data stored in plain text files. However, due to the popularity of Excel, we will also learn how to read data from Excel files into R (Section 4.6), even though their use is otherwise not recommended.\n\n4.2.1 The CSV file format\nHow can one store data in plain text files? There are various solutions, but the one we will be relying on is called a delimited file. These files contain information as in a spreadsheet, with contents from every row in a different line. The column entries are separated by some delimiter—a character that represents the end of the information in one column and the start of the next. As an example, let us consider some actual data on land snail species from the Galápagos Islands.2 The data file is available by clicking on this link: islands-FL.csv.3 Set the working directory in RStudio to the folder where you have saved it. As a reminder, one can do this by executing setwd(\"/path/to/files\"), where one should substitute in one’s own path in place of /path/to/files. Then one can open islands-FL.csv in RStudio by clicking on it in the Files panel in the lower right part of the RStudio window, and then choosing the option “View file” (ignore the other option called “Import dataset…”). Having done this, a new tab opens in the editor panel (upper left region) where something like the following should appear:\nhabitat,species,size,shape\nhumid,ustulatus,17.088,-0.029\nhumid,ustulatus,20.06,-0.001\nhumid,ustulatus,16.34,0.014\narid,calvus,13.734,-0.043\nhumid,nux,21.898,-0.042\nhumid,ustulatus,16.848,-0.023\nhumid,ustulatus,19.162,0.014\nhumid,ustulatus,16.017,0.042\narid,galapaganus,18.894,0.011\nhumid,nux,26.59,0\nAnd so on. Here the first row does not contain data, but instead contains the names of the different columns. Both the names in the first row and the data in subsequent ones are separated by a comma, which is our delimiter that separates information belonging in different columns. This also explains the strange-looking extension .csv to the file: this stands for comma-separated values. Comma-separated value files are some of the most often used ones in science, and we will be relying on them frequently throughout the book.\n\n\n4.2.2 The Galápagos land snail data\nIt is worth explaining what the dataset in the file island-FL.csv represents, as we will be using it repeatedly in subsequent examples. Each row contains information on one land snail individual sampled from the Galápagos Islands. To simplify the data, the individuals have been restricted to just those that are from Floreana Island in the Galápagos. The columns in the data are:\n\nhabitat: This can be either “arid” or “humid”.\nspecies: Which species the individual belongs to. The Genus is always Naesiotus, from the family Bulimulidae. The seven species in the data are Naesiotus calvus, N. galapaganus, N. invalidus, N. nux, N. rugulosus, N. unifasciatus, and N. ustulatus.\nsize: A measurement of the absolute volume of the snail’s shell. It is given in units we are not concerned with here, but larger values correspond to larger shells. See Parent and Crespi (2009), Kraemer et al. (2018), and Kraemer et al. (2022) for more information.\nshape: A measurement for the shell’s shape. Again, the units are not important for us here, but small values represent bulky and round shells, while large values represent long and slender shells.\n\nBelow are some pictures of these snails (courtesy of Dr. Christine E. Parent):\n\n\n\nNaesiotus ustulatus – a land snail from Floreana Island, Galápagos. Photo credits: Dr. Christine E. Parent.\n\n\n\n\n\nN. ochsneri – a land snail from Santa Cruz Island, Galápagos. Photo credits: Dr. Christine E. Parent.\n\n\n\n\n\nN. chemnitzioides – a land snail from San Cristóbal Island, Galápagos. Photo credits: Dr. Christine E. Parent.\n\n\nShell morphology is an important indicator of microhabitat specialization in these snails: species with long slender shells are adaptations to arid environments due to their better surface-to-volume ratios (see the picture of N. chemnitzioides above for an example), whereas species with round and bulky shells are better adapted to humid environments.\n\n\n4.2.3 Tabulator-separated and other delimited files\nAbove we have discussed comma-separated files. Another type of file uses tabulators instead of commas as the column separator. These are called tab-separated value (TSV) files. Just like CSV files, TSV files are also very widely used. An example is provided by the file islands-FL.tsv. The data in this file are exactly identical to the one in islands-FL.csv. The only difference is that the commas are replaced by tabulators. Opening the file in RStudio, the first few lines look like this:\nhabitat species size    shape\nhumid   ustulatus   17.088  -0.029\nhumid   ustulatus   20.06   -0.001\nhumid   ustulatus   16.34   0.014\narid    calvus  13.734  -0.043\nhumid   nux 21.898  -0.042\nhumid   ustulatus   16.848  -0.023\nhumid   ustulatus   19.162  0.014\nhumid   ustulatus   16.017  0.042\narid    galapaganus 18.894  0.011\nhumid   nux 26.59   0\nCommas and tabulators are not the only possible delimiters however, and in principle any character could play that role. The only thing to be wary of is that choosing some “regular” character as the delimiter might create unintended side effects. For instance, if the character 4 is the delimiter, then a number such as 0.143 will no longer be interpreted as a single number, but as two numbers in separate columns (0.1 and 3, respectively). For this reason, only those characters should be used as delimiters which we can be certain will never be confused with the actual data.\nTo give an example: it is perfectly possible and legal (even if not common) to use a “percent-separated value” representation where the symbol % is the delimiter. Here is what that would look like with the same land snail data (one can download it from islands-FL.psv):\nhabitat%species%size%shape\nhumid%ustulatus%17.088%-0.029\nhumid%ustulatus%20.06%-0.001\nhumid%ustulatus%16.34%0.014\narid%calvus%13.734%-0.043\nhumid%nux%21.898%-0.042\nhumid%ustulatus%16.848%-0.023\nhumid%ustulatus%19.162%0.014\nhumid%ustulatus%16.017%0.042\narid%galapaganus%18.894%0.011\nhumid%nux%26.59%0\n\n\n\n\n\n\nWarning\n\n\n\nThe common file extensions .csv and .tsv are useful and suggestive: they can indicate that the data are comma- or tab-separated. However, one must bear in mind that these are mere naming conventions which are not forced in any way. That is, while not necessarily a good idea, it is perfectly possible to save comma-separated data in a file called mydata.tsv, or tab-separated data in mydata.csv. Furthermore, plain text files often have extensions such as .txt (“text”) or .dat (“data”), neither of which reveal what form the data were stored in. To be absolutely sure, one must open the file in a text editor such as RStudio’s top left editor panel, and see what was used as a delimiter. If you open a file like this, make absolutely sure not to modify and save it accidentally. Doing so compromises your hard-won data, which then no longer accurately capture your true observations."
  },
  {
    "objectID": "Data_reading.html#the-tibble-data-structure",
    "href": "Data_reading.html#the-tibble-data-structure",
    "title": "4  Reading tabular data from disk",
    "section": "4.3 The tibble data structure",
    "text": "4.3 The tibble data structure\nThe above raw formats are not yet amenable to processing within R. To make it so, we first need to import them. For delimited files there is a convenient function, read_delim, which makes this especially simple:4\n\nread_delim(\"island-FL.csv\", delim = \",\")\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nThe function read_delim takes two inputs: the name of the file (with a path if needed), and delim, which is the delimiter used to separate the columns in the file. We will discuss how it works in greater detail in Section 4.4.\nLook at the output that was produced by read_delim(\"island-FL.csv\", delim = \",\") above, starting with # A tibble: 223 x 4. A tibble (or data frame5) is the R-equivalent of an Excel-style spreadsheet. In this case, it has 223 rows and 4 columns (hence the 223 x 4). The way to conceive of a tibble is as a collection of vectors, each arranged in a column, glued together side-by-side to form a table of data. Importantly, although each vector must consist of entries of the same type as usual (e.g., they can be vectors of numbers, vectors of strings, or vectors of logical values), the different columns need not share types. For example, in the above table, the first and second columns consist of character strings but the third and fourth ones consist of numerical values. This can be seen right below the header information. Below habitat and species you can see &lt;chr&gt;, which stands for “character string”. Below size and shape we have &lt;dbl&gt; which, confusing as it may look at first, refers simply to ordinary numbers.6 In turn, columns comprising of logical values would have the tag &lt;lgl&gt; underneath them (in this case though, we don’t have such a column). The point is that by looking at the type information below the header, you can see how R has interpreted each of the columns at a glance.\nThe fact that the individual columns are simply vectors can be made explicit, by relying on the $-notation. To access a given column of the table as a vector, we write the name of the table, followed by the $ symbol, followed by the name of the column in question. To make the illustration easier, let us first assign the tibble to a variable called snailDat:\n\nsnailDat &lt;- read_delim(\"island-FL.csv\", delim = \",\")\n\nAnd now one can access e.g. the size column from the snailDat table as a vector of numbers like this:\n\nsnailDat$size\n\n  [1] 17.088 20.060 16.340 13.734 21.898 16.848 19.162 16.017 18.894 26.590\n [11] 17.865 13.857 21.706 15.354 21.369 22.714 16.530 16.604 16.292 16.075\n [21] 18.154 21.025 12.895 25.046 23.227 17.284 16.701 23.515 21.903 17.314\n [31] 23.891 21.349 24.946 14.122 28.467 17.010 16.831 16.641 20.244 22.042\n [41] 18.665 18.680 18.949 16.933 20.399 22.907 18.265 24.269 20.704 16.553\n [51] 19.246 24.894 23.610 14.131 15.617 18.605 18.343 19.340 17.017 16.868\n [61] 17.814 16.311 17.066 15.386 23.167 21.272 16.799 17.383 15.178 17.195\n [71] 19.027 15.011 23.723 17.569 20.091 18.773 22.885 13.661 33.792 19.390\n [81] 19.513 18.246 17.637 19.865 18.812 17.903 16.614 28.810 27.675 31.672\n [91] 17.209 21.618 26.420 19.135 20.493 15.142 17.195 17.382 15.502 14.404\n[101] 22.678 16.210 30.946 14.347 14.541 20.546 20.994 20.103 31.923 24.465\n[111] 24.451 19.188 20.610 16.193 15.989 25.314 16.610 13.507 16.744 21.330\n[121] 20.480 19.246 19.125 18.642 18.250 17.152 18.814 21.081 15.852 16.743\n[131] 16.602 14.879 12.299 21.646 20.539 21.798 19.211 18.168 24.985 19.938\n[141] 20.176 14.846 23.488 24.120 17.320 16.118 18.924 15.928 15.478 21.282\n[151] 15.838 19.506 24.212 18.066 17.427 15.213 20.748 20.504 20.311 17.444\n[161] 20.846 19.351 18.679 24.111 17.906 16.322 21.967 15.992 22.788 17.920\n[171] 18.804 24.760 25.766 23.452 16.145 16.040 18.868 17.528 16.556 22.199\n[181] 24.424 18.800 21.335 20.191 19.225 23.424 15.601 27.679 29.221 20.373\n[191] 16.037 17.577 21.852 18.969 16.933 19.290 24.102 21.398 16.369 19.649\n[201] 25.137 18.394 23.102 32.001 16.946 19.054 18.527 18.646 21.361 14.362\n[211] 15.858 17.201 19.886 19.264 21.503 21.728 23.312 20.423 21.912 18.158\n[221] 16.859 20.488 22.369\n\n\nHere snailDat$size is really just a vector, and can be treated as such. For example, to get the 9th entry of this vector, we can use the usual bracket notation:\n\nsnailDat$size[9]\n\n[1] 18.894\n\n\nThe result is an ordinary numerical value.\nFinally, let us take one more look at the output again:\n\nprint(snailDat)\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nWhen displaying large tibbles, R will not dump all the data at you. Instead, it will display the first 10 rows, with a message indicating how many more rows remain (in our case, we have ...with 213 more rows written at the end of the printout). The system is still aware of the other rows; it just does not show them. To get a full view of a tibble in a more digestible, spreadsheet-like style, one can use the view function. Try running view(snailDat) and see what happens!"
  },
  {
    "objectID": "Data_reading.html#sec-delim",
    "href": "Data_reading.html#sec-delim",
    "title": "4  Reading tabular data from disk",
    "section": "4.4 Reading delimited files",
    "text": "4.4 Reading delimited files\nWe have seen that the function read_delim works by taking two inputs: the name of the file to read, and the delimiter character. So to read a comma-separated file, we can write\n\nread_delim(\"island-FL.csv\", delim = \",\")\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nSimilarly, in case we want to read a percent-separated file such as islands-FL.psv, all we need to do is change the delimiter:\n\nread_delim(\"island-FL.psv\", delim = \"%\")\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nThe output is exactly as before, since the file contained the same information (just represented differently due to the different delimiter).\nTo read a tab-separated file, remember not to simply press the tabulator key between the quotes of the delim argument. Instead, R uses the character string \"\\t\" to represent a single press of the tabulator:\n\nread_delim(\"island-FL.tsv\", delim = \"\\t\")\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nWhat happens if the wrong delimiter is specified—for instance, if we type read_delim(\"island-FL.csv\", delim = \"%\")? In that case R will look for the % character to separate the columns, but since these do not occur anywhere, each row will be interpreted as a single column entry in its entirety:\n\nread_delim(\"island-FL.csv\", delim = \"%\")\n\n# A tibble: 223 × 1\n   `habitat,species,size,shape` \n   &lt;chr&gt;                        \n 1 humid,ustulatus,17.088,-0.029\n 2 humid,ustulatus,20.06,-0.001 \n 3 humid,ustulatus,16.34,0.014  \n 4 arid,calvus,13.734,-0.043    \n 5 humid,nux,21.898,-0.042      \n 6 humid,ustulatus,16.848,-0.023\n 7 humid,ustulatus,19.162,0.014 \n 8 humid,ustulatus,16.017,0.042 \n 9 arid,galapaganus,18.894,0.011\n10 humid,nux,26.59,0            \n# ℹ 213 more rows\n\n\nAs seen, we have a tibble with a single column called `habitat,species,size,shape1,shape2`, which has the type &lt;chr&gt; (character string). One must beware of such mistakes, because the computer will not signal any errors: for all it cares, we did intend % to be the delimiter. However, one will not be able to properly work with the data subsequently if it gets loaded in this mistaken form.\nThe CSV and TSV file formats are so common that there are shorthands available for loading them. Instead of writing read_delim(\"island-FL.csv\", delim = \",\"), one can equivalently type read_csv(\"island-FL.csv\") for the same effect:\n\nread_csv(\"island-FL.csv\")\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nSimilarly, read_tsv(\"island-FL.tsv\") is an equivalent shorthand to read_delim(\"island-FL.tsv\", delim = \"\\t\"):\n\nread_tsv(\"island-FL.tsv\")\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows"
  },
  {
    "objectID": "Data_reading.html#sec-rename",
    "href": "Data_reading.html#sec-rename",
    "title": "4  Reading tabular data from disk",
    "section": "4.5 Naming (and renaming) columns",
    "text": "4.5 Naming (and renaming) columns\nAll of the files we have looked at so far contained a header as their first row: instead of containing data, they contained the names of the data columns. Having such a header is good practice. Sometimes however, the header may be missing. For example, the file islands-FL-nohead.csv contains the same land snail data as the other files above, but it lacks a header. The first few lines of the file look like this:\nhumid,ustulatus,17.088,-0.029\nhumid,ustulatus,20.06,-0.001\nhumid,ustulatus,16.34,0.014\narid,calvus,13.734,-0.043\nhumid,nux,21.898,-0.042\nhumid,ustulatus,16.848,-0.023\nhumid,ustulatus,19.162,0.014\nhumid,ustulatus,16.017,0.042\narid,galapaganus,18.894,0.011\nhumid,nux,26.59,0\nIf one tries to read this file either with read_delim(\"island-FL-nohead.csv\", delim = \",\") or the shorthand read_csv(\"island-FL-nohead.csv\"), we get the following:\n\nread_delim(\"island-FL-nohead.csv\", delim = \",\")\n\n# A tibble: 222 × 4\n   humid ustulatus   `17.088` `-0.029`\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n 1 humid ustulatus       20.1   -0.001\n 2 humid ustulatus       16.3    0.014\n 3 arid  calvus          13.7   -0.043\n 4 humid nux             21.9   -0.042\n 5 humid ustulatus       16.8   -0.023\n 6 humid ustulatus       19.2    0.014\n 7 humid ustulatus       16.0    0.042\n 8 arid  galapaganus     18.9    0.011\n 9 humid nux             26.6    0    \n10 arid  calvus          17.9   -0.024\n# ℹ 212 more rows\n\n\nThe fact that a header is missing is obvious to a human reading the file—but not to the computer, which simply took the first row to be the header names anyway and interpreted the data in there as if they were column names. To avoid doing this, one can pass col_names = FALSE as an argument to either read_delim, read_csv, or read_tsv. The col_names argument is set by default to TRUE; to override it, we must explicitly change it like this:\n\nread_delim(\"island-FL-nohead.csv\", delim = \",\", col_names = FALSE)\n\n# A tibble: 223 × 4\n   X1    X2             X3     X4\n   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid ustulatus    17.1 -0.029\n 2 humid ustulatus    20.1 -0.001\n 3 humid ustulatus    16.3  0.014\n 4 arid  calvus       13.7 -0.043\n 5 humid nux          21.9 -0.042\n 6 humid ustulatus    16.8 -0.023\n 7 humid ustulatus    19.2  0.014\n 8 humid ustulatus    16.0  0.042\n 9 arid  galapaganus  18.9  0.011\n10 humid nux          26.6  0    \n# ℹ 213 more rows\n\n\nWhile this works, the column names now default to the moderately informative labels X1, X2, and so on. In fact, one can use the col_names argument to explicitly specify a vector of character strings, which are then interpreted as the names to be given to the columns in case no header information exists within the file itself. For example:\n\nread_delim(\"island-FL-nohead.csv\", delim = \",\",\n           col_names = c(\"habitat\", \"species\", \"size\", \"shape\"))\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nAnd now these data are identical to our earlier snailDat tibble."
  },
  {
    "objectID": "Data_reading.html#sec-excel",
    "href": "Data_reading.html#sec-excel",
    "title": "4  Reading tabular data from disk",
    "section": "4.6 Excel tables",
    "text": "4.6 Excel tables\nAlthough their use is discouraged in science, one should know how to read data from an Excel spreadsheet. To do this, one needs to load the readxl package. This package is part of the tidyverse, but does not get automatically attached when running library(tidyverse). Therefore, we first load the package:\n\nlibrary(readxl)\n\nWe can now load Excel files with the function read_excel(). At the start, we downloaded an Excel version of the land snail data, called island-FL.xlsx. It holds the exact same data as the original CSV file, just saved in Excel format for instructive purposes. Let us load this file:\n\nread_excel(\"island-FL.xlsx\")\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nThe function read_excel has several further options. For example, given an Excel table with multiple sheets, one can specify which one to import, using the sheet argument. Check the help page of read_excel, and experiment with its options."
  },
  {
    "objectID": "Data_reading.html#writing-data-to-files",
    "href": "Data_reading.html#writing-data-to-files",
    "title": "4  Reading tabular data from disk",
    "section": "4.7 Writing data to files",
    "text": "4.7 Writing data to files\nFinally, data can not only be read from a file, but also written out to one. Then, instead of read_delim, read_csv, read_tsv and the like, one uses write_delim, write_csv, write_tsv, and so on. For instance, to save some tibble called dat in CSV form, one can do either\n\nwrite_delim(dat, file = \"/path/to/file.csv\", delim = \",\")\n\nor the equivalent but shorter\n\nwrite_csv(dat, file = \"/path/to/file.csv\")\n\nwhere /path/to/file.csv should be replaced by the path and file name with which the data ought to be saved."
  },
  {
    "objectID": "Data_reading.html#sec-reading-exercises",
    "href": "Data_reading.html#sec-reading-exercises",
    "title": "4  Reading tabular data from disk",
    "section": "4.8 Exercises",
    "text": "4.8 Exercises\n\nGoldberg et al. (2010) collected data on self-incompatibility in the family Solanaceae (nightshades). It contains a list of 356 species, along with a column determining self-incompatibility status (0: self-incompatible; 1: self-compatible; 2-5: more complicated selfing scenarios). The data are in the file Goldberg2010_data.csv. An equivalent version is available in Goldberg2010_data.xlsx, with the only difference that it is saved in Excel format.\n\nRead the file Goldberg2010_data.csv using read_delim.\nRead the file Goldberg2010_data.csv using read_csv.\nRead the Excel file Goldberg2010_data.xlsx using read_excel from the readxl package.\nRead the file Goldberg2010_data.csv using read_csv, and assign it to a variable called goldbergDat.\nExtract the species names (first column, called Species) from goldbergDat as a vector of character strings, using the $ notation. What are the 42nd and 137th entries of this vector?\n\nThe data file ladybugs.csv contains information on whether ladybugs sampled from rural versus industrial areas tend to be red or black morphs. Black morphs are thought to be an adaptation to more polluted industrial environments where the dark coloration provides better camouflage. The file is comma-separated and lacks headers. The meanings of the columns are, in order: habitat type, site code where the sampling was done, morph color, and the number of individuals sampled.\n\nLoad this file into a tibble in R, and give the following names to its columns: habitat, site, morph, and number.\nExtract the last column (number) as a vector and calculate its mean and variance.\n\nSmith et al. (2003) compiled a database of the body masses of mammals of the late Quaternary period. The data are in the file Smith2003_data.txt. The column names are not specified in the file, but they are, in order: Continent (AF=Africa, etc.), Status (extinct, historical, introduction, or extant), Order, Family, Genus, Species, Base-10 Log Mass, Combined Mass (grams), and Reference (numbers, referring to a numerically ordered list of published works—no need to worry about the details).\n\nWhat is the delimiter in this data file?\nLoad the data and give appropriate names to its columns, based on the information above.\n\n\n\n\n\n\nBarabás, György, Christine Parent, Andrew Kraemer, Frederik Van de Perre, and Frederik De Laender. 2022. “The evolution of trait variance creates a tension between species diversity and functional diversity.” Nature Communications 13 (2521): 1–10. https://doi.org/10.1038/s41467-022-30090-4.\n\n\nGoldberg, Emma E., Joshua R. Kohn, Russell Lande, Kelly A. Robertson, Stephen A. Smith, and Boris Igić. 2010. “Species Selection Maintains Self-Incompatibility.” Science 330 (6003): 493–95. https://doi.org/10.1126/science.1194513.\n\n\nKraemer, Andrew C., C. W. Philip, A. M. Rankin, and C. E. Parent. 2018. “Trade-Offs Direct the Evolution of Coloration in Galápagos Land Snails.” Proceedings of the Royal Society B 286: 20182278.\n\n\nKraemer, Andrew C., Yannik E. Roell, Nate F. Shoobs, and Christine E. Parent. 2022. “Does island ontogeny dictate the accumulation of both species richness and functional diversity?” Global Ecology and Biogeography 31 (1): 123–37. https://doi.org/10.1111/geb.13420.\n\n\nParent, C. E., and B. J. Crespi. 2009. “Ecological Opportunity in Adaptive Radiation of Galápagos Endemic Land Snails.” American Naturalist 174: 898–905.\n\n\nSmith, Felisa A., S. Kathleen Lyons, S. K. Morgan Ernest, Kate E. Jones, Dawn M. Kaufman, Tamar Dayan, Pablo A. Marquet, James H. Brown, and John P. Haskell. 2003. “Body Mass of Late Quaternary Mammals.” Ecology 84 (12): 3403. https://doi.org/10.1890/02-9003."
  },
  {
    "objectID": "Data_reading.html#footnotes",
    "href": "Data_reading.html#footnotes",
    "title": "4  Reading tabular data from disk",
    "section": "",
    "text": "If you are using a tidyverse version earlier than 2.0.0, only eight packages are loaded—lubridate, a package for working with dates, is missing from the list.↩︎\nThe data have been adapted from Barabás et al. (2022).↩︎\nThis and other data files used throughout the book will be accessible in a similar way. They are compressed in a zip file; extract them to obtain the data.↩︎\nWarning: there exists a similarly-named function called read.delim which is part of base R. It does much the same thing as read_delim; however, its use is clunkier and less flexible. You can think of read_delim as the tidyverse upgrade to the original read.delim. My recommendation is to stick with using just read_delim—it is simpler and at the same time more powerful than its predecessor.↩︎\nData frames (a feature of base R) and tibbles (a tidyverse construct) are equivalent for most practical purposes. Tibbles offer some features that are absent from data frames and omit quirks of data frames which tend to get in the way. Like with read_delim and read.delim, tibbles can be thought of as a slightly upgraded and more user-friendly version of data frames. You do not need to be overly concerned with the precise differences between the two. In this book we will be using tibbles almost exclusively.↩︎\nThe abbreviation &lt;dbl&gt; happens to stand for double-precision numerical value, a standard way of representing numbers on computers.↩︎"
  },
  {
    "objectID": "Basic_data_wrangling.html#important-functions-for-transforming-data",
    "href": "Basic_data_wrangling.html#important-functions-for-transforming-data",
    "title": "5  Basic data manipulation",
    "section": "5.1 Important functions for transforming data",
    "text": "5.1 Important functions for transforming data\nLet us start by loading the tidyverse:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nAs seen from the message output above, the dplyr package is part of the tidyverse and gets loaded by default. It allows one to arrange and manipulate data efficiently. The basic functions one should know are select, filter, slice, rename, arrange, and mutate. Additionally, a useful (though not as commonly used) function is distinct, also explained below. We will use the island-FL.csv data file we worked with in Chapter 4:\n\nsnailDat &lt;- read_csv(\"island-FL.csv\")\nprint(snailDat)\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nNow we will give examples of each of the functions select, filter, slice, rename, arrange, mutate, and distinct. They all work similarly in that the first argument they take is the data, in the form of a tibble. Their other arguments, and what they each do, are explained below.\n\n5.1.1 select\nThis function chooses columns of the data. The second and subsequent arguments of the function are the columns which should be retained. For example, select(snailDat, species) will keep only the species column of snailDat:\n\nselect(snailDat, species)\n\n# A tibble: 223 × 1\n   species    \n   &lt;chr&gt;      \n 1 ustulatus  \n 2 ustulatus  \n 3 ustulatus  \n 4 calvus     \n 5 nux        \n 6 ustulatus  \n 7 ustulatus  \n 8 ustulatus  \n 9 galapaganus\n10 nux        \n# ℹ 213 more rows\n\n\nIt is also possible to deselect columns, by prepending a minus sign (-) in front of the column names. To drop the species column, we can type:\n\nselect(snailDat, -species)\n\n# A tibble: 223 × 3\n   habitat  size  shape\n   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid    17.1 -0.029\n 2 humid    20.1 -0.001\n 3 humid    16.3  0.014\n 4 arid     13.7 -0.043\n 5 humid    21.9 -0.042\n 6 humid    16.8 -0.023\n 7 humid    19.2  0.014\n 8 humid    16.0  0.042\n 9 arid     18.9  0.011\n10 humid    26.6  0    \n# ℹ 213 more rows\n\n\nNow we are left with only the columns habitat, size, and shape. One can also select and deselect multiple columns in a similar vein. For example, select(snailDat, -species, -habitat) removes both columns and leaves only size, and shape (try it).\nThere are several other options within select, which mostly help with selecting several columns at a time fulfilling certain criteria. The collection of these options and methods is called tidy selection. First of all, tidy selection allows one to specify, as character strings, what the column names should start or end with, using starts_with and ends_with:\n\nselect(snailDat, starts_with(\"s\"))\n\n# A tibble: 223 × 3\n   species      size  shape\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 ustulatus    17.1 -0.029\n 2 ustulatus    20.1 -0.001\n 3 ustulatus    16.3  0.014\n 4 calvus       13.7 -0.043\n 5 nux          21.9 -0.042\n 6 ustulatus    16.8 -0.023\n 7 ustulatus    19.2  0.014\n 8 ustulatus    16.0  0.042\n 9 galapaganus  18.9  0.011\n10 nux          26.6  0    \n# ℹ 213 more rows\n\nselect(snailDat, starts_with(\"sh\"))\n\n# A tibble: 223 × 1\n    shape\n    &lt;dbl&gt;\n 1 -0.029\n 2 -0.001\n 3  0.014\n 4 -0.043\n 5 -0.042\n 6 -0.023\n 7  0.014\n 8  0.042\n 9  0.011\n10  0    \n# ℹ 213 more rows\n\nselect(snailDat, ends_with(\"e\"))\n\n# A tibble: 223 × 2\n    size  shape\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1  17.1 -0.029\n 2  20.1 -0.001\n 3  16.3  0.014\n 4  13.7 -0.043\n 5  21.9 -0.042\n 6  16.8 -0.023\n 7  19.2  0.014\n 8  16.0  0.042\n 9  18.9  0.011\n10  26.6  0    \n# ℹ 213 more rows\n\nselect(snailDat, ends_with(\"pe\"))\n\n# A tibble: 223 × 1\n    shape\n    &lt;dbl&gt;\n 1 -0.029\n 2 -0.001\n 3  0.014\n 4 -0.043\n 5 -0.042\n 6 -0.023\n 7  0.014\n 8  0.042\n 9  0.011\n10  0    \n# ℹ 213 more rows\n\n\nSimilarly, the function contains can select columns which contain some string anywhere in their names. For example, select(snailDat, ends_with(\"pe\")) above only selected the shape column, but select(snailDat, contains(\"pe\")) additionally selects species:\n\nselect(snailDat, contains(\"pe\"))\n\n# A tibble: 223 × 2\n   species      shape\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 ustulatus   -0.029\n 2 ustulatus   -0.001\n 3 ustulatus    0.014\n 4 calvus      -0.043\n 5 nux         -0.042\n 6 ustulatus   -0.023\n 7 ustulatus    0.014\n 8 ustulatus    0.042\n 9 galapaganus  0.011\n10 nux          0    \n# ℹ 213 more rows\n\n\nOne can combine these selection methods using the & (“and”), | (“or”), and ! (“not”) logical operators. To select all columns which start with \"s\" but do not contain the letter \"z\" in their names:\n\nselect(snailDat, starts_with(\"s\") & !contains(\"z\"))\n\n# A tibble: 223 × 2\n   species      shape\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 ustulatus   -0.029\n 2 ustulatus   -0.001\n 3 ustulatus    0.014\n 4 calvus      -0.043\n 5 nux         -0.042\n 6 ustulatus   -0.023\n 7 ustulatus    0.014\n 8 ustulatus    0.042\n 9 galapaganus  0.011\n10 nux          0    \n# ℹ 213 more rows\n\n\nThe following selects columns that either contain \"ha\" in their names, or end with the letter \"s\":\n\nselect(snailDat, contains(\"ha\") | ends_with(\"s\"))\n\n# A tibble: 223 × 3\n   habitat  shape species    \n   &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;      \n 1 humid   -0.029 ustulatus  \n 2 humid   -0.001 ustulatus  \n 3 humid    0.014 ustulatus  \n 4 arid    -0.043 calvus     \n 5 humid   -0.042 nux        \n 6 humid   -0.023 ustulatus  \n 7 humid    0.014 ustulatus  \n 8 humid    0.042 ustulatus  \n 9 arid     0.011 galapaganus\n10 humid    0     nux        \n# ℹ 213 more rows\n\n\nFinally, it is possible to select a range of columns, using the colon (:) operator. To select the columns species and shape, along with any columns in between them, we write:\n\nselect(snailDat, species:shape)\n\n# A tibble: 223 × 3\n   species      size  shape\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 ustulatus    17.1 -0.029\n 2 ustulatus    20.1 -0.001\n 3 ustulatus    16.3  0.014\n 4 calvus       13.7 -0.043\n 5 nux          21.9 -0.042\n 6 ustulatus    16.8 -0.023\n 7 ustulatus    19.2  0.014\n 8 ustulatus    16.0  0.042\n 9 galapaganus  18.9  0.011\n10 nux          26.6  0    \n# ℹ 213 more rows\n\n\nSuch range selection can also be combined with the logical operations above. For instance, to select the range from habitat to species, as well as any columns whose name contains the letter \"z\":\n\nselect(snailDat, habitat:species | contains(\"z\"))\n\n# A tibble: 223 × 3\n   habitat species      size\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;\n 1 humid   ustulatus    17.1\n 2 humid   ustulatus    20.1\n 3 humid   ustulatus    16.3\n 4 arid    calvus       13.7\n 5 humid   nux          21.9\n 6 humid   ustulatus    16.8\n 7 humid   ustulatus    19.2\n 8 humid   ustulatus    16.0\n 9 arid    galapaganus  18.9\n10 humid   nux          26.6\n# ℹ 213 more rows\n\n\n\n\n5.1.2 filter\nWhile select chooses columns, filter chooses rows from the data. As with all these functions, the first argument of filter is the tibble to be filtered. The second argument is a logical condition on the columns. Those rows which satisfy the condition are retained; the rest are dropped. Thus, filter keeps only those rows of the data which fulfill some condition.\nFor example, to retain only those individuals from snailDat whose shell size is at least 29:\n\nfilter(snailDat, size &gt;= 29)\n\n# A tibble: 6 × 4\n  habitat species       size  shape\n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 humid   unifasciatus  33.8 -0.07 \n2 humid   unifasciatus  31.7 -0.115\n3 humid   unifasciatus  30.9 -0.074\n4 humid   unifasciatus  31.9 -0.071\n5 humid   nux           29.2 -0.01 \n6 humid   unifasciatus  32.0 -0.087\n\n\nThe filtered data have only 6 rows instead of the original 223—this is the number of snail individuals with a very large shell size. As seen, five of these belong in the species Naesiotus unifasciatus, and only one in the species Naesiotus nux.\n\n\n5.1.3 slice\nWith slice, one can choose rows of the data, just like with filter. Unlike with filter however, slice receives a vector of row indices to retain instead of a condition to be tested on each row. So, for example, if one wanted to keep only the first, second, and fifth rows, then one can do so with slice:\n\nslice(snailDat, c(1, 2, 5))\n\n# A tibble: 3 × 4\n  habitat species    size  shape\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 humid   ustulatus  17.1 -0.029\n2 humid   ustulatus  20.1 -0.001\n3 humid   nux        21.9 -0.042\n\n\n(Note: the numbers in front of the rows in the output generated by tibbles always pertain to the row numbers of the current table, not the one from which they were created. So the row labels 1, 2, 3 above simply enumerate the rows of the sliced data. The actual rows still correspond to rows 1, 2, and 5 in the original snailDat.)\n\n\n5.1.4 rename\nThe rename function simply gives new names to existing columns. The first argument, as always, is the tibble in which the column(s) should be renamed. The subsequent arguments follow the pattern new_name = old_name in replacing column names. For example, in the land snail data, the arid and humid habitats are often referred to as arid or humid zones. To rename habitat to zone, we simply write:\n\nrename(snailDat, zone = habitat)\n\n# A tibble: 223 × 4\n   zone  species      size  shape\n   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid ustulatus    17.1 -0.029\n 2 humid ustulatus    20.1 -0.001\n 3 humid ustulatus    16.3  0.014\n 4 arid  calvus       13.7 -0.043\n 5 humid nux          21.9 -0.042\n 6 humid ustulatus    16.8 -0.023\n 7 humid ustulatus    19.2  0.014\n 8 humid ustulatus    16.0  0.042\n 9 arid  galapaganus  18.9  0.011\n10 humid nux          26.6  0    \n# ℹ 213 more rows\n\n\nMultiple columns can also be renamed. To change all column names to start with capital letters:\n\nrename(snailDat,\n       Habitat = habitat, Species = species, Size = size, Shape = shape)\n\n# A tibble: 223 × 4\n   Habitat Species      Size  Shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\n\n\n5.1.5 arrange\nThis function rearranges the rows of the data, in increasing order of the column given as the second argument. For example, to arrange in increasing order of size, we write:\n\narrange(snailDat, size)\n\n# A tibble: 223 × 4\n   habitat species  size  shape\n   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 arid    calvus   12.3 -0.019\n 2 arid    calvus   12.9 -0.039\n 3 arid    calvus   13.5 -0.012\n 4 arid    calvus   13.7 -0.018\n 5 arid    calvus   13.7 -0.043\n 6 arid    calvus   13.9  0.01 \n 7 arid    calvus   14.1 -0.027\n 8 arid    calvus   14.1 -0.016\n 9 arid    calvus   14.3 -0.011\n10 arid    calvus   14.4  0.002\n# ℹ 213 more rows\n\n\nTo arrange in decreasing order, there is a small helper function called desc. Arranging by desc(size) instead of size will arrange the rows in decreasing order of size:\n\narrange(snailDat, desc(size))\n\n# A tibble: 223 × 4\n   habitat species       size  shape\n   &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   unifasciatus  33.8 -0.07 \n 2 humid   unifasciatus  32.0 -0.087\n 3 humid   unifasciatus  31.9 -0.071\n 4 humid   unifasciatus  31.7 -0.115\n 5 humid   unifasciatus  30.9 -0.074\n 6 humid   nux           29.2 -0.01 \n 7 humid   unifasciatus  28.8 -0.075\n 8 humid   unifasciatus  28.5 -0.088\n 9 humid   nux           27.7 -0.05 \n10 humid   unifasciatus  27.7 -0.047\n# ℹ 213 more rows\n\n\nIt is also perfectly possible to arrange by a column whose type is character string. In that case, the system will sort the rows in alphabetical order—or reverse alphabetical order in case desc is applied. For example, to sort in alphabetical order of species names:\n\narrange(snailDat, species)\n\n# A tibble: 223 × 4\n   habitat species  size  shape\n   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 arid    calvus   13.7 -0.043\n 2 arid    calvus   17.9 -0.024\n 3 arid    calvus   13.9  0.01 \n 4 arid    calvus   16.5 -0.004\n 5 arid    calvus   16.6 -0.006\n 6 arid    calvus   16.1  0.01 \n 7 arid    calvus   18.2 -0.003\n 8 arid    calvus   12.9 -0.039\n 9 arid    calvus   17.3  0.002\n10 arid    calvus   14.1 -0.027\n# ℹ 213 more rows\n\n\nAnd to sort in reverse alphabetical order:\n\narrange(snailDat, desc(species))\n\n# A tibble: 223 × 4\n   habitat species    size  shape\n   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus  17.1 -0.029\n 2 humid   ustulatus  20.1 -0.001\n 3 humid   ustulatus  16.3  0.014\n 4 humid   ustulatus  16.8 -0.023\n 5 humid   ustulatus  19.2  0.014\n 6 humid   ustulatus  16.0  0.042\n 7 humid   ustulatus  15.4 -0.016\n 8 humid   ustulatus  16.3 -0.017\n 9 humid   ustulatus  16.7 -0.034\n10 humid   ustulatus  17.0 -0.018\n# ℹ 213 more rows\n\n\nNotice that when we sort the rows by species, there are many ties—rows with the same value of species. In those cases, arrange will not be able to decide which rows should come earlier, and so any ordering that was present before invoking arrange will be retained. In case we would like to break the ties, we can give further sorting variables, as the third, fourth, etc. arguments to arrange. To sort the data by species, and to resolve ties in order of increasing size, we write:\n\narrange(snailDat, species, size)\n\n# A tibble: 223 × 4\n   habitat species  size  shape\n   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 arid    calvus   12.3 -0.019\n 2 arid    calvus   12.9 -0.039\n 3 arid    calvus   13.5 -0.012\n 4 arid    calvus   13.7 -0.018\n 5 arid    calvus   13.7 -0.043\n 6 arid    calvus   13.9  0.01 \n 7 arid    calvus   14.1 -0.027\n 8 arid    calvus   14.1 -0.016\n 9 arid    calvus   14.3 -0.011\n10 arid    calvus   14.4  0.002\n# ℹ 213 more rows\n\n\nThis causes the table to be sorted primarily by species, but in case there are ties (equal species between multiple rows), they will be resolved in priority of size—first the smallest and then increasingly larger individuals.\n\n\n5.1.6 mutate\nThe mutate function allows us to create new columns from existing ones. We may apply any function or operator we learned about to existing columns, and the result of the computation will go into the new column. We do this in the second argument of mutate (the first, as always, is the data tibble) by first giving a name to the column, then writing =, and then the desired computation. For example, we could create a new column indicating whether a snail is “large” (has a shell size above some threshold—say, 25) or “small”. We can do this using the ifelse function within mutate:\n\nmutate(snailDat, shellSize = ifelse(size &gt; 25, \"large\", \"small\"))\n\n# A tibble: 223 × 5\n   habitat species      size  shape shellSize\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    \n 1 humid   ustulatus    17.1 -0.029 small    \n 2 humid   ustulatus    20.1 -0.001 small    \n 3 humid   ustulatus    16.3  0.014 small    \n 4 arid    calvus       13.7 -0.043 small    \n 5 humid   nux          21.9 -0.042 small    \n 6 humid   ustulatus    16.8 -0.023 small    \n 7 humid   ustulatus    19.2  0.014 small    \n 8 humid   ustulatus    16.0  0.042 small    \n 9 arid    galapaganus  18.9  0.011 small    \n10 humid   nux          26.6  0     large    \n# ℹ 213 more rows\n\n\nThe original columns of the data are retained, but we now also have the additional shellSize column.\nOne very common transformation on quantities such as size and shape is to standardize them: subtract the overall mean from each entry and then divide the result by the standard deviation. This makes the quantities unitless with mean 0 and standard deviation 1. This is how one can perform this standardization with mutate:\n\nmutate(snailDat,\n       stdSize = (size - mean(size)) / sd(size),\n       stdShape = (shape - mean(shape)) / sd(shape))\n\n# A tibble: 223 × 6\n   habitat species      size  shape stdSize stdShape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029  -0.660 -0.575  \n 2 humid   ustulatus    20.1 -0.001   0.124  0.00343\n 3 humid   ustulatus    16.3  0.014  -0.858  0.313  \n 4 arid    calvus       13.7 -0.043  -1.55  -0.864  \n 5 humid   nux          21.9 -0.042   0.609 -0.844  \n 6 humid   ustulatus    16.8 -0.023  -0.723 -0.451  \n 7 humid   ustulatus    19.2  0.014  -0.113  0.313  \n 8 humid   ustulatus    16.0  0.042  -0.943  0.892  \n 9 arid    galapaganus  18.9  0.011  -0.183  0.251  \n10 humid   nux          26.6  0       1.85   0.0241 \n# ℹ 213 more rows\n\n\n\n\n5.1.7 distinct\nWhile not as important as the previous six functions, distinct can also be useful. It takes as its input a tibble, and removes all rows that contain exact copies of any other row. For example, we might wonder how many different species there are in snailDat. One way to answer this is to select the species column only, and then apply distinct to remove duplicated entries:\n\ndistinct(select(snailDat, species))\n\n# A tibble: 7 × 1\n  species     \n  &lt;chr&gt;       \n1 ustulatus   \n2 calvus      \n3 nux         \n4 galapaganus \n5 unifasciatus\n6 invalidus   \n7 rugulosus   \n\n\nSo each individual in the data comes from one of the above seven species."
  },
  {
    "objectID": "Basic_data_wrangling.html#using-pipes-to-our-advantage",
    "href": "Basic_data_wrangling.html#using-pipes-to-our-advantage",
    "title": "5  Basic data manipulation",
    "section": "5.2 Using pipes to our advantage",
    "text": "5.2 Using pipes to our advantage\nLet us take a slightly more complicated (and quite typical) data analysis task. We want to answer the question: how many species are there with at least some individuals whose standardized shell size is larger than a threshold—say, 2? This corresponds to individuals whose size is two standard deviations above the community average.\nTo obtain an answer, we could first mutate a new column that contains the standardized shell size for each individual. We can then filter for those rows only for which this is greater than 2. Afterwards, we can use select to choose just the species column. This will likely have repeated entries (because multiple individuals from the same species could have a standardized size greater than 2), so as a final step, we should remove duplicated rows with distinct.\nSince mutate, filter, select, distinct, etc. are just ordinary functions, they do not “modify” data. They merely take a tibble as input (plus other arguments) and return another tibble. They do not do anything to the original input data. In order for R not to forget their result immediately after they are computed, they have to be stored in variables. So one way of implementing the solution might rely on repeated assignments, as below:\n\nmutatedDat &lt;- mutate(snailDat, stdSize = (size - mean(size)) / sd(size))\nfilteredDat &lt;- filter(mutatedDat, stdSize &gt; 2)\nonlySpeciesDat &lt;- select(filteredDat, species)\nspeciesListDat &lt;- distinct(onlySpeciesDat)\nprint(speciesListDat)\n\n# A tibble: 2 × 1\n  species     \n  &lt;chr&gt;       \n1 unifasciatus\n2 nux         \n\n\nIt turns out that only two out of the seven species have individuals with shells that large.\nWhile this solution works, it requires inventing arbitrary variable names at every step, or else overwriting variables. For such a short example, this is not problematic, but doing the same for a long pipeline of dozens of steps could get confusing, as well as dangerous due to the repeatedly modified variables.\nAnother possible solution is to rely on function composition (Section 3.2). Applying repeated composition is straightforward—in principle. In practice, when composing many functions together, things can get unwieldy quite quickly. Let us see what such a solution looks like:\n\ndistinct(\n  select(\n    filter(\n      mutate(snailDat, stdSize = (size - mean(size)) / sd(size)),\n      stdSize &gt; 2\n    ),\n    species\n  )\n)\n\n# A tibble: 2 × 1\n  species     \n  &lt;chr&gt;       \n1 unifasciatus\n2 nux         \n\n\nThe expression is highly unpleasant: to a human reader, it is not at all obvious what is happening above. It would be nice to clarify this workflow if possible.\nIt turns out that one can do this by making use of the pipe operator |&gt; from Section 3.3. As a reminder: for any function f and function argument x, f(x, y, ...) is the same as x |&gt; f(y, ...), where the ... denote potential further arguments to f. That is, the first argument of the function can be moved from the argument list to in front of the function, before the pipe symbol. The tidyverse functions take the data as their first argument, which means that the use of pipes allow us to very conveniently chain together multiple steps of data analysis. In our case, we can rewrite the above (quite confusing) code block in a much more transparent way:\n\nsnailDat |&gt;\n  mutate(stdSize = (size - mean(size)) / sd(size)) |&gt;\n  filter(stdSize &gt; 2) |&gt;\n  select(species) |&gt;\n  distinct()\n\nAgain, the pipe |&gt; should be pronounced then. We take the data, then we mutate it, then we filter for large-shelled individuals, then we select one of the columns, and then we remove all duplicated entries in that column. In performing these steps, each function both receives and returns data. Thus, by starting out with the original snailDat, we no longer need to write out the data argument of the functions explicitly. Instead, the pipe takes care of that automatically for us, making the functions receive as their first input the piped-in data, and in turn producing transformed data as their output—which becomes the input for the next function in line.\nIn fact, there is no need to even assign snailDat. The pipe can just as well start with read_csv to load the dataset:\n\nread_csv(\"island-FL.csv\") |&gt;\n  mutate(stdSize = (size - mean(size)) / sd(size)) |&gt;\n  filter(stdSize &gt; 2) |&gt;\n  select(species) |&gt;\n  distinct()\n\n# A tibble: 2 × 1\n  species     \n  &lt;chr&gt;       \n1 unifasciatus\n2 nux"
  },
  {
    "objectID": "Basic_data_wrangling.html#sec-wrangling-exercises",
    "href": "Basic_data_wrangling.html#sec-wrangling-exercises",
    "title": "5  Basic data manipulation",
    "section": "5.3 Exercises",
    "text": "5.3 Exercises\n\nThe Smith2003_data.txt dataset we worked with in Section 4.8 occasionally has the entry -999 in its last three columns. This stands for unavailable data. As discussed in Section 3.3, in R there is a built-in way of referring to such information: by setting a variable to NA. Modify these columns using mutate so that the entries which are equal to -999 are replaced with NA.\nAfter replacing -999 values with NA, remove all rows from the data which contain one or more NA values (hint: look up the function drop_na). How many rows are retained? And what was the original number of rows?\n\nThe iris dataset is a built-in table in R. It contains measurements of petal and sepal characteristics from three flower species belonging to the genus Iris (I. setosa, I. versicolor, and I. virginica). If you type iris in the console, you will see the dataset displayed. In solving the problems below, feel free to use the all-important dplyr cheat sheet.\n\nThe format of the data is not a tibble, but a data.frame. As mentioned in Chapter 4, the two are basically the same for practical purposes, though internally tibbles do offer some advantages. Convert the iris data frame into a tibble. (Hint: look up the as_tibble function.)\nVerify that there are indeed three distinct species in the data (hint: combine select and distinct in an appropriate way).\nSelect the columns containing petal and sepal length, and species identity.\nGet those rows of the data with petal length less than 4 cm, but sepal length greater than 4 cm.\nSort the data by increasing petal length, breaking ties by decreasing order of sepal length.\nCreate a new column called MeanLength. It should contain the average of the petal and sepal length (i.e., petal length plus sepal length, divided by 2) of each individual flower.\nPerform the operations from exercises 5-8 sequentially, in a single long function call, using function composition via pipes."
  },
  {
    "objectID": "Summaries_normalization.html#creating-summary-data",
    "href": "Summaries_normalization.html#creating-summary-data",
    "title": "6  Summary statistics and tidy data",
    "section": "6.1 Creating summary data",
    "text": "6.1 Creating summary data\nOne can create summaries of data using the summarise function. This will simply apply some function to a column. For example, to calculate the average population density of species 1 in pop, across both time and patches, one can write\n\npop |&gt; summarise(meanDensity1 = mean(species1))\n\n# A tibble: 1 × 1\n  meanDensity1\n         &lt;dbl&gt;\n1         5.30\n\n\nHere meanDensity1 is the name of the new column to be created, and the mean function is our summary function, collapsing the data into a single number.\nSo far, this is not particularly interesting; in fact, the exact same effect would have been achieved by typing the shorter mean(pop$species1) instead. The real power of summarise comes through when combined with group_by. This groups the data based on the given grouping variables. Let us see how this works in practice:\n\npop |&gt; group_by(patch)\n\n# A tibble: 100 × 5\n# Groups:   patch [2]\n    time patch species1 species2 species3\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 A         8.43     6.62    10.1 \n 2     1 B        10.1      3.28     6.27\n 3     2 A         7.76     6.93    10.3 \n 4     2 B        10.1      3.04     6.07\n 5     3 A         7.09     7.24    10.5 \n 6     3 B        10.1      2.8      5.82\n 7     4 A         6.49     7.54    10.6 \n 8     4 B        10.1      2.56     5.57\n 9     5 A         5.99     7.83    10.7 \n10     5 B        10.1      2.33     5.32\n# ℹ 90 more rows\n\n\nSeemingly nothing has happened; the only difference is the extra line of comment above, before the printed table, saying Groups: patch [2]. What this means is that the rows of the data were internally split into two groups. The first have \"A\" as their patch, and the second have \"B\". Whenever one groups data using group_by, rows which share the same unique combination of the grouping variables now belong together, and subsequent operations will act separately on each group instead of acting on the table as a whole (which is what we have been doing so far). That is, group_by does not actually alter the data; it only alters the behavior of the functions applied to the grouped data.\nIf we group not just by patch but also by time, the comment above the table will read Groups: patch, time [100]:\n\npop |&gt; group_by(patch, time)\n\n# A tibble: 100 × 5\n# Groups:   patch, time [100]\n    time patch species1 species2 species3\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 A         8.43     6.62    10.1 \n 2     1 B        10.1      3.28     6.27\n 3     2 A         7.76     6.93    10.3 \n 4     2 B        10.1      3.04     6.07\n 5     3 A         7.09     7.24    10.5 \n 6     3 B        10.1      2.8      5.82\n 7     4 A         6.49     7.54    10.6 \n 8     4 B        10.1      2.56     5.57\n 9     5 A         5.99     7.83    10.7 \n10     5 B        10.1      2.33     5.32\n# ℹ 90 more rows\n\n\nThis is because there are 100 unique combinations of patch and time: two different patch values (\"A\" and \"B\"), and fifty points in time (1, 2, …, 50). So we have “patch A, time 1” as group 1, “patch B, time 1” as group 2, “patch A, time 2” as group 3, and so on until “patch B, time 50” as our group 100.\nAs mentioned, functions that are applied to grouped data will act on the groups separately. To return to the example of calculating the mean population density of species 1 in the two patches, we can write:\n\npop |&gt;\n  group_by(patch) |&gt;\n  summarise(meanDensity1 = mean(species1))\n\n# A tibble: 2 × 2\n  patch meanDensity1\n  &lt;chr&gt;        &lt;dbl&gt;\n1 A             5.29\n2 B             5.32\n\n\nOne may obtain multiple summary statistics within the same summarise function. Below we compute both the mean and the standard deviation of the densities per patch:\n\npop |&gt;\n  group_by(patch) |&gt;\n  summarise(meanDensity1 = mean(species1), sdDensity1 = sd(species1))\n\n# A tibble: 2 × 3\n  patch meanDensity1 sdDensity1\n  &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 A             5.29      0.833\n2 B             5.32      3.81 \n\n\nLet us see what happens if we calculate the mean density of species 1—but grouping by time instead of patch:\n\npop |&gt;\n  group_by(time) |&gt;\n  summarise(meanDensity1 = mean(species1))\n\n# A tibble: 50 × 2\n    time meanDensity1\n   &lt;dbl&gt;        &lt;dbl&gt;\n 1     1         9.28\n 2     2         8.94\n 3     3         8.60\n 4     4         8.3 \n 5     5         8.04\n 6     6         7.84\n 7     7         7.68\n 8     8         7.54\n 9     9         7.44\n10    10         7.35\n# ℹ 40 more rows\n\n\nThe resulting table has 50 rows—half the number of rows in the original data, but many more than the two rows we get after grouping by patch. The reason is that there are 50 unique time points, and so the average is now computed over those rows which share time. But there are only two rows per moment of time: the rows corresponding to patch A and patch B. When we call summarise after having grouped by time, the averages are computed over the densities in these two rows only, per group. That is why here we end up with a table which has a single row per point in time.\n\n\n\n\n\n\nWarning\n\n\n\nOne common mistake when first encountering grouping and summaries is to assume that if we call group_by(patch), then the subsequent summaries will be taken over patches. This is not the case, and it is important to take a moment to understand why. When we apply group_by(patch), we are telling R to treat different patch values as group indicators. Therefore, when creating a summary, only the patch identities are retained from the original data, to which the newly calculated summary statistics are added. This means that the subsequent summaries are taken over everything except the patches. This should be clear after comparing the outputs of\n\npop |&gt; group_by(patch) |&gt; summarise(meanDensity1 = mean(species1))\n\nand\n\npop |&gt; group_by(time) |&gt; summarise(meanDensity1 = mean(species1))\n\nThe first distinguishes the rows of the data only by patch, and therefore the average is taken over time. The second distinguishes the rows by time, so the average is taken over the patches. Run the two expressions again to see the difference between them!\n\n\nWe can use functions such as mutate or filter on grouped data. For example, we might want to know the difference of species 1’s density from its average in each patch. Doing the following does not quite do what we want:\n\npop |&gt; mutate(species1Diff = species1 - mean(species1))\n\n# A tibble: 100 × 6\n    time patch species1 species2 species3 species1Diff\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n 1     1 A         8.43     6.62    10.1         3.13 \n 2     1 B        10.1      3.28     6.27        4.83 \n 3     2 A         7.76     6.93    10.3         2.46 \n 4     2 B        10.1      3.04     6.07        4.82 \n 5     3 A         7.09     7.24    10.5         1.79 \n 6     3 B        10.1      2.8      5.82        4.82 \n 7     4 A         6.49     7.54    10.6         1.19 \n 8     4 B        10.1      2.56     5.57        4.81 \n 9     5 A         5.99     7.83    10.7         0.685\n10     5 B        10.1      2.33     5.32        4.80 \n# ℹ 90 more rows\n\n\nThis will put the difference of species 1’s density from its mean density across both time and patches into the new column species1Diff. That is not the same as calculating the difference from the mean in a given patch—patch A for rows corresponding to patch A, and patch B for the others. To achieve this, all one needs to do is to group the data by patch before invoking mutate:\n\npop |&gt;\n  group_by(patch) |&gt;\n  mutate(species1Diff = species1 - mean(species1))\n\n# A tibble: 100 × 6\n# Groups:   patch [2]\n    time patch species1 species2 species3 species1Diff\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n 1     1 A         8.43     6.62    10.1         3.14 \n 2     1 B        10.1      3.28     6.27        4.81 \n 3     2 A         7.76     6.93    10.3         2.47 \n 4     2 B        10.1      3.04     6.07        4.80 \n 5     3 A         7.09     7.24    10.5         1.80 \n 6     3 B        10.1      2.8      5.82        4.80 \n 7     4 A         6.49     7.54    10.6         1.20 \n 8     4 B        10.1      2.56     5.57        4.79 \n 9     5 A         5.99     7.83    10.7         0.702\n10     5 B        10.1      2.33     5.32        4.78 \n# ℹ 90 more rows\n\n\nComparing this with the previous table, we see that the values in the species1Diff column are now different, because this time the differences are taken with respect to the average densities per each patch.\nFinally, since group_by changes subsequent behaviour, we eventually want to get rid of the grouping in our data. This can be done with the function ungroup. For example:\n\npop |&gt;\n  group_by(patch) |&gt;\n  mutate(species1Diff = species1 - mean(species1)) |&gt;\n  ungroup()\n\n# A tibble: 100 × 6\n    time patch species1 species2 species3 species1Diff\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n 1     1 A         8.43     6.62    10.1         3.14 \n 2     1 B        10.1      3.28     6.27        4.81 \n 3     2 A         7.76     6.93    10.3         2.47 \n 4     2 B        10.1      3.04     6.07        4.80 \n 5     3 A         7.09     7.24    10.5         1.80 \n 6     3 B        10.1      2.8      5.82        4.80 \n 7     4 A         6.49     7.54    10.6         1.20 \n 8     4 B        10.1      2.56     5.57        4.79 \n 9     5 A         5.99     7.83    10.7         0.702\n10     5 B        10.1      2.33     5.32        4.78 \n# ℹ 90 more rows\n\n\nIt is good practice to always ungroup the data after we have calculated what we wanted using the group structure. Otherwise, subsequent calculations could be influenced by the grouping in unexpected ways."
  },
  {
    "objectID": "Summaries_normalization.html#sec-tidy",
    "href": "Summaries_normalization.html#sec-tidy",
    "title": "6  Summary statistics and tidy data",
    "section": "6.2 Tidy data",
    "text": "6.2 Tidy data\nIn science, we often strive to work with tidy data (also known as long-format data or normal-form data). A dataset is tidy if:\n\nEach variable is in its own column;\nEach observation is in its own row.\n\nTidy data are suitable for performing operations, statistics, and plotting on. Furthermore, tidy data have a certain well-groomed feel to them, in the sense that their organization always follows the same general pattern regardless of the type of dataset one studies. Paraphrasing Tolstoy: tidy data are all alike; by contrast, every non-tidy dataset tends to be messy in its own unique way.\nThe tidyverse offers a simple and convenient way of putting data in tidy format. The pop table from the previous section is not tidy, because although each variable is in its own column, it is not true that each observation is in its own row. Instead, each row contains three observations: the densities of species 1, 2, and 3 at a given time and place. To tidy up these data, we create key-value pairs. We merge the columns for species densities into just two new ones. The first of these (the key) indicates whether it is species 1, or 2, or 3 which the given row refers to. The second column (the value) contains the population density of the given species. Such key-value pairs are created by the function pivot_longer:\n\npop |&gt;\n  pivot_longer(cols = c(species1, species2, species3),\n               names_to = \"species\",\n               values_to = \"density\")\n\n# A tibble: 300 × 4\n    time patch species  density\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1     1 A     species1    8.43\n 2     1 A     species2    6.62\n 3     1 A     species3   10.1 \n 4     1 B     species1   10.1 \n 5     1 B     species2    3.28\n 6     1 B     species3    6.27\n 7     2 A     species1    7.76\n 8     2 A     species2    6.93\n 9     2 A     species3   10.3 \n10     2 B     species1   10.1 \n# ℹ 290 more rows\n\n\nThe function pivot_longer takes three arguments in addition to the first (data) argument that we may pipe in, like above. First, cols is the list of columns to be converted into key-value pairs. It uses the same tidy selection mechanisms as the function select; see Section 5.1.1. (This means that cols = starts_with(\"species\") could also have been used.) Second, the argument names_to is the name of the new key column, specified as a character string. And third, values_to is the name of the new value column, also as a character string.\nThe above table is now in tidy form: each column records a single variable, and each row contains a single observation. Notice that, unlike the original pop which had 100 rows and 5 columns, the tidy version has 300 rows and 4 columns. This is natural: since the number of columns was reduced, there must be some extra rows to prevent the loss of information. And one should notice another benefit to casting the data in tidy format: it forces one to explicitly specify what was measured. By having named the value column density, we now know that the numbers 8.43, 6.62, etc. are density measurements. By contrast, it is not immediately obvious what these same numbers mean under the columns species1, species2, … in the original data.\nIt is possible to undo the effect pivot_longer. To do so, use pivot_wider:\n\npop |&gt;\n  pivot_longer(cols = starts_with(\"species\"),\n               names_to = \"species\",\n               values_to = \"density\") |&gt;\n  pivot_wider(names_from = \"species\", values_from = \"density\")\n\n# A tibble: 100 × 5\n    time patch species1 species2 species3\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 A         8.43     6.62    10.1 \n 2     1 B        10.1      3.28     6.27\n 3     2 A         7.76     6.93    10.3 \n 4     2 B        10.1      3.04     6.07\n 5     3 A         7.09     7.24    10.5 \n 6     3 B        10.1      2.8      5.82\n 7     4 A         6.49     7.54    10.6 \n 8     4 B        10.1      2.56     5.57\n 9     5 A         5.99     7.83    10.7 \n10     5 B        10.1      2.33     5.32\n# ℹ 90 more rows\n\n\nThe two named arguments of pivot_wider above are names_from (which specifies the column from which the names for the new columns will be taken), and values_from (the column whose values will be used to fill in the rows under those new columns).\nAs a remark, one could make the data even “wider”, by not only making columns out of the population densities, but the densities at a given patch. Doing so is simple: one just needs to specify both the species and patch columns from which the new column names will be compiled.\n\npop |&gt;\n  pivot_longer(cols = starts_with(\"species\"),\n               names_to = \"species\",\n               values_to = \"density\") |&gt;\n  pivot_wider(names_from = c(\"species\", \"patch\"), values_from = \"density\")\n\n# A tibble: 50 × 7\n    time species1_A species2_A species3_A species1_B species2_B species3_B\n   &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1     1       8.43       6.62      10.1        10.1       3.28       6.27\n 2     2       7.76       6.93      10.3        10.1       3.04       6.07\n 3     3       7.09       7.24      10.5        10.1       2.8        5.82\n 4     4       6.49       7.54      10.6        10.1       2.56       5.57\n 5     5       5.99       7.83      10.7        10.1       2.33       5.32\n 6     6       5.58       8.1       10.7        10.1       2.12       5.08\n 7     7       5.27       8.34      10.6        10.1       1.92       4.86\n 8     8       5.02       8.54      10.4        10.1       1.74       4.64\n 9     9       4.82       8.7       10.0        10.0       1.58       4.43\n10    10       4.66       8.82       9.66       10.0       1.43       4.23\n# ℹ 40 more rows\n\n\nIf tidy data are what we strive for, what is the practical use of pivot_wider? There are two answers to this question. First, while non-tidy data are indeed less efficient from a computational and data analysis standpoint, they are often more human-readable. For example, the pop table is easy to read despite the lack of tidiness, because each row corresponds to a given time and place. By tidying the data, information referring to any given time and place will be spread out over multiple (in our case, three) rows—one for each species. While this is preferable from a data analysis point of view, it can be more difficult to digest visually. Second, wide data lend themselves very well to a class of statistical techniques called multivariate analysis. In case one wants to perform multivariate analysis, wide-format data are often better than tidy data. We will see an example application of this in Section 14.1.\nFinally, it is worth noting the power of tidy data in, e.g., generating summary statistics. To obtain the mean and the standard deviation of the population densities for each species in each patch, all one has to do is this:\n\npop |&gt;\n  # Tidy up the data:\n  pivot_longer(cols = starts_with(\"species\"),\n               names_to = \"species\",\n               values_to = \"density\") |&gt;\n  # Group data by both species and patch:\n  group_by(patch, species) |&gt;\n  # Obtain statistics:\n  summarise(meanDensity = mean(density), sdDensity = sd(density)) |&gt;\n  # Don't forget to ungroup the data at the end:\n  ungroup()\n\n# A tibble: 6 × 4\n  patch species  meanDensity sdDensity\n  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 A     species1        5.29     0.833\n2 A     species2        8.05     0.559\n3 A     species3        7.51     1.56 \n4 B     species1        5.32     3.81 \n5 B     species2        1.07     0.737\n6 B     species3        6.57     2.48"
  },
  {
    "objectID": "Summaries_normalization.html#exercises",
    "href": "Summaries_normalization.html#exercises",
    "title": "6  Summary statistics and tidy data",
    "section": "6.3 Exercises",
    "text": "6.3 Exercises\nThe first set of problems relies on the iris dataset—the same that we used in the previous chapter’s exercises (Section 5.3). Convert the iris data to a tibble with the as_tibble function, and assign it to a variable.\n\nCreate a new column in the iris dataset which contains the difference of petal lengths from the average of the whole dataset.\nCreate a new column in the iris dataset which contains the difference of petal lengths from the average of each species. (Hint: group_by the species and then mutate!)\nCreate a table where the rows are the three species, and the columns are: average petal length, variance of petal length, average sepal length, and variance of sepal length (each across flowers of the corresponding species).\nCreate key-value pairs in the iris dataset for the petal characteristics. In other words, have a column called Petal.Trait (whose values are either Petal.Length or Petal.Width), and another column called Petal.Value (with the length/width values).\nRepeat the same exercise, but now for sepal traits.\nFinally, do it for both petal and sepal traits simultaneously, to obtain a fully tidy form of the iris data. That is, the key column (call it Flower.Trait) will have the values Petal.Length, Petal.Width, Sepal.Length, and Sepal.Width. And the value column (which you can call Trait.Value) will have the corresponding measurements.\n\nThe subsequent exercises use the Galápagos land snail data from the previous two chapters (see Section 4.2.2 to review the description of the data).\n\nWhat is the average shell size of the whole community? What is its standard deviation? How about the average and standard deviation of shell shape?\nWhat is the average shell size / shell shape of the community in each habitat type (humid/arid)?\nWhat is the average shell size / shell shape in each unique combination of species and habitat type?\nBased on your answer to the previous question: how many species are there which live in both arid and humid environments?\nOrganize the size and shape columns in key-value pairs: instead of the original size and shape, have a column called trait (which will either be \"size\" or \"shape\") and another column called value which holds the corresponding measurement."
  },
  {
    "objectID": "Creating_figures.html#general-philosophy",
    "href": "Creating_figures.html#general-philosophy",
    "title": "7  Creating publication-grade figures",
    "section": "7.1 General philosophy",
    "text": "7.1 General philosophy\nIn science, we want clear and informative plots. Each figure should make it obvious what data you are plotting, what the axes, colors, shapes, and size differences represent, and the overall message the figure is conveying. When writing a scientific paper or report, remember that your future readers are busy people. They often do not have the time to delve into the subtleties of overly refined verbal arguments. Instead, they will most often look for the figures to learn what your work is about. You will want to create figures which makes this possible for them to do.\nHere we will learn how to create accessible, publication-quality scientific graphs in a simple way. We do this using the R package ggplot2 which is a standard part of the tidyverse. The ggplot2 package follows a very special philosophy for creating figures that was originally proposed by Leland Wilkinson (2006). The essence of this view is that, just like the grammar of sentences, graphs have fixed “grammatical” components whose specification defines the plot. The grand idea is that the data ought not be changed in order to display it in different formats. For instance, the same data should be possible to represent either as a box plot, or as a histogram, without changing their format.\nThis last claim needs to be qualified somewhat. It is more accurate to say that one should not need to change the data as long as they are in tidy format. As a reminder, “tidy data” means that every variable is in its own column, and every observation is in its own row (Section 6.2). In case the data are not tidy, one should first wrangle them into such form, for example by using pivot_longer. While this step is not always required (especially for simpler graphs), it can be very useful to tidy the data before analyzing and plotting them when working with larger, more complex datasets."
  },
  {
    "objectID": "Creating_figures.html#basic-ggplot2-usage",
    "href": "Creating_figures.html#basic-ggplot2-usage",
    "title": "7  Creating publication-grade figures",
    "section": "7.2 Basic ggplot2 usage",
    "text": "7.2 Basic ggplot2 usage\nTo see how ggplot2 works, let us load tidyverse, and then use the built-in iris dataset to create some figures. As a reminder, here is what the data look like:\n\nlibrary(tidyverse)\nas_tibble(iris)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\n\nLet us, as a first step, create a plot where sepal length (x-axis) is plotted against petal length (y-axis), with the points referring to different species shown in different colors:\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length, colour = Species) +\n  geom_point()\n\n\n\n\nHere we defined the data (iris; feel free to use pipes, as in iris |&gt; ggplot() + ...), then the aesthetic mappings with aes(), and finally the geometry of how to display the data with geom_point(). The important thing to remember is that the aesthetic mappings are all those aspects of the figure that are governed by the data. For instance, if you wanted to set the color of all points to blue, this would not be an aesthetic mapping, because it applies regardless of what the data are (in case you want to do this, you would have to specify geom_point(colour = \"blue\") in the last line). The geometry of your plot governs the overall visual arrangement of your data (points, lines, histograms, etc). There are many different geom_s; we will learn about some here, but when in doubt, Google and a ggplot2 cheat sheet are your best friends.\nTo look at a different kind of geometry, let us create a histogram of the petal lengths. This is done using geom_histogram:\n\niris |&gt;\n  ggplot() +\n  aes(x = Petal.Length) +\n  geom_histogram()\n\n\n\n\nBefore looking at the result, notice that the different “grammatical” components of the plot (aesthetics, geometry) are added to the plot, using the + symbol for addition. A common source of error is to accidentally keep using the pipe operator |&gt; even within a plot. The rule of thumb is that after invoking ggplot, one must use + to compose the various graph elements, but outside of that, the usual |&gt; is used for function composition. If one uses |&gt; instead of + within a plot, R will give back a confusing message instead of graphics:\n\niris |&gt;\n  ggplot() |&gt;\n  aes(x = Petal.Length) |&gt;\n  geom_histogram()\n\nmapping: x = ~Petal.Length, y = ~ggplot(iris) \ngeom_bar: na.rm = FALSE, orientation = NA\nstat_bin: binwidth = NULL, bins = NULL, na.rm = FALSE, orientation = NA, pad = FALSE\nposition_stack \n\n\nJust be sure to remember this so you can correct the mistake, should you accidentally run up against it.\nLet us now look at the plot above. We see two clusters of data. Why is that? One might suspect that this is because of a species-level difference. To check if that is the case, let us color the histogram by species:\n\niris |&gt;\n  ggplot() +\n  aes(x = Petal.Length, colour = Species) +\n  geom_histogram()\n\n\n\n\n(A color legend was created automatically on the right.) This changes the color of the outline of the histograms, but not the color of their fill. To do so, we need to change the fill color as well, which is a separate aesthetic property:\n\niris |&gt;\n  ggplot() +\n  aes(x = Petal.Length, colour = Species, fill = Species) +\n  geom_histogram()\n\n\n\n\nThis is fine, but now the histogram bars of different species are stacked on top of one another. This means that, for example at around a petal length of 5 cm where individuals of both Iris versicolor and I. virginica are found, the green bar on top of the blue one does not begin from the bottom of the y-axis at 0, but from wherever the blue bar ends and the green one begins (in our case, from around 10 upwards). So the number of I. versicolor individuals in the 5 cm bin is not 12, but only 2.\nIt may be easier to interpret the histogram if all bars start at the bottom, even if this means that the bars will now overlap to an extent. To achieve this overlap, one simply has to add the argument position = \"identity\" to geom_histogram:\n\niris |&gt;\n  ggplot() +\n  aes(x = Petal.Length, colour = Species, fill = Species) +\n  geom_histogram(position = \"identity\")\n\n\n\n\n(There are other options as well, such as position = \"dodge\" – feel free to experiment. In general, you can learn what options each of the geom_s have by taking a look at their help pages, or by asking Google.) The figure above is almost perfect; the one remaining problem is that the bars belonging to different species cover each other completely. So let us change the fill’s transparency, which is called alpha in ggplot. The alpha property can take on any value between 0 (fully transparent object, to the point of invisibility) to 1 (fully solid object with no transparency, like above; this is the default). Setting it to 0.2 (say) will finally fully reveal the distribution of each species:\n\niris |&gt;\n  ggplot() +\n  aes(x = Petal.Length, colour = Species, fill = Species) +\n  geom_histogram(position = \"identity\", alpha = 0.2)\n\n\n\n\nThere are plenty of other geom_s as well, such as geom_boxplot, geom_violin, etc. Let us try geom_boxplot for instance. Let us create one box plot of the distribution of petal lengths for each species. Putting the species along the x-axis and petal length along the y-axis:\n\niris |&gt;\n  ggplot() +\n  aes(x = Species, y = Petal.Length) +\n  geom_boxplot()\n\n\n\n\nAlthough this is sufficient, one should feel free to make the plots prettier. For instance, one could use colors, like before. One can also use different themes. There are pre-defined themes such as theme_classic(), theme_bw(), and so on. For example, using theme_bw() gets rid of the default gray background and replaces it with a white one. The version of the graph below applies this theme, and also changes the color and fill to be based on species identity:\n\niris |&gt;\n  ggplot() +\n  aes(x = Species, y = Petal.Length, colour = Species, fill = Species) +\n  geom_boxplot(alpha = 0.2) +\n  theme_bw()"
  },
  {
    "objectID": "Creating_figures.html#sec-CI",
    "href": "Creating_figures.html#sec-CI",
    "title": "7  Creating publication-grade figures",
    "section": "7.3 Summaries and confidence intervals",
    "text": "7.3 Summaries and confidence intervals\nProviding appropriate information on experimental errors is a hallmark of any credible scientific graph. Choose a type of error based on the conclusion that you want the reader to draw. While the standard deviation represents the dispersion of the data, the standard error and confidence intervals report the certainty of the estimate of a value (e.g., certainty in estimating the mean). Let us see examples of each.\nLet us first obtain the mean and the standard deviation of the petal lengths for each species. We then would like to plot them. To do this, we proceed by creating a workflow which first uses group_by and summarise to obtain the required statistics (means and standard deviations), and then uses these data for plotting. One can include all these steps in a logical workflow:\n\niris |&gt;\n  group_by(Species) |&gt; # Perform summary calculations for each species\n  summarise(mP = mean(Petal.Length), # Mean petal length\n            sP = sd(Petal.Length)) |&gt; # Standard deviation of petal length\n  ungroup() |&gt; # Ungroup data\n  ggplot() + # Start plotting\n  aes(x = Species, y = mP, ymin = mP - sP, ymax = mP + sP) + # Mean +/- sd\n  geom_point() + # This takes the y aesthetic, for plotting the mean\n  geom_errorbar(width = 0.2) # Takes the ymin and ymax aesthetics\n\n\n\n\nNote that the y-axis label is not very descriptive. This is because it inherited the name of the corresponding data column, mP. There are multiple ways to change it; the simplest is to add ylab(\"Petal length\") to the plot. Another way of doing so is discussed in Chapter 8.\nIn case we want to calculate the 95% confidence intervals of the mean values, we first obtain some necessary summary statistics: the number of observations (sample size) in each group; the standard error of the mean (standard deviation divided by the square root of the sample size); and finally, the confidence interval itself (read off from the t-distribution, with one fewer degrees of freedom than the sample size). We can then include these confidence intervals on top of the mean values:\n\niris |&gt;\n  group_by(Species) |&gt;\n  summarise(mP = mean(Petal.Length), # Mean petal length per species\n            sP = sd(Petal.Length), # Standard deviation per species\n            N = n(), # Sample size (number of observations) per species\n            SEM = sP / sqrt(N), # Standard error of the mean\n            CI = SEM * qt(1 - 0.025, N - 1)) |&gt; # Confidence interval,\n                 # read off at 1 - 0.025 because +/- 2.5% adds up to 5%\n  ggplot() +\n  aes(x = Species, y = mP, ymin = mP - CI, ymax = mP + CI) +\n  geom_point(colour = \"steelblue\", fill = \"steelblue\") +\n  geom_errorbar(width = 0.2, colour = \"steelblue\") +\n  ylab(\"Petal length\") +\n  theme_bw()\n\n\n\n\nIn the literature, it is common to encounter bar-and-whisker plots to represent the same information as above. These replace the points showing the means with bars that start from zero. One should be aware of how to read and make such graphs. This is almost trivially simple: all one needs to do is replace geom_point with geom_col.\n\niris |&gt;\n  group_by(Species) |&gt;\n  summarise(mP = mean(Petal.Length),\n            sP = sd(Petal.Length),\n            N = n(),\n            SEM = sP / sqrt(N),\n            CI = SEM * qt(1 - 0.025, N - 1)) |&gt;\n  ggplot() +\n  aes(x = Species, y = mP, ymin = mP - CI, ymax = mP + CI) +\n  # Below we change geom_point to geom_col (for \"column\"):\n  geom_col(alpha = 0.4, colour = \"steelblue\", fill = \"steelblue\") +\n  geom_errorbar(width = 0.2, colour = \"steelblue\") +\n  ylab(\"Petal length\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhile means and their confidence intervals are often displayed with bar-and-whisker plots, there is an important reasons why this is not a good idea. By starting the bars from zero, the plot implies that zero is a natural point of comparison for all the data. Unfortunately, this can visually distort the information we wish to convey. Consider the following graph:\n\ntibble(Species = c(\"species 1\", \"species 2\"),\n       Average = c(148, 152),\n       CI = c(0.8, 0.9)) |&gt;\n  ggplot() +\n  aes(x = Species, y = Average, ymin = Average - CI, ymax = Average + CI) +\n  geom_col(alpha = 0.4, colour = \"steelblue\", fill = \"steelblue\") +\n  geom_errorbar(width = 0.2, colour = \"steelblue\") +\n  ylab(\"Trait value\") +\n  theme_bw()\n\n\n\n\nIt is impossible to see whether there are any relevant differences between the two species. The following is exactly the same, but with the mean values shown with points instead of bars:\n\ntibble(Species = c(\"species 1\", \"species 2\"),\n       Average = c(148, 152),\n       CI = c(0.8, 0.9)) |&gt;\n  ggplot() +\n  aes(x = Species, y = Average, ymin = Average - CI, ymax = Average + CI) +\n  geom_point(colour = \"steelblue\") +\n  geom_errorbar(width = 0.2, colour = \"steelblue\") +\n  ylab(\"Trait value\") +\n  theme_bw()\n\n\n\n\nIt is now obvious that the two observations are distinct. Due to this problem, my recommendation is to avoid bar-and-whisker plots and simply use point-and-whisker plots instead."
  },
  {
    "objectID": "Creating_figures.html#sec-ggplot-exercises",
    "href": "Creating_figures.html#sec-ggplot-exercises",
    "title": "7  Creating publication-grade figures",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\nFauchald et al. (2017) tracked the population size of various herds of caribou in North America over time, and correlated population cycling with the amount of vegetation and sea-ice cover. The part of their data that we will use consists of two files: pop_size.tsv (data on herd population sizes), and sea_ice.tsv (on sea levels of sea ice cover per year and month).\n\nThe file sea_ice.tsv is in human-readable, wide format. Note however that the rule “each set of observations is stored in its own row” is violated. We would like to organize the data in a tidy tibble with four columns: Herd, Year, Month, and Cover. To this end, apply the function pivot_longer to columns 3-14 in the tibble, gathering the names of the months in the new column Month and the values in the new column Cover.\nUse pop_size.tsv to make a plot of herd sizes through time. Let the x-axis be Year, the y-axis be population size. Show different herds in different colors. For the geometry, use points.\nThe previous plot is actually not that easy to see and interpret. To make it better, add a line geometry as well, which will connect the points with lines.\nMake a histogram out of all population sizes in the data.\nMake the same histogram, but break it down by herd, using a different color and fill for each herd.\nInstead of a histogram, make a density plot with the same data and display (look up geom_density if needed).\nMake box plots of the population size of each herd. Along the x-axis, each herd should be separately displayed; the y-axis should be population size. The box plots should summarize population sizes across all years.\nLet us go back to sea_ice.tsv. Make the following plot. Along the x-axis, have Year. Along the y-axis, Month. Then, for each month-year pair, color the given part of the plot darker for lower ice cover and lighter for more. (Hint: look up geom_tile if needed.) Finally, make sure to do all this only for the herd with the label WAH (filter the data before plotting).\n\nThe following plotting exercises use the Galápagos land snail data (Section 4.2.2).\n\nCreate a density plot (geom_density) with shell size along the x-axis and the corresponding density of individuals along the y-axis. How much overlap is there between the sizes of the different species?\nRepeat the above exercise, but use the standardized size instead of the raw measurements. As a reminder, this means subtracting out the mean from each entry, and dividing the results by the standard deviation (Section 5.1.6). Make sure to have clean, descriptive axis labels on the plot. What is the difference between this figure and the one obtained in the previous exercise using the non-standardized size values?\nNow do exercises 9-10 for shell shape instead of size.\nCreate a scatterplot with size along the x-axis and shape along the y-axis. Each individual should be a point on the graph.\nRepeat the same, but use the standardized size and shape measurements along the axes. Compare the figures. In what do they differ?\nCreate the same scatterplot with standardized size along the x-axis and shape along the y-axis, but with the points colored based on habitat. That is, individuals found in humid environments should all have one color, and those found in arid regions should have another. Do you see any patterns, in terms of whether certain shell sizes or shapes are more associated with a given type of habitat?\nModify the previous plot by coloring the points based on species instead of habitat. How much trait overlap is there between individuals belonging to different species? How do you interpret this result, especially in light of what you previously saw in exercises 9-11?\n\n\n\n\n\nFauchald, Per, Taejin Park, Hans Tømmervik, Ranga Myneni, and Vera Helene Hausner. 2017. “Arctic greening from warming promotes declines in caribou populations.” Science Advances 3 (4): e1601365. https://doi.org/10.1126/sciadv.1601365.\n\n\nWilkinson, Leland. 2006. The Grammar of Graphics. Secaucus, NJ, USA: Springer Science & Business Media."
  },
  {
    "objectID": "Further_plotting_options.html#sec-smooth",
    "href": "Further_plotting_options.html#sec-smooth",
    "title": "8  Some further plotting options; introducing factors",
    "section": "8.1 Smoothing and regression lines",
    "text": "8.1 Smoothing and regression lines\nIn Chapter 7 we learned about aesthetic mappings and various geom_ options such as geom_point, geom_histogram, and geom_boxplot. Let us explore another type of geom_, which approximates the trend of a set of data points with a line and an error bar that shows the confidence interval of the estimate at each point:\n\nlibrary(tidyverse)\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\nWhile such fits are occasionally useful, we often want a linear least-squares regression on our data. To get such a linear fit, add the argument method = lm to geom_smooth() (lm stands for “linear model”):\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\nLinear regression lines are usually shown without the confidence intervals (the gray band around the regression line). To drop this, set se = FALSE:\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE)\n\n\n\n\nWhat happens if we color the data points by species? Let us add colour = Species to the list of aesthetic mappings:\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length, colour = Species) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE)\n\n\n\n\nNow the regression line is automatically fitted to the data within each of the groups separately—a highly useful behavior.\nNotice also that a color legend was automatically created and appended to the right of the graph. This legend positioning is the default in ggplot2. You can move the legend to another position by specifying the legend.position option within the theme function that can be added onto the plot:\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length, colour = Species) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  theme(legend.position = \"left\") # Or: \"top\", \"bottom\", \"right\", \"none\"\n\n\n\n\nSpecifying legend.position = \"none\" omits the legend altogether.\nA word of caution: in case the legend positioning is matched with a generic theme such as theme_bw(), one should put the legend position after the main theme definition. The reason is that pre-defined themes like theme_bw() override any specific theme options you might specify. The rule of thumb is: any theme() component to your plot should be added only after the generic theme definition. Otherwise the theme() component will be overridden and will not take effect. For example, this does not work as intended:\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length, colour = Species) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  theme(legend.position = \"left\") + # Position legend at the left\n  theme_bw() # Define general theme - and thus override the line above...\n\n\n\n\nBut this one does:\n\nggplot(iris) +\n  aes(x = Sepal.Length, y = Petal.Length, colour = Species) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  theme_bw() + # This defines the general theme\n  theme(legend.position = \"left\") # Now override default legend positioning"
  },
  {
    "objectID": "Further_plotting_options.html#scales",
    "href": "Further_plotting_options.html#scales",
    "title": "8  Some further plotting options; introducing factors",
    "section": "8.2 Scales",
    "text": "8.2 Scales\nThe aesthetic mappings of a graph (x-axis, y-axis, color, fill, size, shape, alpha, …) are automatically rendered into the displayed plot, based on certain default settings within ggplot2. These defaults can be altered, however. Consider the following bare-bones plot:\n\niris |&gt;\n  ggplot() +\n  aes(x = Species, y = Petal.Length) +\n  geom_boxplot()\n\n\n\n\nWe can now change, for example, how the y-axis is displayed. The component to be added to the plot is scale_y_continuous(). Here scale means we are going to change the scaling of some aesthetic mapping, y refers to the y-axis (as expected, it can be replaced with x, colour, fill, etc.), and continuous means that the scaling of the axis is not via discrete values (e.g., either 1 or 2 or 3 but nothing in between), but continuous (every real number is permissible along the y-axis). The plot component scale_y_continuous() takes several arguments; take a look at its help page to see all possible options. Here we mention a few of them. First, there is the name option, which is used to relabel the axis. The limits argument accepts a vector of two values, containing the lower and upper limits of the plot. If any of them is set to NA, the corresponding limit will be determined automatically. Next, the breaks argument controls where the tick marks along the axis go. It is given as a vector, with its entries corresponding to the y-coordinates of the tick marks. Finally, labels determines what actually gets written on the axis at the tick mark points—it is therefore also a vector, its length matching that of breaks.\nAs an example, let us scale the y-axis of the previous graph in the following way. The axis label should read “Petal length [cm]”, instead of the current “Petal.Length”. It should go from 0 to 7, with a break at those two values and also halfway in between at 3.5. Here is how to do this:\n\niris |&gt;\n  ggplot() +\n  aes(x = Species, y = Petal.Length) +\n  geom_boxplot() +\n  scale_y_continuous(name = \"Petal length [cm]\",\n                     limits = c(0, 7),\n                     breaks = c(0, 3.5, 7))\n\n\n\n\nWhat should we do if, for some reason, we would additionally like the “3.5” in the middle to be displayed as “7/2” instead (an exact value)? In that case, we can add an appropriate labels option as an argument to scale_y_continuous:\n\niris |&gt;\n  ggplot() +\n  aes(x = Species, y = Petal.Length) +\n  geom_boxplot() +\n  scale_y_continuous(name = \"Petal length [cm]\",\n                     limits = c(0, 7),\n                     breaks = c(0, 3.5, 7),\n                     labels = c(\"0\", \"7/2\", \"7\"))\n\n\n\n\nThe x-axis can be scaled similarly. One important difference though is that here the x-axis has a discrete scale. Since we are displaying the species along it, any value must be either setosa or versicolor or virginica; it makes no sense to talk about what is “halfway in between setosa and versicolor”. Therefore, one should use scale_x_discrete(). Its options are similar to those of scale_x_continuous(). For instance, let us override the axis label, spelling out that the three species belong to the genus Iris:\n\niris |&gt;\n  ggplot() +\n  aes(x = Species, y = Petal.Length) +\n  geom_boxplot() +\n  scale_y_continuous(name = \"Petal length [cm]\",\n                     limits = c(0, 7),\n                     breaks = c(0, 3.5, 7),\n                     labels = c(\"0\", \"7/2\", \"7\")) +\n  scale_x_discrete(name = \"Species (genus: Iris)\")\n\n\n\n\nAlternatively, one could also redefine the labels and get an equally good graph:\n\niris |&gt;\n  ggplot() +\n  aes(x = Species, y = Petal.Length) +\n  geom_boxplot() +\n  scale_y_continuous(name = \"Petal length [cm]\",\n                     limits = c(0, 7),\n                     breaks = c(0, 3.5, 7),\n                     labels = c(\"0\", \"7/2\", \"7\")) +\n  scale_x_discrete(labels = c(\"Iris setosa\", \"Iris versicolor\",\n                              \"Iris virginica\"))\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn case you would like to display the species names in italics, as is standard requirement when writing binomial nomenclature, feel free to add theme(axis.text.x = element_text(face = \"italic\")) to the end of the plot. We will not be going into more detail on tweaking themes, but feel free to explore the possibilities by looking at the help pages or Googling them.\n\n\nOther aesthetic mappings can also be adjusted, such as colour, fill, size, or alpha. One useful way to do it is through scale_colour_manual(), scale_fill_manual(), and so on. These are like scale_colour_discrete(), scale_fill_discrete() etc., except that they allow one to specify a discrete set of values by hand. Let us do this for color and fill:\n\niris |&gt;\n  ggplot() +\n  aes(x = Species, y = Petal.Length, colour = Species, fill = Species) +\n  geom_boxplot(alpha = 0.2) +\n  scale_y_continuous(name = \"Petal length [cm]\",\n                     limits = c(0, 7),\n                     breaks = c(0, 3.5, 7),\n                     labels = c(\"0\", \"7/2\", \"7\")) +\n  scale_x_discrete(labels = c(\"Iris setosa\", \"Iris versicolor\",\n                              \"Iris virginica\")) +\n  scale_colour_manual(values = c(\"steelblue\", \"goldenrod\", \"forestgreen\")) +\n  scale_fill_manual(values = c(\"steelblue\", \"goldenrod\", \"forestgreen\"))\n\n\n\n\nWe used the built-in color names \"steelblue\", \"goldenrod\", and \"forestgreen\" above. A useful R color cheat sheet can be found here, for more options and built-in color names."
  },
  {
    "objectID": "Further_plotting_options.html#sec-factors",
    "href": "Further_plotting_options.html#sec-factors",
    "title": "8  Some further plotting options; introducing factors",
    "section": "8.3 Reordering labels using factors",
    "text": "8.3 Reordering labels using factors\nIn the previous examples, the order of text labels was always automatically determined. The basic rule in R is that the ordering follows the alphabet: the default is for setosa to precede versicolor, which will be followed by virginica. This default ordering can be inconvenient, however. Consider the data file temperature_Linkoping.csv of average temperatures per month, measured in the town of Linköping, Sweden:\n\nread_csv(\"temperature_Linkoping.csv\")\n\n# A tibble: 12 × 2\n   month average_temperature_C\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 Jan                    -1.9\n 2 Feb                    -1.7\n 3 Mar                     0.9\n 4 Apr                     6.3\n 5 May                    11.5\n 6 Jun                    15.4\n 7 Jul                    17.9\n 8 Aug                    16.6\n 9 Sep                    12.8\n10 Oct                     7.5\n11 Nov                     3.4\n12 Dec                     0.1\n\n\nThe table has two columns: month and average_temperature_C, giving the mean temperature in each month across years 1991-2021.1 So far so good. However, if we plot this with months along the x-axis and temperature along the y-axis, we run into trouble because R displays items by alphabetical instead of chronological order:\n\nread_csv(\"temperature_Linkoping.csv\") |&gt;\n  ggplot() +\n  aes(x = month, y = average_temperature_C) +\n  geom_point(colour = \"steelblue\") +\n  scale_y_continuous(name = \"average temperature (Celsius)\") +\n  theme_bw()\n\n\n\n\nTo fix this, one must convert the type of month from a simple vector of character strings to a vector of factors. Factors are categorical variables (i.e., take on well-defined distinct values instead of varying on a continuous scale like double-precision numbers), but with an extra attribute which determines the order of those values. This ordering is often referred to as the levels of the factor. The first of the values has level 1, the next one level 2, and so on.\nOne very convenient way of assigning factor levels is through the tidyverse function as_factor.2 This function takes a vector of values and, if the values are numeric, assigns them levels based on those numerical values. However, if the values are character strings, then the levels are assigned in order of appearance within the vector. This is perfect for us, because the months are in proper order already within the tibble:\n\nread_csv(\"temperature_Linkoping.csv\") |&gt;\n  mutate(month = as_factor(month)) |&gt;\n  ggplot() +\n  aes(x = month, y = average_temperature_C) +\n  geom_point(colour = \"steelblue\") +\n  scale_y_continuous(name = \"average temperature (Celsius)\") +\n  theme_bw()\n\n\n\n\nIt is also possible to take a factor and reassign its levels manually. This can be done with the fct_relevel function:\n\nread_csv(\"temperature_Linkoping.csv\") |&gt;\n  mutate(month = fct_relevel(month, \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n                             \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")) |&gt;\n  ggplot() +\n  aes(x = month, y = average_temperature_C) +\n  geom_point(colour = \"steelblue\") +\n  scale_y_continuous(name = \"average temperature (Celsius)\") +\n  theme_bw()\n\n\n\n\nAlso, the use of fct_relevel need not be this laborious. If all we want to do is place a few factor levels to be first ones without changing any of the others, it is possible to enter just their names. Often, for example, a factor column holds various experimental treatments, one of which is called \"control\". In that case, all we might want to do is to make the control be the first factor level, without altering any of the others. If treatment is the name of the vector (or column in a data frame) that holds the different experimental treatment names, then this can be done with fct_relevel(treatment, \"control\")."
  },
  {
    "objectID": "Further_plotting_options.html#sec-faceting",
    "href": "Further_plotting_options.html#sec-faceting",
    "title": "8  Some further plotting options; introducing factors",
    "section": "8.4 Facets",
    "text": "8.4 Facets\nPlots can be faceted (subplots created and arranged in a grid layout) based on some variable or variables. For instance, let us create histograms of petal lengths in the iris dataset, like we did last time:\n\niris |&gt;\n  ggplot() +\n  aes(x = Petal.Length) +\n  geom_histogram()\n\n\n\n\nThis way, one cannot see which part of the histogram belongs to which species. One fix to this is to color the histogram by species—this is what we have done before. Another is to separate the plot into three facets, each displaying data for one of the species only:\n\niris |&gt;\n  ggplot() +\n  aes(x = Petal.Length) +\n  geom_histogram() +\n  facet_grid(. ~ Species)\n\n\n\n\nThe component facet_grid(x ~ y) means that the data will be grouped based on columns x and y, with the distinct values of column x making up the rows and those of column y the columns of the grid of plots. If one of them is replaced with a dot (as above), then that variable is ignored, and only the other variable is used in creating a row (or column) of subplots. So, to display the same data but with the facets arranged in one column instead of one row, we simply replace facet_grid(. ~ Species) with facet_grid(Species ~ .):\n\niris |&gt;\n  ggplot() +\n  aes(x = Petal.Length) +\n  geom_histogram() +\n  facet_grid(Species ~ .)\n\n\n\n\nIn this particular case, the above graph is preferable to the previous one, because the three subplots now share the same x-axis. This makes it easier to compare the distribution of petal lengths across the species.\nTo illustrate how to make a two-dimensional grid of facets, let us put the iris dataset in tidy form using pivot_longer():\n\nas_tibble(iris) |&gt;\n  pivot_longer(cols = c(Sepal.Length,Sepal.Width,Petal.Length,Petal.Width),\n               names_to = \"Trait\",\n               values_to = \"Measurement\")\n\n# A tibble: 600 × 3\n   Species Trait        Measurement\n   &lt;fct&gt;   &lt;chr&gt;              &lt;dbl&gt;\n 1 setosa  Sepal.Length         5.1\n 2 setosa  Sepal.Width          3.5\n 3 setosa  Petal.Length         1.4\n 4 setosa  Petal.Width          0.2\n 5 setosa  Sepal.Length         4.9\n 6 setosa  Sepal.Width          3  \n 7 setosa  Petal.Length         1.4\n 8 setosa  Petal.Width          0.2\n 9 setosa  Sepal.Length         4.7\n10 setosa  Sepal.Width          3.2\n# ℹ 590 more rows\n\n\nAs seen, now the Measurement in every row is characterized by two other variables: Species and Trait (i.e., whether the given value refers to the sepal length, petal width etc. of the given species). We can create a histogram of each measured trait for each species now, in a remarkably simple way:\n\nas_tibble(iris) |&gt;\n  pivot_longer(cols = c(Sepal.Length,Sepal.Width,Petal.Length,Petal.Width),\n               names_to = \"Trait\",\n               values_to = \"Measurement\") |&gt;\n  ggplot() +\n  aes(x = Measurement) +\n  geom_histogram() +\n  facet_grid(Species ~ Trait)"
  },
  {
    "objectID": "Further_plotting_options.html#saving-plots",
    "href": "Further_plotting_options.html#saving-plots",
    "title": "8  Some further plotting options; introducing factors",
    "section": "8.5 Saving plots",
    "text": "8.5 Saving plots\nTo save the most recently created ggplot figure, simply type\n\nggsave(filename = \"graph.pdf\", width = 4, height = 3)\n\nHere filename is the name (with path and extension) of the file you want to save the figure into. The extension is important: by having specified .pdf, the system automatically saves the figure in PDF format. To use, say, PNG instead:\n\nggsave(filename = \"graph.png\", width = 4, height = 3)\n\nPDF is a vectorized file format: the file contains the instructions for generating the plot elements instead of a pixel representation of the image. Consequently, PDF figures are arbitrarily scalable, and are therefore the preferred way of saving and handling scientific graphs.\nThe width and height parameters specify, in inches, the dimensions of the saved plot. Note that this also scales some other plot elements, such as the size of the axis labels and plot legends. This means you can play with the width and height parameters to save the figure at a size where the labels are clearly visible without being too large.\nIn case you would like to save a figure that is not the last one that was generated, you can specify the plot argument to ggsave(). to do so, first you should assign a plot to a variable. For example:\n\np &lt;- ggplot(iris) + # Assign the ggplot object to the variable p\n  aes(x = Petal.Length) +\n  geom_histogram()\n\nand then\n\nggsave(filename = \"graph.pdf\", plot = p, width = 4, height = 3)"
  },
  {
    "objectID": "Further_plotting_options.html#sec-moreggplot-exercises",
    "href": "Further_plotting_options.html#sec-moreggplot-exercises",
    "title": "8  Some further plotting options; introducing factors",
    "section": "8.6 Exercises",
    "text": "8.6 Exercises\nLet us revisit the data of Fauchald et al. (2017) which we used in Section 7.4, tracking the population size of various herds of caribou in North America over time and correlating population cycling with the amount of vegetation and sea-ice cover. Using the file sea_ice.tsv (sea ice cover per year and month for each caribou herd), do the following:\n\nOne exercise from Section 7.4 was to plot Year along the x-axis and Month along the y-axis, with color tile shading indicating the level of ice cover for the herd labeled WAH in each month-year pair (using geom_tile). The resulting graph had an obvious weakness: the months along the y-axis were not in proper chronological order. Fix this problem by converting the Month column from character strings to factors whose levels go from January to December.\nCreate a similar plot, but do not filter for one single herd. Instead, have each herd occupy a different facet (sub-plot). Do this with the different herds making up a single row of facets, and then a single column of them.\nSince there are 11 different herds altogether, neither the row-wise nor the column-wise arrangement of the facets above is very satisfying: there are too many of them next to one another. Try re-plotting the results, but instead of facet_grid, use facet_wrap(~ Herd) which allows one to lay out the facets in several rows. (As usual, look up the help pages of facet_wrap if needed.)\n\nThe remaining exercises use the Galápagos land snail data (Section 4.2.2).\n\nCreate a plot with standardized size along the x-axis, standardized shape along the y-axis, each individual represented by a point colored by species, and with two facets corresponding to humid and arid habitat types. The facets should be side by side. How does this figure influence the interpretation you had in Section 7.4, exercise 15? That is: does the splitting of communities based on habitat type increase or decrease the overlap between different species?\nRe-create the previous plot with the two side-by-side facets, but in reverse order: the humid facet should be on the left and arid on the right. (Hint: convert habitat to factors!)\n\n\n\n\n\nFauchald, Per, Taejin Park, Hans Tømmervik, Ranga Myneni, and Vera Helene Hausner. 2017. “Arctic greening from warming promotes declines in caribou populations.” Science Advances 3 (4): e1601365. https://doi.org/10.1126/sciadv.1601365."
  },
  {
    "objectID": "Further_plotting_options.html#footnotes",
    "href": "Further_plotting_options.html#footnotes",
    "title": "8  Some further plotting options; introducing factors",
    "section": "",
    "text": "Data from https://climate-data.org.↩︎\nThere also exists a similarly-named function called as.factor, in addition to as_factor. It is the base R version of the same functionality. As usual, the tidyverse version offers improvements over the original, so it is recommended not to use as.factor at all, relying on just as_factor instead.↩︎"
  },
  {
    "objectID": "Joining_data.html#merging-two-related-tables-into-one",
    "href": "Joining_data.html#merging-two-related-tables-into-one",
    "title": "9  Joining data",
    "section": "9.1 Merging two related tables into one",
    "text": "9.1 Merging two related tables into one\nSo far, we have been working with a single table of data at a time. Often however, information about the same observation or experiment is scattered across multiple tables and files. In such cases, we sometimes want to join those separate tables into a single one. To illustrate how this can be done, let us create two simple tables. The first will contain the names of students, along with their chosen subject:\n\nlibrary(tidyverse)\n\nstudies &lt;- tibble(\n  name    = c(\"Sacha\", \"Gabe\", \"Alex\"),\n  subject = c(\"Physics\", \"Chemistry\", \"Biology\")\n)\n\nprint(studies)\n\n# A tibble: 3 × 2\n  name  subject  \n  &lt;chr&gt; &lt;chr&gt;    \n1 Sacha Physics  \n2 Gabe  Chemistry\n3 Alex  Biology  \n\n\nThe second table contains slightly different information: it holds which year a given student is currently into their studies.\n\nstage &lt;- tibble(\n  name = c(\"Sacha\", \"Alex\", \"Jamie\"),\n  year = c(3, 1, 2)\n)\n\nprint(stage)\n\n# A tibble: 3 × 2\n  name   year\n  &lt;chr&gt; &lt;dbl&gt;\n1 Sacha     3\n2 Alex      1\n3 Jamie     2\n\n\nNotice that, while Sacha and Alex appear in both tables, Gabe is only included in studies and Jamie only in stage. While in our tiny tables might seem like an avoidable oversight, such non-perfect alignment of data is often the norm when working with data spanning hundreds or more rows. Here, for the purposes of illustration, we use small tables, but the principles we learn here apply in a broader context as well.\nThere are four commonly used ways of joining these tables into one single dataset. All of them follow the same general pattern: the arguments are two tibbles to be joined, plus a by = argument which defines a join specification of the columns by which the tables should be joined. The join specification can be given using a helper function called join_by. It simply receives the names of the columns to use for joining.1 The output is always a single tibble, containing some type of joining of the data. Let us now look at each joining method in detail.\n\n9.1.1 left_join\nThe left_join function keeps only those rows that appear in the first of the two tables to be joined:\n\nleft_join(studies, stage, by = join_by(name))\n\n# A tibble: 3 × 3\n  name  subject    year\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 Sacha Physics       3\n2 Gabe  Chemistry    NA\n3 Alex  Biology       1\n\n\nThere are two things to notice. First, Jamie is missing from the name column above, even though s/he did appear in the stage tibble. This is exactly the point of left_join: if a row entry in the joining column (specified in by = join_by(...)) does not appear in the first table listed in the arguments, then it is omitted. Second, the year entry for Gabe is NA. This is because Gabe is absent from the stage table, and therefore has no associated year of study. Rather than make up nonsense, R fills out such missing data with NA values.\n\n\n9.1.2 right_join\nThis function works just like left_join, except only those rows are retained which appear in the second of the two tables to be joined:\n\nright_join(studies, stage, by = join_by(name))\n\n# A tibble: 3 × 3\n  name  subject  year\n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Sacha Physics     3\n2 Alex  Biology     1\n3 Jamie &lt;NA&gt;        2\n\n\nIn other words, this is exactly the same as calling left_join with its first two arguments reversed:\n\nleft_join(stage, studies, by = join_by(name))\n\n# A tibble: 3 × 3\n  name   year subject\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;  \n1 Sacha     3 Physics\n2 Alex      1 Biology\n3 Jamie     2 &lt;NA&gt;   \n\n\nThe only difference is in the ordering of the columns, but the data contained in the tables are identical.\nIn this case, the column subject is NA for Jamie. The reason is the same as it was before: since the studies table has no name entry for Jamie, the corresponding subject area is filled in with a missing value NA.\n\n\n9.1.3 inner_join\nThis function retains only those rows which appear in both tables to be joined. For our example, since Gabe only appears in studies and Jamie only in stage, they will be dropped by inner_join and only Sacha and Alex are retained (as they appear in both tables):\n\ninner_join(studies, stage, by = join_by(name))\n\n# A tibble: 2 × 3\n  name  subject  year\n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Sacha Physics     3\n2 Alex  Biology     1\n\n\n\n\n9.1.4 full_join\nThe complement to inner_join, this function retains all rows in all tables, filling in missing values with NAs everywhere:\n\nfull_join(studies, stage, by = join_by(name))\n\n# A tibble: 4 × 3\n  name  subject    year\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 Sacha Physics       3\n2 Gabe  Chemistry    NA\n3 Alex  Biology       1\n4 Jamie &lt;NA&gt;          2\n\n\n\n\n9.1.5 Joining by multiple columns\nIt is also possible to use the above joining functions specifying multiple columns to join data by. To illustrate how to do this and what this means, imagine that we slightly modify the student data. The first table will contain the name, study area, and year of study for each student. The second table will contain the name and study area of each student, plus whether they have passed their most recent exam:\n\nprogram  &lt;- tibble(\n  name    = c(\"Sacha\", \"Gabe\", \"Alex\"),\n  subject = c(\"Physics\", \"Chemistry\", \"Biology\"),\n  year    = c(1, 3, 2)\n)\n\nprint(program)\n\n# A tibble: 3 × 3\n  name  subject    year\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 Sacha Physics       1\n2 Gabe  Chemistry     3\n3 Alex  Biology       2\n\n\n\nprogress &lt;- tibble(\n  name     = c(\"Sacha\", \"Gabe\", \"Jamie\"),\n  subject  = c(\"Physics\", \"Chemistry\", \"Biology\"),\n  examPass = c(TRUE, FALSE, TRUE)\n)\n\nprint(progress)\n\n# A tibble: 3 × 3\n  name  subject   examPass\n  &lt;chr&gt; &lt;chr&gt;     &lt;lgl&gt;   \n1 Sacha Physics   TRUE    \n2 Gabe  Chemistry FALSE   \n3 Jamie Biology   TRUE    \n\n\nAnd now, since the tables share not just one but two columns, it makes sense to join them using both. This can be done by specifying each column inside join_by in the by = argument. For example, left-joining program and progress by both name and subject leads to a joint table in which all unique name-subject combinations found in program are retained, but those found only in progress are discarded:\n\nleft_join(program, progress, by = join_by(name, subject))\n\n# A tibble: 3 × 4\n  name  subject    year examPass\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;   \n1 Sacha Physics       1 TRUE    \n2 Gabe  Chemistry     3 FALSE   \n3 Alex  Biology       2 NA      \n\n\nThe other joining functions also work as expected:\n\nright_join(program, progress, by = join_by(name, subject))\n\n# A tibble: 3 × 4\n  name  subject    year examPass\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;   \n1 Sacha Physics       1 TRUE    \n2 Gabe  Chemistry     3 FALSE   \n3 Jamie Biology      NA TRUE    \n\n\n\ninner_join(program, progress, by = join_by(name, subject))\n\n# A tibble: 2 × 4\n  name  subject    year examPass\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;   \n1 Sacha Physics       1 TRUE    \n2 Gabe  Chemistry     3 FALSE   \n\n\n\nfull_join(program, progress, by = join_by(name, subject))\n\n# A tibble: 4 × 4\n  name  subject    year examPass\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;   \n1 Sacha Physics       1 TRUE    \n2 Gabe  Chemistry     3 FALSE   \n3 Alex  Biology       2 NA      \n4 Jamie Biology      NA TRUE"
  },
  {
    "objectID": "Joining_data.html#binding-rows-and-columns-to-a-table",
    "href": "Joining_data.html#binding-rows-and-columns-to-a-table",
    "title": "9  Joining data",
    "section": "9.2 Binding rows and columns to a table",
    "text": "9.2 Binding rows and columns to a table\nOccasionally, a simpler problem presents itself: there is a single dataset, but its rows are contained across separate tables. For example, a table containing student names and subject areas might be spread across two tables, like this:\n\nstudies1 &lt;- tibble(\n  name    = c(\"Sacha\", \"Gabe\", \"Alex\"),\n  subject = c(\"Physics\", \"Chemistry\", \"Biology\")\n)\n\nprint(studies1)\n\n# A tibble: 3 × 2\n  name  subject  \n  &lt;chr&gt; &lt;chr&gt;    \n1 Sacha Physics  \n2 Gabe  Chemistry\n3 Alex  Biology  \n\n\n\nstudies2 &lt;- tibble(\n  name    = c(\"Jamie\", \"Ashley\", \"Dallas\", \"Jordan\"),\n  subject = c(\"Geology\", \"Mathematics\", \"Philosophy\", \"Physics\")\n)\n\nprint(studies2)\n\n# A tibble: 4 × 2\n  name   subject    \n  &lt;chr&gt;  &lt;chr&gt;      \n1 Jamie  Geology    \n2 Ashley Mathematics\n3 Dallas Philosophy \n4 Jordan Physics    \n\n\nThe tables have the exact same structure, in that the column names and types are identical. It’s just that the rows are, for some reason, disparate. To combine them together, we could recourse to full-joining the tables by both their columns:\n\nfull_join(studies1, studies2, by = join_by(name, subject))\n\n# A tibble: 7 × 2\n  name   subject    \n  &lt;chr&gt;  &lt;chr&gt;      \n1 Sacha  Physics    \n2 Gabe   Chemistry  \n3 Alex   Biology    \n4 Jamie  Geology    \n5 Ashley Mathematics\n6 Dallas Philosophy \n7 Jordan Physics    \n\n\nThis, however, is not necessary. Whenever all we need to do is take two tables and stick their rows together, there is the simpler bind_rows:\n\nbind_rows(studies1, studies2)\n\n# A tibble: 7 × 2\n  name   subject    \n  &lt;chr&gt;  &lt;chr&gt;      \n1 Sacha  Physics    \n2 Gabe   Chemistry  \n3 Alex   Biology    \n4 Jamie  Geology    \n5 Ashley Mathematics\n6 Dallas Philosophy \n7 Jordan Physics    \n\n\nSimilarly, in case two tables have the same number of rows but different columns, one can stick their columns together using bind_cols. For example, suppose we have\n\nstudies &lt;- tibble(\n  name    = c(\"Sacha\", \"Gabe\", \"Alex\"),\n  subject = c(\"Physics\", \"Chemistry\", \"Biology\")\n)\n\nprint(studies)\n\n# A tibble: 3 × 2\n  name  subject  \n  &lt;chr&gt; &lt;chr&gt;    \n1 Sacha Physics  \n2 Gabe  Chemistry\n3 Alex  Biology  \n\n\nas well as a table with year of study and result of last exam only:\n\nyearExam &lt;- tibble(\n  year     = c(3, 1, 2),\n  examPass = c(FALSE, TRUE, TRUE)\n)\n\nprint(yearExam)\n\n# A tibble: 3 × 2\n   year examPass\n  &lt;dbl&gt; &lt;lgl&gt;   \n1     3 FALSE   \n2     1 TRUE    \n3     2 TRUE    \n\n\nWe can now join these using bind_cols:\n\nbind_cols(studies, yearExam)\n\n# A tibble: 3 × 4\n  name  subject    year examPass\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;   \n1 Sacha Physics       3 FALSE   \n2 Gabe  Chemistry     1 TRUE    \n3 Alex  Biology       2 TRUE"
  },
  {
    "objectID": "Joining_data.html#exercises",
    "href": "Joining_data.html#exercises",
    "title": "9  Joining data",
    "section": "9.3 Exercises",
    "text": "9.3 Exercises\nWe have used the data of Fauchald et al. (2017) before in other exercises, in Section 7.4 and Section 8.6. As a reminder, they tracked the population size of various herds of caribou in North America over time, and correlated population cycling with the amount of vegetation and sea-ice cover. Two files from their data are pop_size.tsv (herd population sizes) and sea_ice.tsv (sea ice cover per year and month).\n\nLoad these two datasets into two variables. They could be called pop and ice, for instance. Look at the data to familiarize yourself with them. How many rows and columns are in each?\nBefore doing anything else: how many rows will there be in the table that is the left join of pop and ice, based on the two columns Herd and Year? Perform the left join to see if you were correct. Where do you see NAs in the table, and why?\nNow do the same with right-joining, inner-joining, and full-joining pop and ice.\n\n\n\n\n\nFauchald, Per, Taejin Park, Hans Tømmervik, Ranga Myneni, and Vera Helene Hausner. 2017. “Arctic greening from warming promotes declines in caribou populations.” Science Advances 3 (4): e1601365. https://doi.org/10.1126/sciadv.1601365."
  },
  {
    "objectID": "Joining_data.html#footnotes",
    "href": "Joining_data.html#footnotes",
    "title": "9  Joining data",
    "section": "",
    "text": "It can also do a lot more. You can check its help page (?join_by) after you finish reading this chapter—by that time, it will make much more sense.↩︎"
  },
  {
    "objectID": "Intro_statistics.html#sec-example-wilcox",
    "href": "Intro_statistics.html#sec-example-wilcox",
    "title": "10  Introducing statistical inference",
    "section": "10.1 Introductory example and the Wilcoxon rank sum test",
    "text": "10.1 Introductory example and the Wilcoxon rank sum test\nIn this chapter we will take a first look at statistical tests of a very simple kind: comparing two groups of data. To illustrate why statistical inference is needed in such comparisons, let us take a look at a fictive dataset (fictive_bird_example.csv) which contains equally fictive weight measurements of different bird individuals from the same species. The birds are assumed to come from two islands, a larger and a smaller one. The question is: do the data provide evidence of insular dwarfism—the phenomenon that the body sizes of species tend to decline on small islands over evolutionary time?\nWe can load the data:\n\nlibrary(tidyverse)\n\nbird &lt;- read_csv(\"fictive_bird_example.csv\")\nprint(bird, n = Inf)\n\n# A tibble: 40 × 2\n   island  weight\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 smaller  21.8 \n 2 smaller  19.0 \n 3 smaller  19.0 \n 4 smaller  21.2 \n 5 smaller  15.8 \n 6 smaller  14.3 \n 7 smaller  19.9 \n 8 smaller   9.68\n 9 smaller  17.0 \n10 smaller  12.7 \n11 smaller  22.0 \n12 smaller  15.7 \n13 smaller  16.7 \n14 smaller  27.9 \n15 smaller  17.4 \n16 smaller  12.6 \n17 smaller  33.4 \n18 smaller  25.8 \n19 smaller  20.1 \n20 smaller  21.3 \n21 larger   20.4 \n22 larger   28.3 \n23 larger   19.2 \n24 larger   24.1 \n25 larger   27.7 \n26 larger   16.1 \n27 larger   17.3 \n28 larger   21.2 \n29 larger   28.8 \n30 larger   21.9 \n31 larger   17.4 \n32 larger   13.0 \n33 larger   26.6 \n34 larger   27.0 \n35 larger   19.0 \n36 larger   25.2 \n37 larger   16.5 \n38 larger   25.2 \n39 larger   24.4 \n40 larger   24.7 \n\n\nA quick visualization below looks promising, with individuals on the smaller island indeed appearing to be smaller:\n\nggplot(bird) +\n  aes(x = island, y = weight) +\n  geom_boxplot(colour = \"steelblue\", fill = \"steelblue\",\n               alpha = 0.2, outlier.shape = NA) +\n  geom_jitter(alpha = 0.4, width = 0.05, colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the above plot, geom_jitter was used to display the actual data points that are summarized by the boxplots. The function geom_jitter is just like geom_point, except it adds a random sideways displacement to the data points, to reduce visual overlap between them. The width = 0.05 option restricts this convulsion of the points to a relatively narrow band. Since all data points are now displayed, it makes no sense to rely on the feature of box plots which explicitly draws points that are classified as outliers—their plotting is turned off by the outlier.shape = NA argument to geom_boxplot. It is a useful exercise to play around with these settings, to see what the effects of changing them are.\n\n\nFurthermore, the computed difference between the means and medians of the two samples are also clearly different:\n\nbird |&gt;\n  group_by(island) |&gt;\n  summarise(mean = mean(weight), median = median(weight)) |&gt;\n  ungroup()\n\n# A tibble: 2 × 3\n  island   mean median\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 larger   22.2   23.0\n2 smaller  19.2   19.0\n\n\nCan we conclude that the two samples are indeed different, and birds on the smaller island tend to be smaller, supporting the insular dwarfism hypothesis? As mentioned above, the data are fictive—they are not based on actual measurements. In fact, these “observations” were created by sampling each data point from the same distribution, regardless of island: a normal distribution with mean 20 and standard deviation 5. This means that any supposedly observed difference between the samples must be accidental. Would various statistical methods detect a difference? One thing we can do is what we have learned in Section 7.3: compute the 95% confidence intervals of the means, and see whether and how much they overlap.\n\nbird |&gt;\n  group_by(island) |&gt;\n  summarise(mean = mean(weight), # Means of the two groups\n            sd = sd(weight), # Standard deviations\n            N = n(), # Sample sizes\n            SEM = sd / sqrt(N), # Standard errors of the means\n            CI = qt(1 - 0.025, N - 1)) |&gt; # Confidence intervals\n  ungroup() |&gt; # Ungroup the data (not necessary here, but a good habit)\n  ggplot(aes(x = island, y = mean, ymin = mean - CI, ymax = mean + CI)) +\n  geom_point(colour = \"steelblue\") +\n  geom_errorbar(colour = \"steelblue\", width = 0.2) +\n  ylab(\"mean and 95% confidence interval\") +\n  theme_bw()\n\n\n\n\nAs seen, there is overlap between the 95% confidence intervals. While that by itself is not conclusive, it is an indication that the difference between the two samples may not be as relevant as it might have initially looked.\nLet us wait no longer, and perform a statistical test. One widely used test to check if two samples differ from one another is the Wilcoxon rank sum test (also known as the Mann-Whitney test). Its implementation is as follows:\n\nwilcox.test(weight ~ island, data = bird)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 271, p-value = 0.05589\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe function takes two arguments: a formula, and the data to be analyzed, in the form of a data frame or tibble. The formula in the first argument establishes a relationship between two (or more) columns of the data. We will discuss formulas and their syntax in more detail later. For now: the way to write them is to first type the variable we wish to predict, then a tilde (~), and then the explanatory variable (predictor) by which the data are subdivided into the two groups. In our case, we are trying to explain the difference in weight between the islands, so weight comes before the tilde and the predictor island comes after.\nLet us now look at the output produced by wilcox.test above. Most of it is not particularly relevant for us: we are first informed that a Wilcoxon rank sum exact test is being performed; then we see that we are explaining weight differences by island; then we see the test statistic W itself (we need not concern ourselves with its precise meaning); then the p-value; and finally, a reminder of what the alternative hypothesis is (the null hypothesis is that the shift in location is in fact zero).\nOne piece of datum we are interested in is the p-value. This tells us the probability of observing a result at least as extreme as we see in our data, given that the null hypothesis is true. In our case, we are measuring how different the two samples are from one another, and the null hypothesis is that the only difference we might observe is due purely to chance. We find that the probability of this being the case, given our data, is 0.056. What this number means is that we have a roughly one-in-eighteen chance that the observed difference is nothing but a fluke. Since science leans towards erring on the side of caution (i.e., we would rather miss out on making a discovery than falsely claim having seen an effect), this value is in general a bit too high for comfort. And indeed: since in this case we know that the data were generated by sampling from the same distribution, any distinctiveness between them is incidental.\n\n\n\n\n\n\nWarning\n\n\n\nIn many subfields of science, it is standard practice to consider p-values falling below 0.05 as “significant” and those falling above as “non-significant”. Besides the fact that such a one-size-fits-all approach ought to be suspect even under the best of circumstances, a significance threshold of 0.05 is awfully permissive to errors. In fact, we should expect about one out of twenty (\\(1 / 0.05\\)) of all papers ever published which have reported \\(p = 0.05\\) to be wrong! Digging deeper into this issue reveals that the figure is possibly much worse—see, e.g., Colquhoun (2014). One way to ameliorate the problem is to adopt a less exclusive and parochial view of p-values. Instead of having rigid significance thresholds, p-values can simply be reported as-is, and interpreted for what they are: the probability that the outcome is at least as extreme as observed, assuming that the null model holds. Treating this information as just another piece of the larger data puzzle is a first step towards avoiding the erroneous classification of random patterns as results.\n\n\nApart from the p-value which measures whether the observed effect is likely to have been due to chance alone, another important piece of information is some measure of the effect size: how different are the two samples? We have already computed the means and medians; their differences across the islands provide one way of measuring this effect size. It is possible to add a calculation of the effect size, as well as the confidence intervals, to the Wilcoxon rank sum test. All one needs to do is pass conf.int = TRUE as an argument:\n\nwilcox.test(weight ~ island, data = bird, conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 271, p-value = 0.05589\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -0.018386  6.820811\nsample estimates:\ndifference in location \n              3.436204 \n\n\nAs additional output, we now receive the 95% confidence interval, as well as the explicit difference between the “locations” of the two samples. (A small word of caution: this “difference in location” is neither the difference of the means nor the difference of the medians, but the median of the difference between samples from the two groups of data—feel free to check the help pages by typing ?wilcox.test for more details.) The confidence interval, as seen, includes zero, which can be interpreted as being difficult to rule out the possibility that the observed difference in location is just due to chance.\nThe default confidence level of 95% can be changed via the conf.level argument. For example, to use a 99% confidence interval instead:\n\nwilcox.test(weight ~ island, data = bird, conf.int = TRUE, conf.level = 0.99)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 271, p-value = 0.05589\nalternative hypothesis: true location shift is not equal to 0\n99 percent confidence interval:\n -1.16832  7.70439\nsample estimates:\ndifference in location \n              3.436204 \n\n\nIn summary, the Wilcoxon rank sum test failed to yield serious evidence in favor of rejecting the null hypothesis. Therefore, we cannot claim with any confidence that our fictive birds have different sizes across the two islands. While failure to reject the null is not the same as confirming that the null is true (absence of evidence is not evidence of absence!), the notion that the two samples are different could not be supported. In this particular case, since we ourselves have created the original data using the null hypothesis, we have the privilege of knowing that this is the truth. When working with real data, such knowledge is generally not available."
  },
  {
    "objectID": "Intro_statistics.html#some-general-conclusions",
    "href": "Intro_statistics.html#some-general-conclusions",
    "title": "10  Introducing statistical inference",
    "section": "10.2 Some general conclusions",
    "text": "10.2 Some general conclusions\nThe example of Section 10.1 illustrates two important general points.\nFirst, instead of jumping into statistical tests, we started the analysis with qualitative and descriptive data exploration: we plotted the data, computed its means and medians, etc. This is almost always the correct way to go.1 To perform tests blindly, without visually exploring the data first, is rarely a good idea.\nSecond, we only performed the statistical test after we have made an effort to understand the data, and after setting clear expectations about what we might find. We knew, going into the test, that finding a difference between the two island samples was questionable. And indeed, the test revealed that such a distinction cannot be made in good conscience. Following a similar strategy for all statistical inference can prevent a lot of frustration. To make the point more sharply: do not perform a statistical test without knowing what its result will be! A more nuanced way of saying the same thing is that if you think you see a relationship in your data, then you should also make sure that your observation is not just a mirage, by using a statistical test. But if a relationship is not visually evident, you should first ask the question whether it makes much sense to try to explore it statistically."
  },
  {
    "objectID": "Intro_statistics.html#parametric-versus-non-parametric-tests",
    "href": "Intro_statistics.html#parametric-versus-non-parametric-tests",
    "title": "10  Introducing statistical inference",
    "section": "10.3 Parametric versus non-parametric tests",
    "text": "10.3 Parametric versus non-parametric tests\nThe Wilcoxon rank sum test we employed is non-parametric: it does not assume either the data or the residuals to come from any particular distribution. This is both the test’s advantage and disadvantage. It is an advantage because fewer assumptions must be satisfied for the test to be applicable. Non-parametric techniques also tend to fare better when only limited data are available. Since in many areas of biology (such as ecology) data are hard to come by, datasets are correspondingly small, making non-parametric techniques a natural candidate for interrogating the data with. On the downside, non-parametric tests tend to be less powerful than parametric ones: the additional assumptions of parametric tests may allow for sharper inference—provided that those assumptions are actually met by the real data, of course.\nOne of the most common parametric alternatives to the Wilcoxon rank sum test is Welch’s t-test. Like the Wilcoxon rank sum test, the t-test assumes the independence of the two samples. In addition, it also assumes that the samples are normally distributed. Let us see what the t-test says about our fictive bird data:\n\nt.test(weight ~ island, data = bird)\n\n\n    Welch Two Sample t-test\n\ndata:  weight by island\nt = 1.8704, df = 36.92, p-value = 0.06937\nalternative hypothesis: true difference in means between group larger and group smaller is not equal to 0\n95 percent confidence interval:\n -0.2520327  6.2997175\nsample estimates:\n mean in group larger mean in group smaller \n             22.18479              19.16095 \n\n\nThe format of the output is similar to the one with the Wilcoxon rank sum test; the main differences are that the confidence intervals are included by default, and that the observed means of the two samples are also reported. Like with wilcox.test, the confidence level can be adjusted through the argument conf.level:\n\nt.test(weight ~ island, data = bird, conf.level = 0.99)\n\n\n    Welch Two Sample t-test\n\ndata:  weight by island\nt = 1.8704, df = 36.92, p-value = 0.06937\nalternative hypothesis: true difference in means between group larger and group smaller is not equal to 0\n99 percent confidence interval:\n -1.366524  7.414209\nsample estimates:\n mean in group larger mean in group smaller \n             22.18479              19.16095 \n\n\nImportantly, the message conveyed by the p-value of 0.069 is in line with what we saw from the Wilcoxon rank sum test: there is no strong evidence to claim that the two samples are different. In this particular case, it does not make any qualitative difference whether one uses the parametric t-test or the non-parametric Wilcoxon rank sum test.\nIn other situations, the choice of test may matter more. The data in t_wilcox_example.csv have also been artificially created as an example. They contain two groups of “measurements”. In the first group (group x), the data have been sampled from a lognormal distribution with mean 0 and standard deviation 1. In the second group (group y), the data have been sampled from a lognormal distribution with mean 1 and standard deviation 1. Thus, we know a priori that the two distributions from which the samples were created have different means, and a statistical test may be able to reveal this difference. However, since the data are heavily non-normal, a t-test will struggle to do so. Let us load and visualize the data first:\n\nexample &lt;- read_csv(\"t_wilcox_example.csv\")\n\nggplot(example, aes(x = measurement)) +\n  geom_histogram(bins = 25, alpha = 0.3,\n                 colour = \"steelblue\", fill = \"steelblue\") +\n  facet_wrap(~ group, labeller = label_both) +\n  theme_bw()\n\n\n\n\nApplying a t-test first:\n\nt.test(measurement ~ group, data = example)\n\n\n    Welch Two Sample t-test\n\ndata:  measurement by group\nt = -1.4796, df = 19.622, p-value = 0.1549\nalternative hypothesis: true difference in means between group x and group y is not equal to 0\n95 percent confidence interval:\n -19.853561   3.388384\nsample estimates:\nmean in group x mean in group y \n        2.33571        10.56830 \n\n\nThe parametric t-test cannot detect a difference between the samples. The fact is, since its assumption of normality is violated, such a test should not have been attempted in the first place. By contrast, the non-parametric Wilcoxon rank sum test correctly suggests that there might be a difference:\n\nwilcox.test(measurement ~ group, data = example, conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  measurement by group\nW = 106, p-value = 0.01031\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -3.039534 -0.378063\nsample estimates:\ndifference in location \n             -1.798516 \n\n\nThe p-value of 0.01 means that there is a one-in-a-hundred probability that the observed difference is due only to chance."
  },
  {
    "objectID": "Intro_statistics.html#sec-dot",
    "href": "Intro_statistics.html#sec-dot",
    "title": "10  Introducing statistical inference",
    "section": "10.4 Statistical tests in a data analysis pipeline: The underscore notation",
    "text": "10.4 Statistical tests in a data analysis pipeline: The underscore notation\nOne natural question is whether statistical tests can be included in a data analysis pipeline. The answer is yes, but not without having to consider some complications that are due to historical accidents. Attempting the following results in an error:\n\nbird |&gt;\n  wilcox.test(weight ~ island, conf.int = TRUE)\n\nError in wilcox.test.default(bird, weight ~ island, conf.int = TRUE): 'x' must be numeric\n\n\nThe reason for the error is in how the pipe operator |&gt; works: it substitutes the expression on the left of the pipe into the first argument of the expression on the right. This works splendidly with functions of the tidyverse, since these are designed to always take the data as their first argument. However, a quick glance at the documentation of both wilcox.test and t.test (neither of which are tidyverse functions) reveals that they take the data as their second argument, with the formula coming first. A naive application of the pipe operator, like above, will therefore fail.\nFortunately, there is a way to work around this. One may use the underscore (_) in an expression to stand for whatever was being piped into it. The following will therefore work:\n\nbird |&gt;\n  wilcox.test(weight ~ island, data = _, conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 271, p-value = 0.05589\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -0.018386  6.820811\nsample estimates:\ndifference in location \n              3.436204 \n\n\nThe underscore stands for the object being piped in—which, in our case, is simply the tibble bird. The data = _ argument therefore evaluates to data = bird.\nThis almost concludes all there is to using the underscore notation, except there are three subtleties involved. The first is a simple rule: the underscore must always be used in conjunction with a named argument. So in the above piece of code, writing wilcox.test(weight ~ island, _, conf.int = TRUE) will fail—the argument name must be written out, as data = _. Second, the underscore can only appear at most once in a function call. And third, using the underscore overrides the default behavior of silently substituting the data as the first argument to the function on the right of the pipe. That is, if one uses the underscore, then the data on the left of the pipe will be substituted only where the underscore is.\n\n\n\n\n\n\nNote\n\n\n\nIn case you are using the magrittr pipe %&gt;% instead of the built-in pipe |&gt;, you can observe all of the rules above, except you should replace the underscore by a dot (.). So\n\nbird |&gt;\n  wilcox.test(weight ~ island, data = _, conf.int = TRUE)\n\ncan be equivalently rewritten as\n\nbird %&gt;%\n  wilcox.test(weight ~ island, data = ., conf.int = TRUE)\n\nThere are other subtle differences in how %&gt;% works compared with |&gt;; you can look at the help pages of both (?`%&gt;%`, resp. ?`|&gt;`) for a more thorough explanation."
  },
  {
    "objectID": "Intro_statistics.html#exercises",
    "href": "Intro_statistics.html#exercises",
    "title": "10  Introducing statistical inference",
    "section": "10.5 Exercises",
    "text": "10.5 Exercises\n\nAtmospheric ozone levels are important for urban gardens, because ozone concentrations above 80 parts per hundred million can damage lettuce plants. The file ozone.csv contains measured ozone concentrations in gardens distributed around a city. The gardens were either east or west of the city center. Is there a difference in average ozone concentration between eastern vs. western gardens? Create a plot of the concentrations first and try to guess the answer, before performing any statistical analyses. Then do the statistics, using both nonparametric and parametric methods. What result do you find? Do the outcomes of the different methods agree with one another?\nIn the built-in iris dataset, the sepal and petal characteristics of the various species differ. Let us focus on just petal length here. Is there a detectable difference between the petal lengths of Iris versicolor and I. virginica? (Since we are not working with I. setosa in this exercise, make sure to remove them from the data first—e.g., by using filter.) Like before, start with a graph which you can base a hypothesis on. Then perform both nonparametric and parametric tests to see if your hypothesis can be supported.\nRepeat the above exercise for any and all of the possible trait-species pair combinations in the iris dataset. E.g., one can compare the sepal widths of I. setosa and I. virginica, or the petal widths of I. versicolor and I. setosa, and so on.\n\n\n\n\n\nColquhoun, David. 2014. “An investigation of the false discovery rate and the misinterpretation of p-values.” Royal Society Open Science 1 (3): 140216. https://doi.org/10.1098/rsos.140216."
  },
  {
    "objectID": "Intro_statistics.html#footnotes",
    "href": "Intro_statistics.html#footnotes",
    "title": "10  Introducing statistical inference",
    "section": "",
    "text": "The exception is when one analyzes data from a pre-registered experimental design. In that case, one must follow whatever statistical techniques were agreed upon before data collection even started.↩︎"
  },
  {
    "objectID": "Linear_regression.html#sec-linreg-intro",
    "href": "Linear_regression.html#sec-linreg-intro",
    "title": "11  Simple linear regression",
    "section": "11.1 The intercept and slope of a linear model",
    "text": "11.1 The intercept and slope of a linear model\nAs an introductory example, let us load the Galápagos land snail data from Floreana Island (Section 4.2.2). We look only at individuals belonging to the species Naesiotus nux, and plot the shell shape measurement against the shell size measurement for each of them:\n\nlibrary(tidyverse)\n\nsnails &lt;- read_csv(\"island-FL.csv\")\n\nsnails |&gt;\n  filter(species == \"nux\") |&gt;\n  ggplot() +\n  aes(x = size, y = shape) +\n  geom_point(colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\nThe question we want to answer using this plot is whether larger shells (indicated by larger values of the size variable) tend to also be more elongated (larger values of shape) or not. A casual look would suggest that this is indeed the case. In fact, one can plot these points together with a linear estimator (Section 8.1) showing the trend in the data:\n\nsnails |&gt;\n  filter(species == \"nux\") |&gt;\n  ggplot() +\n  aes(x = size, y = shape) +\n  geom_point(colour = \"steelblue\") +\n  geom_smooth(method = lm, se = FALSE, colour = \"goldenrod\") +\n  theme_bw()\n\n\n\n\nThere are two questions we would like to answer:\n\nHow can one obtain the above line? More generally: how can one determine the best-fitting line going through any set of points?\nIs the observed upwards slope meaningful? That is, if we were to scatter a bunch of points randomly on a plot, how likely is it that we would get a best-fitting line that is as steep as the one above (or steeper) just by chance? The problem is the same as it was in Section 10.1, where we weren’t sure whether the observed difference between the two hypothetical groups of birds has any meaning.\n\nLet us begin with the first of these questions. It is easy to get an approximately correct answer just by looking at the data. A straight line is defined by two parameters: its intercept and slope. The intercept is the point at which the line crosses the \\(y\\)-axis (i.e., its value when \\(x\\) is zero). The slope is the amount of rise or fall in \\(y\\), given one unit of increase in \\(x\\). We can estimate these quantities even without the help of the yellow line above. We see that the data go from about \\(20\\) to \\(28\\) along the \\(x\\)-axis, and from about \\(-0.05\\) to \\(-0.02\\) along the \\(y\\)-axis. So the total rise is \\(0.03\\) vertically, over \\(8\\) units horizontally—for an approximate slope of \\(0.03 / 8 \\approx 0.0038\\). Then, since the line is at about \\(-0.05\\) when \\(x\\) is \\(20\\), we can back-calculate the intercept as \\(-0.05 - 20\\cdot 0.0038 \\approx -0.126\\). So as a crude guess, we know that the line’s intercept and slope cannot be too far off from these values.\nTo go further and obtain the best-fitting line to a set of points, we have to agree on what we mean by “best”. Here is the idea. Let us denote the intercept by \\(\\beta_0\\) and the slope by \\(\\beta_1\\). We can then use the data to write down the following set of equations, one for each data point \\(i\\):\n\\[\ny_i\n= \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i\n\\tag{11.1}\\]\nHere \\(x_i\\) is the \\(x\\)-coordinate of the \\(i\\)th data point, and \\(y_i\\) is its \\(y\\)-coordinate (these quantities are therefore known from the data). Then, since it is unlikely that the line will go through any one point exactly, one has to also add the error that is the distance of the line from the point—that is what \\(\\epsilon_i\\) does.\nAny choice of \\(\\beta_0\\) and \\(\\beta_1\\) define a line, and therefore also define the \\(\\epsilon_i\\) as the distances of the points from that line. This is visualized for two examples below, the first of which is the crude estimate we have calculated above:\n\n\n\n\n\nThe vertical lines are the \\(\\epsilon_i\\), measuring the deviation from the line and the actual data points. That is, \\(\\epsilon_i\\) is the leftover error after we attempt to model the location of the points using our simple line. Except that in statistics, we tend to use the Latin word for “leftover”, and call the \\(\\epsilon_i\\) the residual errors or simply the residuals.\nIt is intuitively obvious that the first line (with intercept \\(\\beta_0 = -0.126\\) and slope \\(\\beta_1 = 0.0038\\)) is better than the second one (intercept \\(\\beta_0 = 0.148\\) and slope \\(\\beta_1 = -0.0051\\)). This is because the magnitudes of the residuals \\(\\epsilon_i\\) tend to be larger for the right panel. We can formalize this intuition by defining the best line as the line with \\(\\beta_0\\) and \\(\\beta_1\\) values that minimize \\(\\sum_i \\epsilon_i^2\\), the sum of squared residuals.1 The process of finding the line is thus to try out many different combinations of the slope \\(\\beta_0\\) and the intercept \\(\\beta_1\\), calculate \\(\\sum_i \\epsilon_i^2\\) for each, and pick the line that minimizes this sum.\nIn reality, there are of course better ways of finding the best-fitting line than this trial-and-error method, but conceptually this is the idea behind obtaining the intercept and slope. R has a built-in function which will obtain the best line, called lm (short for “linear model”). It works similarly to the wilcox.test and t.test functions from Chapter 10: it first takes a formula as an input, and then the data. In our example:\n\nlm(shape ~ size, data = snails |&gt; filter(species == \"nux\"))\n\n\nCall:\nlm(formula = shape ~ size, data = filter(snails, species == \"nux\"))\n\nCoefficients:\n(Intercept)         size  \n  -0.129240     0.004083  \n\n\nThe formula is a shorthand way of writing Equation 11.1. The way to write it is simple: we first write the name of the column in the data that is the response variable—that is, the \\(y_i\\) that we wish to predict. We then write a tilde (~), followed by the name of the data column that is the predictor variable—the \\(x_i\\). For a linear model, it is understood that there is always an intercept \\(\\beta_0\\) and a residual term \\(\\epsilon_i\\). We therefore do not bother writing those out. It is also understood that there is a coefficient \\(\\beta_1\\) multiplying the predictor (size, in our case), so we do not need to write that out either. This really simplifies writing formulas: instead of having to enter the equivalent of Equation 11.1, we simply type out response ~ predictor. That said, it is important to be aware that what is really going on under the hood is using Equation 11.1 to find the intercept \\(\\beta_0\\) and the slope \\(\\beta_1\\).\nThese are seen in the output above. Instead of \\(\\beta_0\\) and \\(\\beta_1\\), the intercept is denoted (Intercept), and the slope is simply given the same name as the predictor variable—size, in this case. So what we learn is that the intercept of the best-fitting line is -0.129, and its slope is 0.0041.\nAs a reminder, the above code chunk could have also been written using pipes and the underscore notation (Section 10.4), for identical results:\n\nsnails |&gt;\n  filter(species == \"nux\") |&gt;\n  lm(shape ~ size, data = _)\n\n\nCall:\nlm(formula = shape ~ size, data = filter(snails, species == \"nux\"))\n\nCoefficients:\n(Intercept)         size  \n  -0.129240     0.004083  \n\n\nThe fitting of a line to data goes by the (perhaps confusing) name of linear regression—see the historical note below for why this is so.\n\n\n\n\n\n\nNote\n\n\n\nThe term “regression” comes from a paper by the method’s inventor, Sir Francis Galton (half-cousin to Charles Darwin). Galton was studying a specific problem: the relationship between the body heights of humans and their children. He noticed that when plotting the heights of parents along the y-axis and that of their children along the x-axis, the slope of the best-fitting line is always smaller than one. The implication is that very tall parents, on average, tend to have shorter children than themselves, and conversely: children of very short parents are on average taller than their elders. Galton described this phenomenon in a study called “Regression towards mediocrity in hereditary stature” (Galton 1886). In it, he also worked out the method of finding the best line that fits the data, and thus the name regression stuck to the method—even though the process of fitting a line to points has nothing whatsoever to do with anything “regressing”.\nIncidentally, the very brief biological explanation of Galton’s finding is as follows. An individual’s height \\(H\\) can be modeled as \\(H = G + E\\), where \\(G\\) denotes heritable (genetic) contributions, and \\(E\\) denotes other, more or less random, environmental contributions. An exceptionally tall individual likely scores high on both \\(G\\) and \\(E\\). Consequently, the individual’s offspring will also likely score high on \\(G\\), inheriting it from the parents. But, since \\(E\\) is not inherited, the child’s chances of also scoring really high on \\(E\\) are exactly those as for anyone else in the population: it could happen, but is unlikely. Typically, the child will have a more or less average value for \\(E\\). Thus, parents with high \\(G\\) and high \\(E\\) will tend to have offspring with high \\(G\\) and average \\(E\\), and so very tall parents end up with tall-but-not-quite-as-tall children as themselves. The same argument holds for very short parents, mutatis mutandis. More broadly, the same holds for any heritable and continuously varying trait (body size, length of mandibles, amount of oil in seeds, rooting depth, and so on, for millions of examples), as long as it can be modeled as emerging from the sum of genetic and environmental factors."
  },
  {
    "objectID": "Linear_regression.html#interpreting-the-results-of-a-linear-regression",
    "href": "Linear_regression.html#interpreting-the-results-of-a-linear-regression",
    "title": "11  Simple linear regression",
    "section": "11.2 Interpreting the results of a linear regression",
    "text": "11.2 Interpreting the results of a linear regression\nWhat do the intercept and slope found above actually mean? And what is the purpose of fitting a straight line to data in the first place?\nThe purpose is twofold. First, we want to be able to make useful predictions using the data. By finding the best-fitting line, we are able to say: “increasing the shell size of an N. nux individual by one unit contributes, on average, \\(0.0041\\) extra units to the shape”. Of course, any such prediction is to be used within a limited range. For example, with the coefficients \\(\\beta_0 = -0.129\\) and \\(\\beta_1 = 0.0041\\), it would follow that a snail of size zero would have a predicted shell shape of \\(y = \\beta_0 + \\beta_1 \\cdot 0 = \\beta_0 = -0.129\\). But a shell of size zero cannot have any sort of shape (obviously). The implication is clear: for shell sizes that are close to the range of observed values, one can use the linear model as an approximation, but beyond that range it can easily break down. This is important to keep in mind at all times when interpreting regression results.\nThe second purpose is to be able to say whether there is any actual relationship between the response and the predictor (here, shell shape and shell size). Let us look at the plot again:\n\nsnails |&gt;\n  filter(species == \"nux\") |&gt;\n  ggplot() +\n  aes(x = size, y = shape) +\n  geom_point(colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\nIf we are being fully honest with ourselves, it is not quite obvious whether the observed positive relationship isn’t simply a fluke: intuitively, it is quite conceivable that the above plot could have been obtained by just randomly spewing points on a canvas. The procedure of finding the intercept and slope of a line which minimizes the sum of squared residuals \\(\\sum_i \\epsilon_i^2\\) can be applied to any set of points. It is an entirely different question whether the inferred intercept and slope are meaningfully different from zero. One function of linear regression is to provide an answer to that question.\nIf being able to do so sounds a bit too good to be true, then there are some bad news coming: it is. Unfortunately, there is no fully general procedure that will distinguish randomness from real patterns. That said, there are certain assumptions one can make which do allow us to do just that:\n\nThe error terms \\(\\epsilon_i\\) are independent from one another.\nThey have constant variance (that is, the variation in the data is roughly the same regardless of the value of the predictor).\nFinally, the \\(\\epsilon_i\\) are also normally distributed.\n\nWe should not fool ourselves: these assumptions are restrictive. But they hold often enough to have some use. We will see shortly how one can ascertain that they do in fact hold (Section 11.3). Before doing that however, let us see how we can get the information about whether the inferred parameters are reliably different from zero. This is achieved using the function summary, which takes a fitted linear model (created by lm) and returns a table:\n\nsnails |&gt;\n  filter(species == \"nux\") |&gt;\n  lm(shape ~ size, data = _) |&gt; # Fit linear model\n  summary() # Obtain regression table\n\n\nCall:\nlm(formula = shape ~ size, data = filter(snails, species == \"nux\"))\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.048880 -0.016772  0.001423  0.013181  0.048515 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -0.129240   0.040706  -3.175  0.00307 **\nsize         0.004083   0.001777   2.297  0.02752 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02275 on 36 degrees of freedom\nMultiple R-squared:  0.1279,    Adjusted R-squared:  0.1036 \nF-statistic: 5.278 on 1 and 36 DF,  p-value: 0.02752\n\n\nThe output first prints the function call to lm we used. Then it gives a quick overview of the residuals: the minimum and maximum values, the point of the first and third quantiles, and the median—in other words, it contains the same information one would use to create a box plot with. This ought to give a quick and rough idea of whether the residuals are violently skewed, or have at least a chance of being normally distributed, which was one of the key assumption behind linear regression above. (We will discuss a better method for assessing the normality of the residuals in Section 11.3.) The next item in the output is the most important part: the table of fitted regression coefficients (intercept and slope). Here we can find the estimated values of the coefficients, their standard error, an associated t-statistic (ignore this for now), and the p-values (the Pr(&gt;|t|) column).\nThe p-values measure how likely it would have been to obtain the same regression parameters just by chance—by randomly throwing points on the plot. By “random”, we mean a process that strictly observes the assumptions about the independence, normality, and constant variance of the residuals. Based on this, it is highly unlikely that the intercept’s true value is zero. But generally we are more interested in the slope, because this is what tells us whether there is in fact any relationship between the measured quantities. Here the p-value associated with the slope is 0.028. That is, if we were to repeatedly generate as many random points as we see in the data and measure the slope of the best fitting line, we would observe a slope at least as steep as our inferred \\(0.0041\\) once every \\(1/0.028 \\approx 36\\) tries. While that is good indication that the relationship is not simply due to chance, one would not bet one’s life on it either.2\nThere is some further information at the bottom of the output. The “residual standard error” is an estimate for the standard deviation of the residuals \\(\\epsilon_i\\) (recall that we assumed this standard deviation to be constant and independent of \\(i\\)). “Multiple R-squared” is the fraction of variation in the data explained by the model. “Adjusted R-squared” is the same but takes into account how many parameters were used to fit the model. Finally, there is information on the F-statistic, which we will not be look at here."
  },
  {
    "objectID": "Linear_regression.html#sec-diagnostics",
    "href": "Linear_regression.html#sec-diagnostics",
    "title": "11  Simple linear regression",
    "section": "11.3 Diagnostic plots",
    "text": "11.3 Diagnostic plots\nAs advertised, it is possible (and important) to check whether the assumptions behind linear regression actually hold. There is a convenient way to do so, via diagnostic plots. Such plots can be created via the autoplot function of the ggfortify package. We can therefore install this package first, in case we do not have it already:\n\ninstall.packages(\"ggfortify\")\n\nAnd then load it:\n\nlibrary(ggfortify)\n\nAnd now, we can simply give the result of the lm function as input to the function autoplot:\n\nsnails |&gt;\n  filter(species == \"nux\") |&gt;\n  lm(shape ~ size, data = _) |&gt;\n  autoplot()\n\n\n\n\nWe see four plots above. The top left of these shows the residuals against the fitted values. If the points have no trends of increase, decrease, valleys, or humps, and the spread of the points is roughly constant, then they adhere to the assumptions of linear regression. The blue line attempts to capture whether there is an overall trend in the data; in this case, it would be hard to argue that any trend is due to something else than chance.\nThe bottom left plot is much the same as the top left one, except it takes the absolute values of the residuals. This is done because, since residuals will by definition be more or less symmetrically distributed around zero, one can effectively double the precision of the diagnostic by focusing only on magnitudes. For statistical reasons that do not concern us here, the square roots of these absolute values tend to behave much better, which is why we see the square root being taken along the y-axis. This plot is good for spotting whether the variance of the residuals depends on the fitted values. To satisfy the requirements of linear regression, there should be no such dependence.3 The blue line is again a locally-weighted estimate, which ought to be as flat as possible.\nThe top right graph offers a visualization of how well the residuals follow a normal distribution. The idea behind this quantile-quantile plot (Q-Q plot) is that if the residuals are indeed normally distributed, then we can line them up in increasing order along the x-axis, and for each of them, plot the theoretically expected value (based on normality) along the y-axis. If these observed vs. theoretically predicted values fall on the dashed straight line, then there is a perfect match between theory and observation, and the residuals are normally distributed. The stronger the deviation from the dashed line indicating a perfect theoretical match, the more non-normal the residuals are.\nThe bottom right graph measures the “leverage” of each point, which is a measure of how sensitively the regression reacts to removing one data point. We will not be concerned with this plot.\nThe blue smoother lines often do more to confuse than to help. For example, one can see a slight dip in the blue line in the bottom left graph, but looking at all points together reveals how little this trend means. The smooth.colour = NA argument removes the blue lines:\n\nsnails |&gt;\n  filter(species == \"nux\") |&gt;\n  lm(shape ~ size, data = _) |&gt;\n  autoplot(smooth.colour = NA)\n\n\n\n\nOne may go even further. Since autoplot returns a ggplot object, we can add various theme options to make the plot prettier. We can also increase the transparency of the points to better see where and how much they overlap with one another:\n\nsnails |&gt;\n  filter(species == \"nux\") |&gt;\n  lm(shape ~ size, data = _) |&gt;\n  autoplot(smooth.colour = NA, colour = \"steelblue\", alpha = 0.7) +\n  theme_bw()\n\n\n\n\nNow that we have a cleaner plot, how do we interpret it. Let us proceed one by one. The top left plot is as good as it can get: there is a blurb of points scattered around zero, with no systematic increase or decrease. Similarly, the bottom left plot is excellent, revealing that the spread of the residuals does not depend on the fitted values. Finally, the top right quantile-quantile plot reveals that the residuals follow the theoretically expected normal distribution beautifully. In short, these diagnostics show that the assumptions behind the linear regression model are nicely fulfilled.\nTo give an example of what not-so-good diagnostics might look like, let us repeat the procedure for a different species: N. galapaganus instead of N. nux. Here are the raw data together with the fitted line:\n\nsnails |&gt;\n  filter(species == \"galapaganus\") |&gt; # Choose a different species\n  ggplot() +\n  aes(x = size, y = shape) +\n  geom_point(colour = \"steelblue\") +\n  geom_smooth(method = lm, se = FALSE, colour = \"goldenrod\") +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAnd the corresponding diagnostic plots:\n\nsnails |&gt;\n  filter(species == \"galapaganus\") |&gt;\n  lm(shape ~ size, data = _) |&gt;\n  autoplot(smooth.colour = NA, colour = \"steelblue\", alpha = 0.7) +\n  theme_bw()\n\n\n\n\nLooking at the top left, one gets at the very least somewhat suspicious that there is a trend in these points, first going down, then up, then down again at the end. Next, while the bottom left plot is not disastrous, it again might suggest that the variance of the residuals is higher for smaller fitted values than for larger ones. Finally, the quantile-quantile plot is clearly no good: small residuals are consistently overestimated (the theoretical values are larger than the actual ones), and large residuals are underestimated. This means that any results obtained from a linear regression ought to be treated with caution. Possibly, methods other than linear regression are needed to understand these data."
  },
  {
    "objectID": "Linear_regression.html#sec-anscombe",
    "href": "Linear_regression.html#sec-anscombe",
    "title": "11  Simple linear regression",
    "section": "11.4 More on diagnostics: Anscombe’s quartet",
    "text": "11.4 More on diagnostics: Anscombe’s quartet\nTo further illustrate how diagnostics can be used to visually judge whether the assumptions of linear regression are met, let us take a look at a famous dataset that was designed for precisely this purpose (Anscombe 1973). The data are built into R (with the name anscombe), but are not in the most convenient format:\n\nprint(anscombe)\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\n\nThese are actually four datasets merged into one: x1 and y1 are \\(x\\)- and \\(y\\)-coordinates of the points from the first set, x2 and y2 from the second set, and so on. We can use pivot_longer to put these data in tidy format:\n\nans_long &lt;- anscombe |&gt;\n  pivot_longer(cols = everything(), names_to = c(\".value\", \"set\"),\n               names_pattern = \"(.)(.)\")\nprint(ans_long)\n\n# A tibble: 44 × 3\n   set       x     y\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1        10  8.04\n 2 2        10  9.14\n 3 3        10  7.46\n 4 4         8  6.58\n 5 1         8  6.95\n 6 2         8  8.14\n 7 3         8  6.77\n 8 4         8  5.76\n 9 1        13  7.58\n10 2        13  8.74\n# ℹ 34 more rows\n\n\nWe can now visualize each set, along with linear fits:\n\nans_long |&gt;\n  ggplot() +\n  aes(x = x, y = y, colour = set) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  facet_wrap(~ set, nrow = 2, labeller = label_both) +\n  theme_bw()\n\n\n\n\nThe data have been carefully crafted so that the least-squares regression line has an intercept of 3 and a slope of 0.5 for each of the four sets. Furthermore, the p-values are also identical to many decimal places. But this visual representation reveals what would have been much harder to intuit otherwise: that only the first set has a real chance of conforming to the assumptions of linear regression. Performing the regression on just this set and creating diagnostic plots:\n\nlm(y ~ x, data = filter(ans_long, set == \"1\")) |&gt; summary()\n\n\nCall:\nlm(formula = y ~ x, data = filter(ans_long, set == \"1\"))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx             0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm(y ~ x, data = filter(ans_long, set == \"1\")) |&gt;\n  autoplot(smooth.colour = NA, colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\nWhile the number of data points is small, there is otherwise nothing to suggest in these diagnostic plots that there is anything wrong with the regression.\nThe situation changes for the other three sets. Let us look at set 2:\n\nlm(y ~ x, data = filter(ans_long, set == \"2\")) |&gt; summary()\n\n\nCall:\nlm(formula = y ~ x, data = filter(ans_long, set == \"2\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx              0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm(y ~ x, data = filter(ans_long, set == \"2\")) |&gt;\n  autoplot(smooth.colour = NA, colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\nBlindly reading off the p-values without considering the diagnostic plots might lead one to take them seriously. This would be wrong however, as the assumptions of the linear regression are clearly not fulfilled. The left two diagnostics show that the residuals are not independent, and certainly not homoscedastic. The Q-Q plot additionally shows that they are not even normally distributed.\nIn set 3, the trends are driven too much by a single outlier:\n\nlm(y ~ x, data = filter(ans_long, set == \"3\")) |&gt; summary()\n\n\nCall:\nlm(formula = y ~ x, data = filter(ans_long, set == \"3\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx             0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm(y ~ x, data = filter(ans_long, set == \"3\")) |&gt;\n  autoplot(smooth.colour = NA, colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\nAs before, the left two diagnostic plots show that the independence of the residuals is violated. Finally, in set 4, the whole regression is based on a single point whose predictor x is different from that of the rest:\n\nlm(y ~ x, data = filter(ans_long, set == \"4\")) |&gt; summary()\n\n\nCall:\nlm(formula = y ~ x, data = filter(ans_long, set == \"4\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx             0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nlm(y ~ x, data = filter(ans_long, set == \"4\")) |&gt;\n  autoplot(smooth.colour = NA, colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\nClearly, the assumption that the residual variances are independent of the predictor is heavily violated.\nThese examples are there to urge caution when interpreting regression statistics. This problem becomes much more acute when relying on multiple regression, where there is more than one predictor variable (Chapter 13). Since high-dimensional data cannot be visualized as easily as the datasets above, often the diagnostic plots are the only way to tell whether the assumptions of regression hold or not."
  },
  {
    "objectID": "Linear_regression.html#sec-theilsen",
    "href": "Linear_regression.html#sec-theilsen",
    "title": "11  Simple linear regression",
    "section": "11.5 A non-parametric method: Theil–Sen regression",
    "text": "11.5 A non-parametric method: Theil–Sen regression\nThe method of linear regression discussed so far is called least-squares regression, due to the fact that it relies on minimizing the sum of squared residuals \\(\\sum_i \\epsilon_i^2\\). A non-parametric alternative to this method is Theil–Sen regression. This is generally much more robust against outliers than the least-squares method. It also does not require that the residuals are normally distributed. There are also two disadvantages, the main one being that it can only be used for simple regression (one single predictor). It can also be slower to compute, but with today’s computers, this is rarely an issue.\nThe way Theil–Sen regression works is simple:\n\nA line is fit between all possible pairs of points, and their slopes are recorded.\nThe overall regression slope m is the median of all these pairwise slopes.\nThe intercept b is the median of all yi – m xi values, where xi is the ith measurement of the predictor and yi the corresponding response.\n\nTo use the Theil–Sen regression, one has to install the package mblm (“median-based linear models”):\n\ninstall.packages(\"mblm\")\n\nlibrary(mblm)\n\nThe function performing the regression is itself called mblm. A note of caution: its data argument, for some reason, is not called data but dataframe. Let us apply it to set 3 in the Anscombe dataset (the one with the single strong outlier):\n\nmblm(y ~ x, dataframe = filter(ans_long, set == \"3\")) |&gt; summary()\n\n\nCall:\nmblm(formula = y ~ x, dataframe = filter(ans_long, set == \"3\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.0045 -0.0022  0.0000  0.0025  4.2435 \n\nCoefficients:\n             Estimate       MAD V value Pr(&gt;|V|)   \n(Intercept) 4.0050000 0.0074130      65  0.00501 **\nx           0.3455000 0.0007413      66  0.00380 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.415 on 9 degrees of freedom\n\n\nAs seen, the predicted intercept and slope are no longer 3 and 0.5, but 4 and 0.35 instead. Also, in the regression table above, the median absolute deviation (MAD) of the parameters is reported instead of their standard error, as the MAD is a non-parametric measure of spread.4 The p-values (Pr(&gt;|V|)) in the table are like those in the regression tables from applying summary to lm—however, since Theil–Sen regression is a non-parametric method, these p-values are based on a Wilcoxon rank sum test instead of a t-test.\nWe can visualize the Theil–Sen regression alongside the least-squares regression, for a better comparison of what they do:\n\nleastSquaresFit &lt;- lm(y ~ x, data = filter(ans_long, set == \"3\"))\nTheilSenFit &lt;- mblm(y ~ x, dataframe = filter(ans_long, set == \"3\"))\n\nans_long |&gt;\n  filter(set == \"3\") |&gt;\n  mutate(`least squares` = predict(leastSquaresFit),\n         `Theil-Sen` = predict(TheilSenFit)) |&gt;\n  pivot_longer(cols = c(\"least squares\", \"Theil-Sen\"),\n               names_to = \"type\", values_to = \"prediction\") |&gt;\n  ggplot() +\n  geom_point(aes(x = x, y = y), colour = \"steelblue\") +\n  geom_line(aes(x = x, y = prediction), colour = \"goldenrod\") +\n  facet_grid(. ~ type) +\n  theme_bw()\n\n\n\n\nThe Theil–Sen regression correctly recognizes the outlier for what it is, and remains unaffected by it.\n\n\n\n\n\n\nNote\n\n\n\nIn the code above, we relied on a function called predict. This simply returns the model-predicted results for each value of the predictor:\n\npredict(leastSquaresFit)\n\n       1        2        3        4        5        6        7        8 \n7.999727 7.000273 9.498909 7.500000 8.499455 9.998636 6.000818 5.001364 \n       9       10       11 \n8.999182 6.500545 5.501091 \n\npredict(TheilSenFit)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n7.4600 6.7690 8.4965 7.1145 7.8055 8.8420 6.0780 5.3870 8.1510 6.4235 5.7325"
  },
  {
    "objectID": "Linear_regression.html#exercises",
    "href": "Linear_regression.html#exercises",
    "title": "11  Simple linear regression",
    "section": "11.6 Exercises",
    "text": "11.6 Exercises\n\nThe file plant_growth_rate.csv contains individual plant growth data (mm/week), as a function of soil moisture content. Do plants grow better in more moist soils? Visualize the relationship, then perform and interpret a linear regression using the parametric (least squares) method. Use diagnostic plots to check whether the assumptions of the model are satisfied.\nIt is difficult to measure the height of a tree. By contrast, the diameter at breast height (DBH) is easy to measure. Can one infer the height of a tree by measuring its DBH? The built-in dataset trees contains DBH data (labeled Girth), as well as measured height and timber volume of 31 felled black cherry trees. You can ignore timber volume, and focus instead on how well DBH predicts tree height. Plot the relationship, perform least-squares linear regression, and create diagnostic plots. Interpret the results, and summarize how reliable it is to use DBH to infer tree height.\nThe Galápagos land snail data (Section 4.2.2) contain seven species. Apart from Naesiotus nux and N. galapaganus that were analyzed in this chapter, it also has N. calvus, N. invalidus, N. rugulosus, N. unifasciatus, and N. ustulatus. Perform the analysis of regressing shell shape against shell size for each of these, using least-squares linear regression. Use diagnostic plots to interpret the regression results.\nRepeat exercises 1-3 using Theil–Sen regression instead of ordinary least squares. (Hint for interpreting the results: this method makes no assumptions about the normality of the residuals.)\n\n\n\n\n\nAnscombe, Francis J. 1973. “Graphs in Statistical Analysis.” American Statistician 27 (1): 17–21. https://doi.org/10.1080/00031305.1973.10478966.\n\n\nGalton, Francis. 1886. “Regression Towards Mediocrity in Hereditary Stature.” Journal of the Anthropological Institute of Great Britain and Ireland 15: 246–63."
  },
  {
    "objectID": "Linear_regression.html#footnotes",
    "href": "Linear_regression.html#footnotes",
    "title": "11  Simple linear regression",
    "section": "",
    "text": "Why the sum of squared residuals, and not just their simple sum? This is to prevent very large positive and very large negative deviations canceling each other out in the sum, making it appear as if the total deviation was very small. Squared deviations are always nonnegative and therefore do not suffer from this cancellation problem.↩︎\nImagine you go bungee jumping and are told by the staff: “It is completely safe. Only one out of 36 jumps on average end up with a fatality—for a negligible probability of 0.028.” Would you take the jump? Translated to statistics: would a p-value of 0.028 (or worse, the higher 0.05 which is often a “standard” cutoff) truly convince you that what you are seeing is a real effect?↩︎\nIn technical terms, the variation should be homoscedastic (be approximately the same for all values of the predictor) as opposed to heteroscedastic. This is the same as saying that the variance of the residuals \\(\\epsilon_i\\) should be independent of \\(i\\).↩︎\nThe median absolute deviation (MAD) over a set of data points \\(x_i\\) is defined as \\(\\text{MAD} = \\text{median}(|x_i - \\text{median}(x)|)\\), where \\(\\text{median}(x)\\) is the median of the data.↩︎"
  },
  {
    "objectID": "Anova.html#the-kruskalwallis-and-dunn-tests",
    "href": "Anova.html#the-kruskalwallis-and-dunn-tests",
    "title": "12  The Kruskal–Wallis test and one-way ANOVA",
    "section": "12.1 The Kruskal–Wallis and Dunn tests",
    "text": "12.1 The Kruskal–Wallis and Dunn tests\nThe tools of Chapter 10 allow us to compare two groups of data. But what do we do when we have more than two groups? For example, we might wish to know how different treatments influence plant growth, as measured by dry weight. If there are two treatments, then there will be three (instead of two) groups, because the treatments will be compared with a control group which does not receive treatment.\nSuch a dataset is in fact built into R, and is called PlantGrowth:\n\nlibrary(tidyverse)\n\nPlantGrowth |&gt; as_tibble() |&gt; print(n = Inf)\n\n# A tibble: 30 × 2\n   weight group\n    &lt;dbl&gt; &lt;fct&gt;\n 1   4.17 ctrl \n 2   5.58 ctrl \n 3   5.18 ctrl \n 4   6.11 ctrl \n 5   4.5  ctrl \n 6   4.61 ctrl \n 7   5.17 ctrl \n 8   4.53 ctrl \n 9   5.33 ctrl \n10   5.14 ctrl \n11   4.81 trt1 \n12   4.17 trt1 \n13   4.41 trt1 \n14   3.59 trt1 \n15   5.87 trt1 \n16   3.83 trt1 \n17   6.03 trt1 \n18   4.89 trt1 \n19   4.32 trt1 \n20   4.69 trt1 \n21   6.31 trt2 \n22   5.12 trt2 \n23   5.54 trt2 \n24   5.5  trt2 \n25   5.37 trt2 \n26   5.29 trt2 \n27   4.92 trt2 \n28   6.15 trt2 \n29   5.8  trt2 \n30   5.26 trt2 \n\n\n(The PlantGrowth data are in the form of a data frame instead of a tibble; hence we convert it above by using as_tibble.) We see that each group consists of ten observations, and we have three groups: the control (ctrl) treatment 1 (trt1), and treatment 2 (trt2). As usual, we visualize the data first, before doing any tests:\n\nggplot(PlantGrowth) +\n  aes(x = group, y = weight) +\n  geom_boxplot(colour = \"steelblue\", fill = \"steelblue\",\n               alpha = 0.2, outlier.shape = NA) +\n  geom_jitter(alpha = 0.4, width = 0.05, colour = \"steelblue\") +\n  theme_bw()\n\n\n\n\nThere are two questions one can ask, and they naturally build on each other. First, we might wonder if the observed differences between the distribution of the data in the different groups is meaningful. Could it be that all data points are actually drawn from the same underlying distribution, and the observed differences are simply due to chance? If so, what is the likelihood of that situation? And second, provided we can reject the idea that the data in the different groups are identically distributed, which groups are the ones that differ from one another? For example, based on the figure above, it appears reasonable to expect a real difference between treatment 1 and treatment 2, but it is unclear whether the control truly differs from any of the treatments.\nStarting with the first of these questions, there exists an analogue to the Wilcoxon rank sum test which works when there are more than two groups of data. This is the Kruskal–Wallis test, which can be used with any number of groups as long as those groups vary within a single factor.1 The Kruskal–Wallis test is non-parametric, and therefore does not rely on assumptions such as the normality of the residuals. Its implementation in R, kruskal.test, is analogous to wilcox.test, t.test lm, or mblm: it takes a formula and the data as inputs. Therefore, to perform the test on the PlantGrowth data, we write:\n\nkruskal.test(weight ~ group, data = PlantGrowth)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  weight by group\nKruskal-Wallis chi-squared = 7.9882, df = 2, p-value = 0.01842\n\n\nThe null hypothesis of the Kruskal–Wallis test is that the observations from all groups were sampled from the same underlying distribution—that is, that there are no differences between the groups other than those attributed to random noise. Consequently, when the p-value is low (like above), this means that it is unlikely that the data in all groups come from the same distribution, and thus that at least one group differs from the others.\nNow that we can be reasonably confident about the first question (do all the data come from the same distribution or not?), we can go on to the second: are there differences between particular pairs of groups? One way of finding out could be to perform pairwise Wilcoxon rank sum tests between each pair of groups. This, in fact, would be quite feasible here, because there are only three comparisons to be made (control vs. treatment 1; control vs. treatment 2, and treatment 1 vs. treatment 2). Here are the results of doing this:\n\nas_tibble(PlantGrowth) |&gt; # The PlantGrowth dataset, converted to a tibble\n  filter(group != \"trt2\") |&gt; # Keep only the control and treatment 1\n  wilcox.test(weight ~ group, data = _, conf.int = TRUE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  weight by group\nW = 67.5, p-value = 0.1986\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -0.2899731  1.0100554\nsample estimates:\ndifference in location \n             0.4213948 \n\nas_tibble(PlantGrowth) |&gt;\n  filter(group != \"trt1\") |&gt; # Keep only the control and treatment 2\n  wilcox.test(weight ~ group, data = _, conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 25, p-value = 0.06301\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -1.00  0.04\nsample estimates:\ndifference in location \n                 -0.49 \n\nas_tibble(PlantGrowth) |&gt;\n  filter(group != \"ctrl\") |&gt; # Keep only treatments 1 and 2\n  wilcox.test(weight ~ group, data = _, conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -1.50 -0.31\nsample estimates:\ndifference in location \n                -0.945 \n\n\nAs expected, the difference between the control and the two treatments is not particularly credible (especially in the case of treatment 1), but there is good indication that the difference between the two treatments is not just due to chance.\nWhile this method of using repeated Wilcoxon rank sum tests works fine in our example, there are some problems with this approach. One is that it can quickly get out of hand, because having \\(n\\) groups means there will be \\(n (n - 1) / 2\\) pairs to consider. For instance, if the number of groups is 12 (not a particularly large number), then there are 66 unique pairs already. It would not be pleasant to perform this many tests, even if any single test is quite simple to run.\nThe other problem has to do with multiple testing and its influence on the interpretation of p-values. Very simply put, the problem is that if sufficiently many groups are compared, then we might find at least one pair with a low p-value—not because the null hypothesis is false, but because across a large number of observations some p-values will turn out lower than others just by chance. The p-values measure, in effect, the probability that the observed differences are too stark to be due to simple coincidence. But if we create sufficiently many opportunities for such a coincidence to arise, then of course one eventually will. One of the best explanations of this point is in the following cartoon by xkcd:\n\nFortunately, there is a way of solving both the problem of automating many pairwise comparisons, as well as adjusting the p-values to account for multiple testing. The way forward is to perform a post hoc (Latin “after this”) test. In this case, the Dunn test is one such post hoc test. This test is implemented in R, but not in any of the basic packages. To use it, one must first install the FSA package:\n\ninstall.packages(\"FSA\")\n\nOnce it is installed, the package should be loaded:\n\nlibrary(FSA)\n\nAnd then, the Dunn test (dunnTest) follows the familiar syntax of receiving a formula and the data:\n\ndunnTest(weight ~ group, data = PlantGrowth)\n\nDunn (1964) Kruskal-Wallis multiple comparison\n\n\n  p-values adjusted with the Holm method.\n\n\n   Comparison         Z    P.unadj      P.adj\n1 ctrl - trt1  1.117725 0.26368427 0.26368427\n2 ctrl - trt2 -1.689290 0.09116394 0.18232789\n3 trt1 - trt2 -2.807015 0.00500029 0.01500087\n\n\nThe table above is the output of the test, and has four columns. The first column shows which two groups are being compared. The next column, called Z, is the value of the test statistic, which we need not concern ourselves with here. Next, we have the unadjusted p-values; and finally, the adjusted p-values (P.adj), which have been corrected to account for the multiple testing problem mentioned above. Therefore, the adjusted p-values will always be as large or larger than the unadjusted ones.\nWhat has the post hoc Dunn test revealed? Precisely what we have been suspecting: that the only difference worth noting is the one between the two treatments (last row of the table, where the adjusted p-value is sufficiently small to have a chance of pointing at a real difference)."
  },
  {
    "objectID": "Anova.html#sec-onewayanova",
    "href": "Anova.html#sec-onewayanova",
    "title": "12  The Kruskal–Wallis test and one-way ANOVA",
    "section": "12.2 One-way ANOVA and Tukey’s test",
    "text": "12.2 One-way ANOVA and Tukey’s test\nThe Kruskal–Wallis test is the non-parametric analogue of one-way ANOVA (ANalysis Of VAriance). The “one-way” in the name refers to the property that there is a single factor indexing the groups, as discussed earlier. ANOVA relies on assumptions such as normality of the residuals and having the same number of observations in each group (balanced design). Otherwise, it is much like the Kruskal–Wallis test.2 Perhaps surprisingly, the function in R that performs an ANOVA is the same lm that we have used in Chapter 11 for performing linear regression:\n\nlm(weight ~ group, data = PlantGrowth)\n\n\nCall:\nlm(formula = weight ~ group, data = PlantGrowth)\n\nCoefficients:\n(Intercept)    grouptrt1    grouptrt2  \n      5.032       -0.371        0.494  \n\n\nThe output takes some care to interpret correctly: out of the three coefficients listed, (Intercept) is the mean weight in the control group; grouptrt1 is the difference of the mean weight in treatment 1 from the control; and grouptrt2 is the difference of the mean weight in treatment 2 from the control. (We will come back to this, as well as why the lm function was used, in Section 12.3.) To get more information, one can pass the result of lm to a function called anova. Despite its name, the role of this function is not to actually perform the ANOVA (that was done by lm), but to display its results using the sum-of-squares table, which is also known as the ANOVA table. In this table, each factor indexing the groups, as well as their interactions (if present), get one row:\n\nlm(weight ~ group, data = PlantGrowth) |&gt; anova()\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ngroup      2  3.7663  1.8832  4.8461 0.01591 *\nResiduals 27 10.4921  0.3886                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value above, in the row belonging to the factor group, under the column Pr(&gt;F), is the analogue of the p-value calculated by the Kruskal–Wallis test (which was 0.01842). We can see that the two tests agree qualitatively.\nJust as the Dunn test is a post-hoc test for the Kruskal–Wallis test, Tukey’s honest significance test is a post-hoc test for an ANOVA. The corresponding function in R is called TukeyHSD, and it should be applied to the result of the lm function—with one caveat. Tukey’s test requires the output of the ANOVA to be in a particular format. Therefore, before passing the result of lm to TukeyHSD, we first have to pass it to a helper function called aov (“analysis of variance”). Like this:\n\nlm(weight ~ group, data = PlantGrowth) |&gt; aov() |&gt; TukeyHSD()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = lm(weight ~ group, data = PlantGrowth))\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\n\nThe table is similar to the one produced by the Dunn test, with a few differences. The first column tells us which groups of data are compared. The second column is the raw difference between the group means, the third and fourth are the lower and upper limits of a 95% confidence interval for the difference, and the last one are the p-values (adjusted for multiple comparisons). In this case, the results from the Dunn test and the Tukey test are in agreement: only the difference between the two control groups stands out as having a reasonable chance of being real. The fact that the Kruskal–Wallis test and an ANOVA lead to the same conclusion increases the trust we can place in their results being correct.\nThe aov function serves no purpose above, except to put the data in a format which TukeyHSD can work with. In case using this intermediary feels like a hassle, one can easily make life simpler, by writing a helper function which automatically calls it, together with the Tukey test:\n\ntukey.test &lt;- function(lmFit) {\n  lmFit |&gt; aov() |&gt; TukeyHSD()\n}\n\n(See Chapter 3 to review how functions are written.) Here lmFit is a model fit object returned by the lm function. Using this, we can rewrite the above more simply:\n\nlm(weight ~ group, data = PlantGrowth) |&gt; tukey.test()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = lmFit)\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064"
  },
  {
    "objectID": "Anova.html#sec-lmaov",
    "href": "Anova.html#sec-lmaov",
    "title": "12  The Kruskal–Wallis test and one-way ANOVA",
    "section": "12.3 ANOVA and linear regression",
    "text": "12.3 ANOVA and linear regression\nAt first glance, it appears strange that the function performing an ANOVA is the same as the one doing linear regression, lm. Aren’t the two methods very different from one another? After all, ANOVA compares data in different groups, whereas linear regression fits a line to a bunch of data points.\nAs surprising as it may sound, the two are not different. There is a way of formulating an ANOVA which is exactly analogous to a linear model. To illustrate this idea, let us first consider a dataset with just two groups of data. The fictive bird data from Chapter 10 (fictive_bird_example.csv) will work well:\n\nbird &lt;- read_csv(\"fictive_bird_example.csv\")\nprint(bird, n = Inf)\n\n# A tibble: 40 × 2\n   island  weight\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 smaller  21.8 \n 2 smaller  19.0 \n 3 smaller  19.0 \n 4 smaller  21.2 \n 5 smaller  15.8 \n 6 smaller  14.3 \n 7 smaller  19.9 \n 8 smaller   9.68\n 9 smaller  17.0 \n10 smaller  12.7 \n11 smaller  22.0 \n12 smaller  15.7 \n13 smaller  16.7 \n14 smaller  27.9 \n15 smaller  17.4 \n16 smaller  12.6 \n17 smaller  33.4 \n18 smaller  25.8 \n19 smaller  20.1 \n20 smaller  21.3 \n21 larger   20.4 \n22 larger   28.3 \n23 larger   19.2 \n24 larger   24.1 \n25 larger   27.7 \n26 larger   16.1 \n27 larger   17.3 \n28 larger   21.2 \n29 larger   28.8 \n30 larger   21.9 \n31 larger   17.4 \n32 larger   13.0 \n33 larger   26.6 \n34 larger   27.0 \n35 larger   19.0 \n36 larger   25.2 \n37 larger   16.5 \n38 larger   25.2 \n39 larger   24.4 \n40 larger   24.7 \n\n\nLet us denote the weight of individual \\(i\\) by \\(y_i\\). We wish to predict this based on whether the island is large or small, using a linear model. This can be done in full analogy with Equation 11.1, using the following equation:\n\\[\ny_i\n= \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i\n\\tag{12.1}\\]\nOn the surface, this is exactly the same equation as Equation 11.1. But there is one important difference: in an ANOVA, we give a different meaning to the \\(x_i\\). This variable should take on the value 1 if individual i is from the smaller island and the value 0 if it is from the larger island. (It could also be the other way round; it does not matter as long as we choose one convention and stay consistent.) The \\(\\epsilon_i\\) are still the residuals, measuring the deviation of the points from the ideal prediction. What is the “ideal” prediction here? Well, without the \\(\\epsilon_i\\) term, Equation 12.1 reads \\(y_i = \\beta_0 + \\beta_1 x_i\\). If individual \\(i\\) is from the larger island, \\(x_i = 0\\), and we have \\(y_i = \\beta_0\\). So the intercept \\(\\beta_0\\) corresponds to the mean prediction for the weight of the birds in the larger island. Conversely, if individual \\(i\\) is from the smaller island, then \\(x_i = 1\\), and so \\(y_i = \\beta_0 + \\beta_1 \\cdot 1 = \\beta_0 + \\beta_1\\). In other words, \\(\\beta_1\\) is the difference in mean weight between the smaller and the larger island.\nWe can see this in action by running lm and then summary on these data:\n\nlm(weight ~ island, data = bird) |&gt; summary()\n\n\nCall:\nlm(formula = weight ~ island, data = bird)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.478 -3.390 -0.199  2.856 14.212 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     22.185      1.143   19.41   &lt;2e-16 ***\nislandsmaller   -3.024      1.617   -1.87   0.0691 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.112 on 38 degrees of freedom\nMultiple R-squared:  0.08431,   Adjusted R-squared:  0.06021 \nF-statistic: 3.499 on 1 and 38 DF,  p-value: 0.06914\n\n\nThe intercept, 22.185, is the mean weight on the larger island. To get the mean weight on the smaller one, we must add the intercept to the other coefficient, labelled islandsmaller in the table. That sum is 19.161. Indeed, if we compute the means of the two groups, we get exactly the same answers:\n\nbird |&gt;\n  group_by(island) |&gt;\n  summarise(mean = mean(weight)) |&gt;\n  ungroup()\n\n# A tibble: 2 × 2\n  island    mean\n  &lt;chr&gt;    &lt;dbl&gt;\n1 larger  22.185\n2 smaller 19.161\n\n\nThe rest of the regression table now reads exactly as it would in performing simple linear regression. For example, the p-values are the probability of getting coefficients at least as large as the inferred ones just by chance (assuming normal and independent residuals with equal variance across the groups).\nThere is just one question left: why was the \"larger\" island automatically designated as the one corresponding to the intercept? The reason is that R proceeds in alphabetical order, and since \"larger\" comes before \"smaller\" alphabetically, it is automatically treated as the intercept (that is, the group corresponding to \\(x_i = 0\\) instead of \\(x_i = 1\\)). One can override this by using factors (Section 8.3), in which case the lowest factor level will correspond to the intercept.\nSo now we have established an analogy between linear regression and ANOVA for the special case when there are only two groups to compare. The main idea was to introduce the indicator variable \\(x_i\\) which is 1 if observation \\(i\\) belongs in larger and 0 otherwise. We can rewrite Equation 12.1 with more expressive notation for \\(x_i\\) and \\(y_i\\), to make this idea more transparent:\n\\[\n(\\text{weight})_i\n= \\beta_0\n+ \\beta_1 \\cdot (\\text{bird is from larger island})_i\n+ \\epsilon_i\n\\tag{12.2}\\]\nWhat to do when there are more than two groups—say, in the PlantGrowth data where we have three? In that case, we have to introduce one more indicator variable. Say, we could have \\((\\text{group is trt1})_i\\) and \\((\\text{group is trt2})_i\\); the first is 1 if the observation is from treatment 1 (and 0 otherwise), and the second is 1 only if the observation is from treatment 2. If the observation is in the control, then both indicator variables are zero. We can thus write the following equation:\n\\[\n(\\text{weight})_i\n= \\beta_0\n+ \\beta_1 \\cdot (\\text{group is trt1})_i\n+ \\beta_2 \\cdot (\\text{group is trt2})_i\n+ \\epsilon_i\n\\tag{12.3}\\]\nWe can get the usual statistics on them by using summary:\n\nlm(weight ~ group, data = PlantGrowth) |&gt; summary()\n\n\nCall:\nlm(formula = weight ~ group, data = PlantGrowth)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0710 -0.4180 -0.0060  0.2627  1.3690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.0320     0.1971  25.527   &lt;2e-16 ***\ngrouptrt1    -0.3710     0.2788  -1.331   0.1944    \ngrouptrt2     0.4940     0.2788   1.772   0.0877 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6234 on 27 degrees of freedom\nMultiple R-squared:  0.2641,    Adjusted R-squared:  0.2096 \nF-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n\n\nSince ctrl comes earliest in the alphabet, it serves as the intercept. Its estimated value, 5.032, is the mean plant weight in the control group. The estimates in the rows grouptrt1 and grouptrt2 (these names were created by mushing together the name of the column in which the factor is found with the factors’ names themselves) are the \\(\\beta_1\\) and \\(\\beta_2\\) coefficients of Equation 12.3, respectively. That is, they are the deviations from the baseline provided by (Intercept). So we can surmise that the estimated mean of trt1 is 5.032 - 0.371 = 4.661, and that of trt2 is 5.032 + 0.494 = 5.526. Indeed, this is what we find relying on simple summaries computing the means in each group:\n\nPlantGrowth |&gt;\n  group_by(group) |&gt;\n  summarise(meanWeight = mean(weight)) |&gt;\n  ungroup()\n\n# A tibble: 3 × 2\n  group meanWeight\n  &lt;fct&gt;      &lt;dbl&gt;\n1 ctrl       5.032\n2 trt1       4.661\n3 trt2       5.526\n\n\nIt may be clear from the above, but it is good to point out nevertheless, that one does not need to specify the individual indicator variables \\((\\text{group is trt1})_i\\) and \\((\\text{group is trt2})_i\\) and so on by hand. Instead, when defining the model formula as weight ~ group, R automatically assigns them based on the entries of group. It will also automatically name the corresponding coefficients: \\(\\beta_1\\) will instead be grouptrt1 and \\(\\beta_2\\) grouptrt2, in this example. This is very convenient, but it is important to be aware that this substitution is what really goes on under the hood when performing an ANOVA.\nMore generally, the same procedure can be followed for any number of groups the predictor may come from. For \\(n\\) groups of data, we introduce \\(n-1\\) indicator variables which take the value 1 if a data point belongs in the corresponding group, and the value 0 otherwise. Again, R will create these variables automatically, so we do not need to worry about them when defining a linear model."
  },
  {
    "objectID": "Anova.html#sec-diagnostic-anova",
    "href": "Anova.html#sec-diagnostic-anova",
    "title": "12  The Kruskal–Wallis test and one-way ANOVA",
    "section": "12.4 Using diagnostic plots on an ANOVA",
    "text": "12.4 Using diagnostic plots on an ANOVA\nSince ANOVA is just linear regression in disguise, the same assumptions must be fulfilled about the residuals: their variance should not depend on the group, they should be independent, and they should be normally distributed. We can use the same diagnostic plots as we did in Section 11.3:\n\nlibrary(ggfortify)\n\nlm(weight ~ group, data = PlantGrowth) |&gt;\n  autoplot(smooth.colour = NA, colour = \"steelblue\", alpha = 0.7) +\n  theme_bw()\n\n\n\n\nThe only difference is that, since the data are arranged in groups, the fitted values can only take on well-defined discrete values—the inferred means in each. Otherwise, the plots are read and evaluated just as before. In this case, we can see that the assumptions are fulfilled beautifully: the residuals have equal variance in the groups, there is no trend in them, and the quantile-quantile plot shows that they are normally distributed."
  },
  {
    "objectID": "Anova.html#exercises",
    "href": "Anova.html#exercises",
    "title": "12  The Kruskal–Wallis test and one-way ANOVA",
    "section": "12.5 Exercises",
    "text": "12.5 Exercises\n\nThe file daphnia_growth.csv contains data on the growth rate of Daphnia populations that are infected with various parasites. There are four groups of observations: the control (no parasites), infection with Metschnikowia bicuspidata, infection with Pansporella perplexa, and finally, infection with Pasteuria ramosa. Each group has ten replicate observations. Are growth rates affected by parasite load?\n\nBefore doing any tests, visualize and explore the data, and make sure you have a solid expectation for the results of any statistical analysis.\nAnswer the question whether growth rates affected by parasite load by first applying a non-parametric test (and a subsequent non-parametric post-hoc test if needed).\nNext, apply a parametric test in the same way: by applying the test and running post-hoc tests if needed.\nDo not forget to create diagnostic plots, to see if the assumptions behind the parametric test are satisfied to an acceptable degree. Is that the case? And do the results from the parametric and non-parametric tests agree with one another?\n\nIn ponds.csv, measured acidity data (pH) is reported from four different ponds. Do the ponds differ in acidity, and if so, which ones from which others? Answer using both non-parametric and parametric tests, with appropriate post-hoc analyses. Check whether these different methods of analysis agree, and make sure that the assumptions behind the parametric test are satisfied using diagnostic plots. (Note: in this dataset, some values are missing.)"
  },
  {
    "objectID": "Anova.html#footnotes",
    "href": "Anova.html#footnotes",
    "title": "12  The Kruskal–Wallis test and one-way ANOVA",
    "section": "",
    "text": "In Chapter 13 we will see examples where multiple independent factors are varied, and each possible combination results in a separate group. For example, if the effects of three different dosages of vitamin C are examined on the tooth growth of Guinea pigs, and the vitamin is also supplied in two distinct forms of either orange juice or raw ascorbic acid, then there will be \\(3 \\cdot 2 = 6\\) groups, defined by the two factors of dosage and form of supplement.↩︎\nThe Kruskal–Wallis test is sometimes referred to as “non-parametric ANOVA”. While this is perfectly fine as a label, one should be aware that it is, strictly speaking, a misnomer: the Kruskal–Wallis test does not rely on computing variances at all.↩︎"
  },
  {
    "objectID": "Anova_two_way.html#two-way-anova",
    "href": "Anova_two_way.html#two-way-anova",
    "title": "13  Two-way ANOVA and the Scheirer–Ray–Hare test",
    "section": "13.1 Two-way ANOVA",
    "text": "13.1 Two-way ANOVA\nChapter 12 discussed techniques for analyzing data which fall into multiple categories, but those categories are levels of a single factor. Here we go further and work with data classified by two independent factors.\nA good example is provided by the built-in dataset ToothGrowth, which contains data on the tooth growth of Guinea pigs in response to receiving vitamin C.\n\nlibrary(tidyverse)\n\nas_tibble(ToothGrowth) |&gt; print(n = Inf)\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n11  16.5 VC      1  \n12  16.5 VC      1  \n13  15.2 VC      1  \n14  17.3 VC      1  \n15  22.5 VC      1  \n16  17.3 VC      1  \n17  13.6 VC      1  \n18  14.5 VC      1  \n19  18.8 VC      1  \n20  15.5 VC      1  \n21  23.6 VC      2  \n22  18.5 VC      2  \n23  33.9 VC      2  \n24  25.5 VC      2  \n25  26.4 VC      2  \n26  32.5 VC      2  \n27  26.7 VC      2  \n28  21.5 VC      2  \n29  23.3 VC      2  \n30  29.5 VC      2  \n31  15.2 OJ      0.5\n32  21.5 OJ      0.5\n33  17.6 OJ      0.5\n34   9.7 OJ      0.5\n35  14.5 OJ      0.5\n36  10   OJ      0.5\n37   8.2 OJ      0.5\n38   9.4 OJ      0.5\n39  16.5 OJ      0.5\n40   9.7 OJ      0.5\n41  19.7 OJ      1  \n42  23.3 OJ      1  \n43  23.6 OJ      1  \n44  26.4 OJ      1  \n45  20   OJ      1  \n46  25.2 OJ      1  \n47  25.8 OJ      1  \n48  21.2 OJ      1  \n49  14.5 OJ      1  \n50  27.3 OJ      1  \n51  25.5 OJ      2  \n52  26.4 OJ      2  \n53  22.4 OJ      2  \n54  24.5 OJ      2  \n55  24.8 OJ      2  \n56  30.9 OJ      2  \n57  26.4 OJ      2  \n58  27.3 OJ      2  \n59  29.4 OJ      2  \n60  23   OJ      2  \n\n\nAs seen, there are three dosage levels (0.5, 1, and 2) and two types of supplement (VC for vitamin C in the form of raw ascorbic acid, and OJ for orange juice). As usual, we first visualize the data. In doing so, it is useful to convert dose to a factor (Section 8.3): the three dosage levels play the role of a categorical variable (“low”, “medium” ,and “high” levels of vitamin C dosage), and we are not so interested in the actual magnitudes of those dosages.\n\nas_tibble(ToothGrowth) |&gt;\n  mutate(dose = as_factor(dose)) |&gt;\n  ggplot() +\n  aes(x = supp, y = len) +\n  geom_boxplot(alpha = 0.2, outlier.shape = NA,\n               colour = \"steelblue\", fill = \"steelblue\") +\n  geom_jitter(alpha = 0.4, width = 0.05, colour = \"steelblue\") +\n  labs(x = \"vitamin C dosage [mg/day]\", y = \"tooth length [mm]\") +\n  facet_grid(. ~ dose, labeller = label_both) +\n  theme_bw()\n\n\n\n\nIntuitively, we would expect there to be an effect of dosage, because the higher the dosage the longer the teeth become. We would also expect an effect of supplement type, because orange juice seems to perform better (at least no worse) than raw ascorbic acid in facilitating tooth growth. Continuing with the linear models from Chapter 12, it is easy to include two factors:\n\nToothGrowth |&gt;\n  mutate(dose = as_factor(dose)) |&gt;\n  lm(len ~ dose + supp, data = _) |&gt;\n  anova()\n\nAnalysis of Variance Table\n\nResponse: len\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \ndose       2 2426.43 1213.22  82.811 &lt; 2.2e-16 ***\nsupp       1  205.35  205.35  14.017 0.0004293 ***\nResiduals 56  820.43   14.65                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe new feature above is the inclusion of dose + supp as the predictor, instead of just a single one. Mathematically, this translates to the following model: \\[\n\\begin{split}\n(\\text{length})_i &\n= \\beta_0\n+ \\beta_1 \\cdot (\\text{dose is 1})_i\n+ \\beta_2 \\cdot (\\text{dose is 2})_i \\\\ &\n+ \\beta_3 \\cdot (\\text{supplement is VC})_i\n+ \\epsilon_i\n\\end{split}\n\\tag{13.1}\\] where the coefficients \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\) are all multiplied by indicator variables which take on the value 1 if a data point belongs in that category and 0 otherwise. As seen from the ANOVA table above, both dosage and supplement type appear to have a real effect on tooth growth.\nHowever, this model ignores something that might be potentially relevant: the interaction between the two factors. This means that the nature of the relationship between tooth length and one of the predictors depends on the value of the other predictor. For the Guinea pig data, a case can be made based on the plot above that the effect of the supplement type depends on dosage: when the dosage level is either 0.5 or 1 mg/day, orange juice leads to longer teeth than ascorbic acid—but this benefit disappears at the highest dosage level of 2 mg/day.\nAccounting for interaction terms in a statistical model is easy. All one needs to do is add one more term to the formula, denoted dose:supp:\n\nToothGrowth |&gt;\n  mutate(dose = as_factor(dose)) |&gt;\n  lm(len ~ dose + supp + dose:supp, data = _) |&gt;\n  anova()\n\nAnalysis of Variance Table\n\nResponse: len\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \ndose       2 2426.43 1213.22  92.000 &lt; 2.2e-16 ***\nsupp       1  205.35  205.35  15.572 0.0002312 ***\ndose:supp  2  108.32   54.16   4.107 0.0218603 *  \nResiduals 54  712.11   13.19                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe model result confirms that our intuition was likely correct: there does appear to be a real interaction effect between the two factors. Mathematically, the model reads \\[\n\\begin{split}\n(\\text{length})_i &\n= \\beta_0\n+ \\beta_1 \\cdot (\\text{dose is 1})_i\n+ \\beta_2 \\cdot (\\text{dose is 2})_i\n+ \\beta_3 \\cdot (\\text{supplement is VC})_i \\\\ &\n+ \\beta_4 \\cdot (\\text{dose is 1})_i \\cdot(\\text{supplement is VC})_i \\\\ &\n+ \\beta_5 \\cdot (\\text{dose is 2})_i \\cdot(\\text{supplement is VC})_i\n+ \\epsilon_i\n\\end{split}\n\\tag{13.2}\\] where \\(\\beta_4\\) and \\(\\beta_5\\) are multiplied by products of indicator variables. In other words, the \\(\\beta_4\\) term only shows up in the equation if data point \\(i\\) both has a dose of 1 mg/day and a supplement of VC, and \\(\\beta_5\\) only appears if data point \\(i\\) has a 2 mg/day dose and VC supplement.\nThe inclusion of two factors with their interaction is so common in linear models that there is a shorthand notation to make it easier. Writing dose * supp is exactly the same as the above dose + supp + dose:supp. Let us see this in action:\n\nToothGrowth |&gt;\n  mutate(dose = as_factor(dose)) |&gt;\n  lm(len ~ dose * supp, data = _) |&gt;\n  anova()\n\nAnalysis of Variance Table\n\nResponse: len\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \ndose       2 2426.43 1213.22  92.000 &lt; 2.2e-16 ***\nsupp       1  205.35  205.35  15.572 0.0002312 ***\ndose:supp  2  108.32   54.16   4.107 0.0218603 *  \nResiduals 54  712.11   13.19                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe result is identical to what we had before.\nAs in the case of one-way ANOVA, diagnostic plots and post-hoc testing (Tukey test) are useful tools. The diagnostic plots look excellent, so we can be confident about interpreting the p-values and other statistics of the linear model correctly:\n\nlibrary(ggfortify)\n\nToothGrowth |&gt;\n  mutate(dose = as_factor(dose)) |&gt;\n  lm(len ~ dose * supp, data = _) |&gt;\n  autoplot(smooth.colour = NA, colour = \"steelblue\", alpha = 0.7) +\n  theme_bw()\n\n\n\n\nThe Tukey test can be used to compare each factor in isolation, as well as their combinations:\n\nToothGrowth |&gt;\n  mutate(dose = as_factor(dose)) |&gt;\n  lm(len ~ dose * supp, data = _) |&gt;\n  aov() |&gt;\n  TukeyHSD()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = lm(len ~ dose * supp, data = mutate(ToothGrowth, dose = as_factor(dose))))\n\n$dose\n        diff       lwr       upr   p adj\n1-0.5  9.130  6.362488 11.897512 0.0e+00\n2-0.5 15.495 12.727488 18.262512 0.0e+00\n2-1    6.365  3.597488  9.132512 2.7e-06\n\n$supp\n      diff       lwr       upr     p adj\nVC-OJ -3.7 -5.579828 -1.820172 0.0002312\n\n$`dose:supp`\n                diff        lwr         upr     p adj\n1:OJ-0.5:OJ     9.47   4.671876  14.2681238 0.0000046\n2:OJ-0.5:OJ    12.83   8.031876  17.6281238 0.0000000\n0.5:VC-0.5:OJ  -5.25 -10.048124  -0.4518762 0.0242521\n1:VC-0.5:OJ     3.54  -1.258124   8.3381238 0.2640208\n2:VC-0.5:OJ    12.91   8.111876  17.7081238 0.0000000\n2:OJ-1:OJ       3.36  -1.438124   8.1581238 0.3187361\n0.5:VC-1:OJ   -14.72 -19.518124  -9.9218762 0.0000000\n1:VC-1:OJ      -5.93 -10.728124  -1.1318762 0.0073930\n2:VC-1:OJ       3.44  -1.358124   8.2381238 0.2936430\n0.5:VC-2:OJ   -18.08 -22.878124 -13.2818762 0.0000000\n1:VC-2:OJ      -9.29 -14.088124  -4.4918762 0.0000069\n2:VC-2:OJ       0.08  -4.718124   4.8781238 1.0000000\n1:VC-0.5:VC     8.79   3.991876  13.5881238 0.0000210\n2:VC-0.5:VC    18.16  13.361876  22.9581238 0.0000000\n2:VC-1:VC       9.37   4.571876  14.1681238 0.0000058\n\n\n(Again, due to how TukeyHSD is designed, the aov function must be called before one can use it on a linear model fit.) Here we first have a comparison between the dosage levels, averaging over supplement type. Even after this averaging there is a clear difference between the effects of each dosage level, as can be suspected based on a plot which ignores the supp factor:\n\nas_tibble(ToothGrowth) |&gt;\n  mutate(dose = as_factor(dose)) |&gt;\n  ggplot() +\n  aes(x = dose, y = len) +\n  geom_boxplot(alpha = 0.2, outlier.shape = NA,\n               colour = \"steelblue\", fill = \"steelblue\") +\n  geom_jitter(alpha = 0.4, width = 0.05, colour = \"steelblue\") +\n  labs(x = \"vitamin C dosage [mg/day]\", y = \"tooth length [mm]\") +\n  theme_bw()\n\n\n\n\nSimilarly, the difference between the two supplement types appears to be real (the Tukey test gave p adj = 0.0002312), even when not distinguishing by dosage—although this is somewhat less visible on a graph:\n\nas_tibble(ToothGrowth) |&gt;\n  ggplot() +\n  aes(x = supp, y = len) +\n  geom_boxplot(alpha = 0.2, outlier.shape = NA,\n               colour = \"steelblue\", fill = \"steelblue\") +\n  geom_jitter(alpha = 0.4, width = 0.05, colour = \"steelblue\") +\n  labs(x = \"vitamin C dosage [mg/day]\", y = \"tooth length [mm]\") +\n  theme_bw()\n\n\n\n\nFinally, in the $`dose:supp` part of the table, one can compare every particular experimental group (indexed by both dose and supp) with every other.\nIt is possible to use the summary function instead of anova when running the linear model. However, this table is likely not what we are looking for, because instead of having one row per factor and their interaction, it prints one row per fitted parameter. That said, this can sometimes also be useful:\n\nToothGrowth |&gt;\n  mutate(dose = as_factor(dose)) |&gt;\n  lm(len ~ dose * supp, data = _) |&gt;\n  summary()\n\n\nCall:\nlm(formula = len ~ dose * supp, data = mutate(ToothGrowth, dose = as_factor(dose)))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8.20  -2.72  -0.27   2.65   8.27 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    13.230      1.148  11.521 3.60e-16 ***\ndose1           9.470      1.624   5.831 3.18e-07 ***\ndose2          12.830      1.624   7.900 1.43e-10 ***\nsuppVC         -5.250      1.624  -3.233  0.00209 ** \ndose1:suppVC   -0.680      2.297  -0.296  0.76831    \ndose2:suppVC    5.330      2.297   2.321  0.02411 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.631 on 54 degrees of freedom\nMultiple R-squared:  0.7937,    Adjusted R-squared:  0.7746 \nF-statistic: 41.56 on 5 and 54 DF,  p-value: &lt; 2.2e-16\n\n\nThe named coefficients above correspond to the \\(\\beta\\) parameters of Equation 13.2: (Intercept) is \\(\\beta_0\\), dose1 is \\(\\beta_1\\), dose2 is \\(\\beta_2\\), suppVC is \\(\\beta_3\\), dose1:suppVC is \\(\\beta_4\\), and dose2:suppVC is \\(\\beta_5\\)."
  },
  {
    "objectID": "Anova_two_way.html#the-scheirerrayhare-test",
    "href": "Anova_two_way.html#the-scheirerrayhare-test",
    "title": "13  Two-way ANOVA and the Scheirer–Ray–Hare test",
    "section": "13.2 The Scheirer–Ray–Hare test",
    "text": "13.2 The Scheirer–Ray–Hare test\nFor the sake of completeness, we mention that much like in the case of one-way ANOVA, there is a non-parametric version of the two-way ANOVA as well. This is the Scheirer–Ray–Hare test, which is therefore the two-way analogue of the Kruskal–Wallis test. To use this test, one must install and load the package rcompanion:\n\ninstall.packages(\"rcompanion\")\n\nlibrary(rcompanion)\n\nAnd now, we can use the function scheirerRayHare much like kruskal.test or lm:\n\nToothGrowth |&gt;\n  mutate(dose = as_factor(dose)) |&gt;\n  scheirerRayHare(len ~ dose * supp, data = _)\n\n\nDV:  len \nObservations:  60 \nD:  0.999222 \nMS total:  305 \n\n\n          Df  Sum Sq      H p.value\ndose       2 12394.4 40.669 0.00000\nsupp       1  1050.0  3.445 0.06343\ndose:supp  2   515.5  1.692 0.42923\nResiduals 54  4021.1               \n\n\nNote that this test is skeptical about the role of the supplement type, and definitely thinks that the interaction between it and dosage is not different from what one might get by pure chance. This illustrates one problem with the test: it is not very powerful in detecting patterns, even when they are there. To make matters worse, there is no appropriate post-hoc test available in conjunction with the Scheirer–Ray–Hare test. For these reasons, its use is more restricted than of other non-parametric tests, like the Wilcoxon rank sum and Kruskal–Wallis tests. It is good to know about it as an option, but often one must rely on other methods, such as the parametric two-way ANOVA."
  },
  {
    "objectID": "Anova_two_way.html#sec-exercises-anova-two-way",
    "href": "Anova_two_way.html#sec-exercises-anova-two-way",
    "title": "13  Two-way ANOVA and the Scheirer–Ray–Hare test",
    "section": "13.3 Exercises",
    "text": "13.3 Exercises\n\nThe file cow_growth.csv has data on the growth of individual cows which have received different grains (wheat, oats, or barley) and, independently, one of four different dietary supplements (one of which is no supplement, for control). Each of these diet combinations (twelve diets: three grains, times four supplements) had four cows observed. Is there any effect of these treatments on cow growth? Is there any interaction between the grain and the supplement given to the cows—some secret super-combination which makes the cows grow especially well (or poorly)?\n\nAs usual, before doing any tests, visualize and explore the data, and make sure you have a solid expectation for the results of any statistical analysis.\nAnswer the question by applying a parametric test. Run post-hoc tests as well if needed. Do not forget to create diagnostic plots, to see if the assumptions behind the parametric test are satisfied to an acceptable degree.\n\nThe built-in CO2 data frame contains measurements from an experiment on the cold tolerance of the grass species Echinochloa crus-galli. The dataset has five columns:\n\nPlant: unique identifier for each plant individual.\nType: either Quebec or Mississippi, depending on the origin of the plant.\nTreatment: whether the plant individual was chilled or nonchilled for the experiment.\nconc: carbon dioxide concentration in the surrounding environment.\nuptake: carbon dioxide uptake rate.\n\nHow do uptake rates depend on Type, Treatment, and their interaction? (For this exercise, you can ignore Plant and conc.) Start by forming a hypothesis based on visualizing the data. Then perform a parametric test and a corresponding post-hoc test. Make sure to use diagnostic plots to gauge the quality of the test’s assumptions."
  },
  {
    "objectID": "Nonlinear_regression.html#sec-ancova",
    "href": "Nonlinear_regression.html#sec-ancova",
    "title": "14  More general linear models; nonlinear regression",
    "section": "14.1 Combining categorical and continuous variables in linear models",
    "text": "14.1 Combining categorical and continuous variables in linear models\nSo far we have used at most two predictors when dealing with linear models (lm). This was in Chapter 13, where we looked the effects of two categorical variables, as well as their interaction. Chapter 11 introduced the idea of using a continuous, instead of a categorical, predictor. But we have not been combining these.\nIn fact, one can build arbitrarily complicated linear models from an arbitrary combination of continuous and categorical variables, and their interactions. Let us consider the built-in CO2 dataset as an example, which was already used before in an exercise (Section 13.3). Briefly, the data contain measurements from an experiment on the cold tolerance of the grass species Echinochloa crus-galli. The dataset has five columns: Plant (a unique identifier for each plant individual), Type (either Quebec or Mississippi depending on the origin of the plant), Treatment (whether the plant individual was chilled or nonchilled for the experiment), conc (ambient carbon dioxide concentration), and uptake (carbon dioxide uptake rate by the plant).\n\nlibrary(tidyverse)\nas_tibble(CO2)\n\n# A tibble: 84 × 5\n   Plant Type   Treatment   conc uptake\n   &lt;ord&gt; &lt;fct&gt;  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 Qn1   Quebec nonchilled    95   16  \n 2 Qn1   Quebec nonchilled   175   30.4\n 3 Qn1   Quebec nonchilled   250   34.8\n 4 Qn1   Quebec nonchilled   350   37.2\n 5 Qn1   Quebec nonchilled   500   35.3\n 6 Qn1   Quebec nonchilled   675   39.2\n 7 Qn1   Quebec nonchilled  1000   39.7\n 8 Qn2   Quebec nonchilled    95   13.6\n 9 Qn2   Quebec nonchilled   175   27.3\n10 Qn2   Quebec nonchilled   250   37.1\n# ℹ 74 more rows\n\n\nWe can plot the observed distributions of CO2 uptake rates for each type and treatment:\n\nas_tibble(CO2) |&gt;\n  ggplot(aes(x = 0, y = uptake)) +\n  geom_boxplot(colour = \"steelblue\", fill = \"steelblue\",\n               alpha = 0.2, outlier.shape = NA) +\n  geom_jitter(colour = \"steelblue\", alpha = 0.5, width = 0.05) +\n  facet_grid(Type ~ Treatment) +\n  ylab(\"uptake rate\") +\n  theme_bw() +\n  theme(axis.title.x = element_blank(), # The x-axis is meaningless here,\n        axis.ticks.x = element_blank(), # so remove title, tick marks,\n        axis.text.x = element_blank())  # and labels from it\n\n\n\n\nThis, however, is only part of the story, as becomes obvious if we also plot the ambient CO2 concentrations (conc) along the x-axis:\n\nas_tibble(CO2) |&gt;\n  ggplot(aes(x = conc, y = uptake)) +\n  geom_point(colour = \"steelblue\", alpha = 0.8) +\n  facet_grid(Type ~ Treatment) +\n  labs(x = \"concentration\", y = \"uptake rate\") +\n  theme_bw()\n\n\n\n\nWe see that there is also a clear, saturating relationship between CO2 concentration and uptake rates that is definitely not linear. This does not mean that a linear model is useless for analyzing these data: the trend of whether the data increase or decrease can still be captured (although it is not recommended to use the model for numerical prediction purposes). One model that may come to mind is as follows:\n\nlm(uptake ~ conc + Type * Treatment, data = CO2) |&gt; anova()\n\nAnalysis of Variance Table\n\nResponse: uptake\n               Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nconc            1 2285.0  2285.0 63.5032 1.002e-11 ***\nType            1 3365.5  3365.5 93.5330 4.787e-15 ***\nTreatment       1  988.1   988.1 27.4611 1.300e-06 ***\nType:Treatment  1  225.7   225.7  6.2733   0.01432 *  \nResiduals      79 2842.6    36.0                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn other words, the uptake rates are modeled via a combination of the effect of concentration (a continuous variable) plus the interaction of type and treatment (two categorical variables). Recall that Type * Treatment is shorthand for Type + Treatment + Type:Treatment, the sum of the main effects and the interaction between them. Mathematically, the equation for the model reads: \\[\n\\begin{split}\n(\\text{uptake})_i &\n= \\beta_0\n+ \\beta_1 \\cdot (\\text{conc})_i\n+ \\beta_2 \\cdot (\\text{Type is Mississippi})_i \\\\ &\n+ \\beta_3 \\cdot (\\text{Treatment is chilled})_i \\\\ &\n+ \\beta_4 \\cdot (\\text{Type is Mississippi})_i \\cdot(\\text{Treatment is chilled})_i\n+ \\epsilon_i\n\\end{split}\n\\tag{14.1}\\] where \\((\\text{conc})_i\\) is a continuous predictor and not an indicator variable—that is, it takes on the actual value of the CO2 concentration in observation \\(i\\). By contrast, \\((\\text{Type is Mississippi})_i\\) and \\((\\text{Treatment is chilled})_i\\) are indicator variables that take on the value 1 if data point \\(i\\) belongs in their category and 0 otherwise.\nThe rationale for having chosen the model uptake ~ conc + Type * Treatment is that the box plots above reveal a potential interaction between the two factors Type and Treatment (the effect of changing Treatment from chilled to nonchilled depends on whether the Type was Quebec or Mississippi), and on top of this, we also want to capture the positive dependence on CO2 concentration. The ANOVA table above concurs: each of these categories come out with low p-values, indicating that what we see is unlikely to be due to just chance. To make sure that the assumptions on which this interpretation rests are held, we look at the diagnostic plots:\n\nlibrary(ggfortify)\n\nlm(uptake ~ conc + Type * Treatment, data = CO2) |&gt;\n  autoplot(smooth.colour = NA, colour = \"steelblue\", alpha = 0.7) +\n  theme_bw()\n\n\n\n\nThe Q-Q plot is not good: in the lower quantiles, the realized residuals are consistently larger in magnitude than the theoretical expectation based on the assumption of normality. This, of course, is a consequence of the data depending on concentrations in a manifestly nonlinear way. Apart from the Q-Q plot however, the diagnostics look surprisingly good. We will come back to the point about nonlinearity in Section 14.2.\nIt is also informative to apply the function summary on the model fit in addition to anova, to obtain the regression slopes and intercept (the \\(\\beta\\) parameters of Equation 14.1):\n\nlm(uptake ~ conc + Type * Treatment, data = CO2) |&gt; summary()\n\n\nCall:\nlm(formula = uptake ~ conc + Type * Treatment, data = CO2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.4240  -2.3674   0.7641   3.8749   9.6278 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      27.620528   1.627945  16.966  &lt; 2e-16 ***\nconc                              0.017731   0.002225   7.969 1.00e-11 ***\nTypeMississippi                  -9.380952   1.851185  -5.068 2.59e-06 ***\nTreatmentchilled                 -3.580952   1.851185  -1.934   0.0566 .  \nTypeMississippi:Treatmentchilled -6.557143   2.617972  -2.505   0.0143 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.999 on 79 degrees of freedom\nMultiple R-squared:  0.7072,    Adjusted R-squared:  0.6923 \nF-statistic: 47.69 on 4 and 79 DF,  p-value: &lt; 2.2e-16\n\n\nRegardless of how good this model looks, one can argue based on the plot of the data that there could also be an interaction between conc and the other two factors. After all, the saturation levels of the uptake rate are always higher in Quebec than in Mississippi, and the effect of chilling also depends on Type. A model which accounts for all these effects and their interactions is uptake ~ conc * Type * Treatment. Mathematically: \\[\n\\begin{split}\n(\\text{uptake})_i &\n= \\beta_0\n+ \\beta_1 \\cdot (\\text{conc})_i\n+ \\beta_2 \\cdot (\\text{Type is Mississippi})_i \\\\ &\n+ \\beta_3 \\cdot (\\text{Treatment is chilled})_i \\\\ &\n+ \\beta_4 \\cdot (\\text{conc})_i \\cdot (\\text{Type is Mississippi})_i \\\\ &\n+ \\beta_5 \\cdot (\\text{conc})_i \\cdot (\\text{Treatment is chilled})_i \\\\ &\n+ \\beta_6 \\cdot (\\text{Type is Mississippi})_i \\cdot (\\text{Treatment is chilled})_i \\\\ &\n+ \\beta_7 \\cdot (\\text{conc})_i \\cdot (\\text{Type is Mississippi})_i\n\\cdot (\\text{Treatment is chilled})_i\n+ \\epsilon_i\n\\end{split}\n\\tag{14.2}\\]\n(The \\(\\beta_7\\) term is multiplied by a three-way interaction of concentration, type, and treatment.) Fitting the model and creating diagnostic plots:\n\nlm(uptake ~ conc * Type * Treatment, data = CO2) |&gt; anova()\n\nAnalysis of Variance Table\n\nResponse: uptake\n                    Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nconc                 1 2285.0  2285.0  68.1766 3.545e-12 ***\nType                 1 3365.5  3365.5 100.4164 1.516e-15 ***\nTreatment            1  988.1   988.1  29.4821 6.512e-07 ***\nconc:Type            1  208.0   208.0   6.2060   0.01491 *  \nconc:Treatment       1   31.9    31.9   0.9509   0.33258    \nType:Treatment       1  225.7   225.7   6.7350   0.01134 *  \nconc:Type:Treatment  1   55.5    55.5   1.6570   0.20192    \nResiduals           76 2547.2    33.5                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlm(uptake ~ conc * Type * Treatment, data = CO2) |&gt; summary()\n\n\nCall:\nlm(formula = uptake ~ conc * Type * Treatment, data = CO2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.3773  -2.7602   0.9517   3.7368  10.7414 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                           25.585034   2.255256  11.345  &lt; 2e-16 ***\nconc                                   0.022410   0.004295   5.218 1.52e-06 ***\nTypeMississippi                       -7.131741   3.189414  -2.236   0.0283 *  \nTreatmentchilled                      -4.163993   3.189414  -1.306   0.1956    \nconc:TypeMississippi                  -0.005171   0.006074  -0.851   0.3973    \nconc:Treatmentchilled                  0.001340   0.006074   0.221   0.8259    \nTypeMississippi:Treatmentchilled      -1.747509   4.510513  -0.387   0.6995    \nconc:TypeMississippi:Treatmentchilled -0.011057   0.008589  -1.287   0.2019    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.789 on 76 degrees of freedom\nMultiple R-squared:  0.7376,    Adjusted R-squared:  0.7134 \nF-statistic: 30.52 on 7 and 76 DF,  p-value: &lt; 2.2e-16\n\nlm(uptake ~ conc * Type * Treatment, data = CO2) |&gt;\n  autoplot(smooth.colour = NA, colour = \"steelblue\", alpha = 0.7) +\n  theme_bw()\n\n\n\n\nThis confirms what we saw on the plot of the data: that the basic shape of the relationship between concentration and uptake is unaffected by either Type or Treatment (i.e., the term conc:Type:Treatment in the ANOVA table has a high associated p-value). It also illustrates the general point that there are very often multiple candidate models, and choosing between them is a question of judgment, trial-and-error, and successively improving the model structure based on results from earlier modeling attempts."
  },
  {
    "objectID": "Nonlinear_regression.html#sec-nonlin-regression",
    "href": "Nonlinear_regression.html#sec-nonlin-regression",
    "title": "14  More general linear models; nonlinear regression",
    "section": "14.2 Nonlinear regression",
    "text": "14.2 Nonlinear regression\nThe relationship between CO2 concentration and uptake rates are definitely not linear, regardless of treatment or type. So the question arises: can one fit a nonlinear function to these data? As an example, let us focus on just Quebec and the nonchilled treatment, to better illustrate the ideas behind nonlinear regression. Here are the data:\n\nas_tibble(CO2) |&gt;\n  filter(Type == \"Quebec\", Treatment == \"nonchilled\") |&gt;\n  ggplot(aes(x = conc, y = uptake)) +\n  geom_point(colour = \"steelblue\", alpha = 0.8) +\n  labs(x = \"concentration\", y = \"uptake rate\") +\n  theme_bw()\n\n\n\n\nIf the function we wish to fit is not linear, we have to specify its shape. One commonly used shape for describing the above saturating pattern is the Michaelis–Menten curve. This is given by the following equation: \\[ \\rho = \\frac{V c}{K + c} \\] Here \\(\\rho\\) is the uptake rate, \\(c\\) is the concentration, and \\(V\\) and \\(K\\) are two parameters which can modify the shape of the function. The figure below illustrates what curves one can get by varying these parameters:\n\nexpand_grid(V = c(10, 20, 30), # This function creates a tibble with all\n            K = c(1, 5, 10),   # possible combinations of the inputs\n            concentration = seq(0, 60, l = 201)) |&gt;\n  group_by(V, K) |&gt;\n  mutate(uptake = V * concentration / (K + concentration)) |&gt;\n  ungroup() |&gt;\n  ggplot() +\n  aes(x = concentration, y = uptake) +\n  geom_line(colour = \"steelblue\") +\n  facet_grid(V ~ K, labeller = label_both) +\n  theme_bw()\n\n\n\n\nThe task is to find the values of \\(V\\) and \\(K\\) that provide the best fit to the data. Like in the case of linear regression, this can be done via the least-squares criterion: the best fit is provided by the curve which minimizes the sum of the squared deviations of the observed points from it. Unlike with linear regression however, this curve can be very difficult to find. In fact, there is no known general procedure that would be able to minimize the sum of squares under all circumstances. What algorithms can do is to find the best fit, given some initial guesses for the parameters that are at least not violently off of the true values. Just how close the guess needs to be is context-dependent, and highlights an important problem: nonlinear regression can be as much an art as it is a science. For the types of curves we will be fitting though, the more subtle problems will never come up, and a “good enough” initial guess can vary within a relatively wide range.\nSo, how can one guess the values of \\(V\\) and \\(K\\)? To do this, one has to have an understanding of how the parameters influence the curves. For \\(V\\), this interpretation is not difficult to infer. Notice that if concentrations are very, very large, then in the denominator of the formula \\(\\rho = V c / (K + c)\\), we might as well say that \\(K + c\\) is approximately equal to \\(c\\) (if \\(c\\) is a million and \\(K\\) is one, then one is justified in treating the sum as being about one million still). This means that for large \\(c\\), the formula reduces to \\(\\rho \\approx V c / c = V\\). In other words \\(V\\) is the saturation uptake rate: the maximum value of the uptake. This, incidentally, is clearly visible in the plots above: when \\(V\\) is 10 (top row), the curves always tend towards 10 for large concentrations; when \\(V\\) is 20, they tend towards 20 (middle row), and when \\(V\\) is 30, they tend towards 30.\nThe interpretation of \\(K\\) is slightly less straightforward, but still simple. To see what it means, let us ask what the uptake rate would be, were the concentration’s value equal to \\(K\\). In that case, we get \\(\\rho = V K / (K + K)\\) (we simply substituted \\(c = K\\) into the formula), or \\(\\rho = VK / (2K) = V/2\\). That is, \\(K\\) is the concentration at which the uptake rate reaches half its maximum.\nLooking at the data again, both these parameters can be roughly estimated:\n\nas_tibble(CO2) |&gt;\n  filter(Type == \"Quebec\", Treatment == \"nonchilled\") |&gt;\n  ggplot(aes(x = conc, y = uptake)) +\n  geom_point(colour = \"steelblue\", alpha = 0.8) +\n  geom_hline(yintercept = 43, linetype = \"dashed\", colour = \"steelblue\") +\n  annotate(geom = \"segment\", x = 0, y = 43/2, xend = 125, yend = 43/2,\n           linetype = \"dashed\", colour = \"steelblue\") +\n  annotate(geom = \"segment\", x = 125, y = 43/2, xend = 125, yend = 0,\n           linetype = \"dashed\", colour = \"steelblue\") +\n  scale_x_continuous(name = \"concentration\",\n                     limits = c(0, NA), expand = c(0, 0)) +\n  scale_y_continuous(name = \"uptake rate\",\n                     limits = c(0, NA), expand = c(0, 0)) +\n  theme_bw()\n\n\n\n\nSo guessing that \\(V\\) is about 43 and \\(K\\) is about 125 seems to be close to the mark.\nTo actually perform the nonlinear regression, one can use the nls function (“Nonlinear Least Squares”). It begins much like lm, taking a formula and a data frame. However, the formula is no longer a shorthand notation for a linear model, and therefore has to be entered literally. Additionally, there is another argument to nls called start; this is where the starting values have to be specified. The start argument has to be in the form of a list. Lists are an important data structure, worth a little interlude to explain how they work.\n\n14.2.1 Interlude: lists\nLists are like vectors except they can hold arbitrary data in their entries. So unlike vectors which are composed of either all numbers or all character strings or all logical values, lists may have a combination of these. Furthermore, list entries are not restricted to elementary types: vectors, or even data frames may also be list entries. To define a list, all one needs to do is type list, and then give a sequence of named entries. For example, the following creates a list with three entries: a will be equal to 3, b to the string \"Hello!\", and myTable to a small tibble.\n\nlist(a = 3, b = \"Hello!\", myTable = tibble(x = c(1, 2), y = c(3, 4)))\n\n$a\n[1] 3\n\n$b\n[1] \"Hello!\"\n\n$myTable\n# A tibble: 2 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     3\n2     2     4\n\n\nOne can refer to the entries of the list either with the $ notation, or using double brackets. Assigning the above list to a variable called myList first:\n\nmyList &lt;- list(\n  a = 3,\n  b = \"Hello!\",\n  myTable = tibble(x = c(1, 2), y = c(3, 4))\n)\n\nWe can now access the entries of myList either as\n\nmyList$myTable # Access the entry called myTable in the list\n\n# A tibble: 2 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     3\n2     2     4\n\n\nOr as\n\nmyList[[3]] # Access the third entry (myTable) in the list\n\n# A tibble: 2 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     3\n2     2     4\n\n\nThe $ notation is the same as when one accesses columns of data frames. This is not a coincidence: internally, data frames are represented as lists of columns, with each column being a vector. It follows that the double-bracket notation can also be used in conjunction with data frames: to access the first column of CO2, we can not only write CO2$Plant, but also CO2[[1]].\nA list seems to be just a more flexible version of a vector—so why would we want to use vectors instead of lists in the first place? The answer has to do with efficiency: the price to pay for the increased flexibility offered by lists is that they are slower to do operations on. While this would not be a problem for the applications found in this book, it can become important when dealing with large datasets or heavy numerical computations. As a corollary, R has many useful functions which work on vectors but do not work by default on lists. To mention just the simplest examples: mean, median, sd, and sum will throw an error when applied to lists. That is,\n\nsum(c(1, 2, 3))\n\n[1] 6\n\n\nreturns the expected 6, because it was applied to a vector. But the same expression results in an error when the vector is replaced by a list:\n\nsum(list(1, 2, 3))\n\nError in sum(list(1, 2, 3)): invalid 'type' (list) of argument\n\n\n\n\n14.2.2 Back to nonlinear regression\nWith this brief introduction to lists, we now understand what it means that the start argument to nls must be a list, with the entries named after the parameters to be fitted. Using the previously-guessed values of \\(V\\) and \\(K\\) being around 43 and 125, respectively, means we can use start = list(V = 43, K = 125). We can now look at the nls function and use it to produce a fit of the Michaelis–Menten curve to our data:\n\nnonlinearFit &lt;- as_tibble(CO2) |&gt;\n  filter(Type == \"Quebec\", Treatment == \"nonchilled\") |&gt;\n  nls(uptake ~ V*conc/(K + conc), data = _, start = list(V = 43, K = 125))\n\nprint(nonlinearFit)\n\nNonlinear regression model\n  model: uptake ~ V * conc/(K + conc)\n   data: filter(as_tibble(CO2), Type == \"Quebec\", Treatment == \"nonchilled\")\n     V      K \n 51.36 136.32 \n residual sum-of-squares: 319.2\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 9.728e-06\n\n\nObserve that in the formula, we use the column name conc when we want to use the data inside that column, but use made-up names (in this case, V and K) for the unknown parameters we are trying to obtain. Their starting values were filled in from our earlier visual estimation. From these starting values, the best fitting solution is found. We see that their values are 51.36 for \\(V\\) and 136.32 for \\(K\\).\nThe result of nls can be used inside summary to get more information on the fitted parameters (note: the anova function is not applicable to nonlinear regression). Doing so results in the following regression table:\n\nsummary(nonlinearFit)\n\n\nFormula: uptake ~ V * conc/(K + conc)\n\nParameters:\n  Estimate Std. Error t value Pr(&gt;|t|)    \nV   51.359      2.831  18.140 1.86e-13 ***\nK  136.319     27.027   5.044 7.21e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.099 on 19 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 9.728e-06\n\n\nAs in the case of linear regression, the statistical analysis will only be reliable if the assumptions of the independence, normality, and homoscedasticity of the residuals are maintained.\nTo conclude this section, it can be useful to plot the data together with the fitted nonlinear curve, to make sure visually that the fit is reasonable. There are several possible ways of doing this; here we discuss two of them. First, one can rely on the predict function (Section 11.5) that will tell us, for each value of the predictor, what the model-predicted values are. In the same way as for models generated by lm or mblm,\n\npredict(nonlinearFit)\n\n [1] 21.09265 28.87030 33.23631 36.96287 40.35655 42.72982 45.19795 21.09265\n [9] 28.87030 33.23631 36.96287 40.35655 42.72982 45.19795 21.09265 28.87030\n[17] 33.23631 36.96287 40.35655 42.72982 45.19795\n\n\nreturns a vector for each conc in the original data, documenting what the model thinks the corresponding uptake rate ought to be. They can then be compared with the data:\n\nas_tibble(CO2) |&gt;\n  filter(Type == \"Quebec\", Treatment == \"nonchilled\") |&gt;\n  mutate(pred = predict(nonlinearFit)) |&gt;\n  ggplot() +\n  geom_point(aes(x = conc, y = uptake), colour = \"steelblue\", alpha = 0.8) +\n  geom_line(aes(x = conc, y = pred), linetype = \"dashed\", alpha = 0.5) +\n  labs(x = \"concentration\", y = \"uptake rate\") +\n  theme_bw()\n\n\n\n\nIn the above plot, the data points and the predictions each had their own set of aeshetics. For this reason, the aesthetic mappings were not defined separately, but locally inside each geom_. This is perfectly legal, and can help whenever different geometries take different aesthetics from the data. Second, notice that the prediction was drawn with a dashed, semi-transparent line. This is intentional, to make it distinct from data. It signals that the curve does not correspond to data we are plotting, but to a model’s predictions.\nThe second method can be useful if the data are sufficiently scarce that the fitted line looks “rugged”, a bit too piecewise (this can be seen in the above example as well if one looks carefully). In that case, it is possible to draw the curve of any function using geom_function. We can use the fitted parameters in drawing it, and it will not suffer from being too piecewise:\n\nV_fitted &lt;- coef(nonlinearFit)[\"V\"] # Get fitted values of V and K\nK_fitted &lt;- coef(nonlinearFit)[\"K\"] # from the vector of coefficients\n\nas_tibble(CO2) |&gt;\n  filter(Type == \"Quebec\", Treatment == \"nonchilled\") |&gt;\n  ggplot() +\n  geom_point(aes(x = conc, y = uptake), colour = \"steelblue\", alpha = 0.8) +\n  geom_function(fun = function(x) V_fitted * x / (K_fitted + x),\n                linetype = \"dashed\", alpha = 0.5) +\n  labs(x = \"concentration\", y = \"uptake rate\") +\n  theme_bw()\n\n\n\n\nThe result is much the same as before, although carefully looking at the dashed line shows that the second curve is smoother than the first. In this case, this does not matter much, but it could be aesthetically more relevant in others."
  },
  {
    "objectID": "Nonlinear_regression.html#exercises",
    "href": "Nonlinear_regression.html#exercises",
    "title": "14  More general linear models; nonlinear regression",
    "section": "14.3 Exercises",
    "text": "14.3 Exercises\n\nExponential growth\nLet \\(N(t)\\) be the population abundance of some organism at time \\(t\\). An exponentially growing population increases according to \\[ N(t) = N_0 \\mathrm{e}^{rt} \\] where \\(N_0\\) is the initial population size at time \\(t=0\\), and \\(r\\) is the exponential rate of increase.\n\nDownload the data file pop_growth_1.csv and load it in R.\nUse nls() to fit the above exponential growth model to this dataset. Do not treat \\(N_0\\) as a free parameter, but instead use the actual population size at time \\(t=0\\). This leaves \\(r\\) as the only parameter to be fitted. Do so, using an appropriate starting value.\nAssume that the data describes a population of water lilies, and that a single ‘individual’ weighs 1 gram. If the population would continue to grow unrestricted, what would be its biomass after nine months (about 280 days)? What object would have a comparable weight to this population, and what does that tell us about unrestricted population growth in general?\n\n\n\nNitrogen uptake\nCedergreen and Madsen (2002) reported data from an experiment on nitrogen uptake by the duckweed Lemna minor, where the predictor variable is the initial substrate concentration and the response variable is the uptake rate. In this type of experiment, it is anticipated that the uptake will increase as the concentration increases, approaching a horizontal asymptote. The data are available in uptake.csv.\n\nCreate a plot of the data, with the nitrogen concentrations along the x-axis and the corresponding uptake rates along the y-axis.\nFit a Michaelis-Menten model (describing saturating dynamics) to the data. This model is given by \\[ \\rho = \\frac{V c}{K + c} \\] where \\(V\\) and \\(K\\) are two constants, \\(c\\) is the concentration, and \\(\\rho\\) the uptake rate. Make initial guesses for the two parameters \\(V\\) and \\(K\\) based on the graph, and perform the nonlinear regression.\nGiven your nonlinear regression results, what is the maximum possible nitrogen uptake rate of L. minor?\n\n\n\nLogistic growth\nThe simplest model illustrating population regulation and regulated growth is the logistic model, defined by the differential equation \\[ \\frac{\\mathrm{d} N(t)}{\\mathrm{d} t} = rN(t) \\left( 1 - \\frac{N(t)}{K} \\right) \\] Here \\(N(t)\\) is the population abundance at time \\(t\\), \\(r\\) is the exponential growth rate of the population when rare, and \\(K\\) is the maximum abundance it can sustainably achieve (the “carrying capacity”). It should be obvious that when \\(N(t) = K\\), the derivative vanishes, signalling that the population size no longer changes.\nThe above differential equation is one of the few which can be solved explicitly. Its solution reads \\[ N(t) = N_0 \\frac{\\mathrm{e}^{rt}}{1-(1-\\mathrm{e}^{rt})(N_0/K)} \\] where \\(N_0\\) is the initial population size at time \\(t=0\\). Let us fit this model to some population growth data.\n\nDownload the data file pop_growth_2.csv, load it in R, and plot the population abundances against time.\nFit the above model to the data using the nls() function, with appropriate guesses for the starting values of \\(r\\) and \\(K\\).\nPlot the data and the model prediction together. What are the estimated values of \\(r\\) and \\(K\\)?\n\n\n\n\n\nCedergreen, Nina, and Tom Vindbæk Madsen. 2002. “Nitrogen uptake by the floating macrophyte Lemna minor.” New Phytologist 155 (2): 285–92. https://doi.org/10.1046/j.1469-8137.2002.00463.x."
  },
  {
    "objectID": "Intro_map.html#introduction",
    "href": "Intro_map.html#introduction",
    "title": "15  Higher-order functions and mapping",
    "section": "15.1 Introduction",
    "text": "15.1 Introduction\n\nlibrary(tidyverse) # Loading the tidyverse, before doing anything else\n\nIn Chapter 3 we learned how to create user-defined functions. An example was provided in Section 12.2, where we made our life easier by eliminating the need to always call aov before performing a Tukey test with TukeyHSD. Without the function, we must write:\n\nlm(weight ~ group, data = PlantGrowth) |&gt; aov() |&gt; TukeyHSD()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = lm(weight ~ group, data = PlantGrowth))\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\n\nInstead, we can write a simple function that takes a linear model fit object as its input and produces the Tukey test as its output:\n\ntukeyTest &lt;- function(modelFit) modelFit |&gt; aov() |&gt; TukeyHSD()\n\n\n\n\n\n\n\nNote\n\n\n\nMake sure to review Chapter 3, especially Section 3.1.1, if you need a refresher on how to define functions in R.\n\n\nUsing this function, we can now simply write:\n\nlm(weight ~ group, data = PlantGrowth) |&gt; tukeyTest()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = modelFit)\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\n\nAnother useful function we can write helps use statistical procedures within a pipeline. As we have seen, most statistical functions take a formula as their first argument and the data as their second (and then they may take further, method-specific arguments as well, like conf.int in the function wilcox.test). Since the pipe operator |&gt; is often used in conjunction with functions like select, mutate, pivot_longer, summarise, etc. which all return a data frame, it would be convenient to reverse the order of arguments in all statistical functions, with the data coming first and the formula coming second. In fact, such a function is easy to write. We could call it tidystat:\n\ntidystat &lt;- function(data, formula, method) method(formula, data)\n\nHere method is the statistical function we wish to use. For example:\n\nPlantGrowth |&gt;\n  filter(group != \"ctrl\") |&gt;\n  tidystat(weight ~ group, wilcox.test)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n\n\nThis is now fully equivalent to:\n\nPlantGrowth |&gt;\n  filter(group != \"ctrl\") |&gt;\n  wilcox.test(weight ~ group, data = _)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n\n\nWe can add a further improvement to the tidystat function. As it is, it can only take the formula and the data as inputs, but not any other, function-specific arguments. In R, there is a way of passing arbitrary extra arguments, using the ellipsis (...). We can redefine tidystat this way:\n\ntidystat &lt;- function(data, formula, method, ...) {\n  method(formula, data, ...) # The ... means \"possibly more arguments\"\n}\n\nAnd now, we can pass extra arguments that we would not have been able to do before. For instance, we can request confidence intervals from wilcox.test:\n\nPlantGrowth |&gt;\n  filter(group != \"ctrl\") |&gt;\n  tidystat(weight ~ group, wilcox.test, conf.int = TRUE, conf.level = 0.99)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n99 percent confidence interval:\n -1.70 -0.03\nsample estimates:\ndifference in location \n                -0.945 \n\n\nFrom now on, we can use tidystat for performing various statistical procedures:\n\nlibrary(FSA) # For the Dunn test\nPlantGrowth |&gt; tidystat(weight ~ group, lm) |&gt; anova()\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ngroup      2  3.7663  1.8832  4.8461 0.01591 *\nResiduals 27 10.4921  0.3886                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPlantGrowth |&gt; tidystat(weight ~ group, lm) |&gt; tukeyTest()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = modelFit)\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\nPlantGrowth |&gt; tidystat(weight ~ group, kruskal.test)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  weight by group\nKruskal-Wallis chi-squared = 7.9882, df = 2, p-value = 0.01842\n\nPlantGrowth |&gt; tidystat(weight ~ group, dunnTest)\n\n   Comparison         Z    P.unadj      P.adj\n1 ctrl - trt1  1.117725 0.26368427 0.26368427\n2 ctrl - trt2 -1.689290 0.09116394 0.18232789\n3 trt1 - trt2 -2.807015 0.00500029 0.01500087"
  },
  {
    "objectID": "Intro_map.html#higher-order-functions",
    "href": "Intro_map.html#higher-order-functions",
    "title": "15  Higher-order functions and mapping",
    "section": "15.2 Higher-order functions",
    "text": "15.2 Higher-order functions\nIf we think about the tidystat function above, something strange has happened. We are used to the idea of functions taking numbers, character strings, logical values, or even data frames as their arguments. What we have not paid much attention to before is what happens if the input to a function is itself another function. Yet this is precisely what tidystat does: its method argument is a function object.\nIn R, this is perfectly legal, and its use and interpretation is every bit as natural as it was in tidystat. Functions which take other functions as arguments are often called higher-order functions.1 To emphasize again: there really is nothing special about such functions, and they can be used in much the same way as “ordinary” functions.\nOne very natural example for a higher-order function is integration. An integral (at least in simple cases) takes three inputs: a function to integrate, a lower limit of integration, and an upper limit of integration. The output is the (sign-weighted) area under the function’s curve, evaluated between the lower and upper limits. This is stressed even in the usual mathematical notation for integrals: when we write \\[ \\int_0^1 x^2 \\,\\text{d} x = \\frac{1}{3}\\] (a true statement), we show the lower and upper limits of 0 and 1 at the bottom and top of the integral sign, and the function to be integrated (in this case, \\(f(x) = x^2\\)) in between the integral sign and \\(\\text{d} x\\).\nIf you do not know how integrals do their magic, there is no need to worry, because R has a built-in function called integrate to do the calculations for you. integrate takes the three arguments described above: the function to integrate, and the lower and upper limits of integration. To perform the above integral, we can write:\n\n# The squaring function: sqr(2) returns 4, sqr(4) returns 16, etc.\nsqr &lt;- function(x) x^2\n# Perform the integral between 0 and 1:\nintegrate(sqr, 0, 1)\n\n0.3333333 with absolute error &lt; 3.7e-15\n\n\nThe answer is indeed one-third.2 But there was no obligation to use the square function above. We could have used any other one. For instance, to compute the integral of the cosine function \\(\\cos(x)\\) between \\(0\\) and \\(2\\pi\\), we can type:\n\nintegrate(cos, 0, 2*pi)\n\n4.359836e-16 with absolute error &lt; 4.5e-14\n\n\nWe get the expected result of zero, within numerical error.\nOne thing to know about function objects like sqr is that they do not need a name to be used. In the definition sqr &lt;- function(x) x^2, we assigned the function object function(x) x^2 to the symbol sqr, so we wouldn’t have to write it out all the time. But since sqr is just a name that stands for function(x) x^2, calling (function(x) x^2)(4) is the same as calling sqr(4), both returning 16. If a function is used only once within another (higher-order) function, then we might not wish to bother with naming the function separately. Thus, the following is exactly equivalent to integrating the sqr function:\n\nintegrate(function(x) x^2, 0, 1)\n\n0.3333333 with absolute error &lt; 3.7e-15\n\n\nFunctions without names are often called anonymous functions. They are commonly used within other, higher-order functions. Their use is not mandatory: it is always possible to first define the function with a name, and then use that name instead (e.g., using sqr instead of function(x) x^2, after defining sqr &lt;- function(x) x^2). However, they can be convenient, and it is also important to recognize them in R code written by others."
  },
  {
    "objectID": "Intro_map.html#sec-mapfamily",
    "href": "Intro_map.html#sec-mapfamily",
    "title": "15  Higher-order functions and mapping",
    "section": "15.3 The map family of functions",
    "text": "15.3 The map family of functions\nThe purrr package is a standard, automatically-loaded part of the tidyverse. It contains a large family of mapping functions. These allow one to perform repetitive tasks by applying the same function to all elements of a vector, list, or column in a data frame.\nTo illustrate their use, how would we obtain the squares of all integers from 1 to 10? Using our earlier sqr function, we could painstakingly type out sqr(1), then sqr(2), and so on, up until sqr(10) (we ought to be grateful that the task was to obtain the squares of the first ten integers, instead of the first ten thousand). But there is no need to do this, as this is exactly what map can do. map takes two arguments: some data (e.g., a vector of values), and a function. It then applies that function to all data entries. So a much quicker way of obtaining the squares of all integers from 1 to 10 is this:\n\nmap(1:10, sqr)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n[[6]]\n[1] 36\n\n[[7]]\n[1] 49\n\n[[8]]\n[1] 64\n\n[[9]]\n[1] 81\n\n[[10]]\n[1] 100\n\n\nOr, in case we prefer anonymous functions and do not want to bother with defining our own sqr routine:\n\nmap(1:10, function(x) x^2)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n[[6]]\n[1] 36\n\n[[7]]\n[1] 49\n\n[[8]]\n[1] 64\n\n[[9]]\n[1] 81\n\n[[10]]\n[1] 100\n\n\nThe results are all there, although they are prefaced by double-bracketed indices [[1]], [[2]], and so on. You may recall from Section 14.2.1 that this is the notation used to reference the entries of lists. That is correct: map returns a list of values, not a vector. We will see momentarily that this can be very useful behavior, but here it can feel overkill. Fortunately, it is easy to get back a vector instead of a list. Since the entries of vectors must have a well-defined, uniform type (numeric, character string, logical, etc.), we have to tell R what kind of result we want. In our case, we want numeric results. The function to do this is called map_dbl (“map into double-precision numerical values”). It can be used just like map; the only difference between the two is that the output type changes from list to numeric vector:\n\nmap_dbl(1:10, function(x) x^2)\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\nSimilarly, there are functions map_chr, map_lgl, and some others, which create vectors of the appropriate type. For example, to append the agglutination “-ing” to various verbs, we can do:\n\nc(\"attend\", \"visit\", \"support\", \"help\", \"savour\") |&gt;\n  map_chr(function(text) paste0(text, \"ing\"))\n\n[1] \"attending\"  \"visiting\"   \"supporting\" \"helping\"    \"savouring\" \n\n\nInterestingly, we could also try\n\nmap_chr(1:10, function(x) x^2)\n\nWarning: Automatic coercion from double to character was deprecated in purrr 1.0.0.\nℹ Please use an explicit call to `as.character()` within `map_chr()` instead.\n\n\n [1] \"1.000000\"   \"4.000000\"   \"9.000000\"   \"16.000000\"  \"25.000000\" \n [6] \"36.000000\"  \"49.000000\"  \"64.000000\"  \"81.000000\"  \"100.000000\"\n\n\nand see that, although the computations were performed correctly, the output was converted from numbers to character strings encoding those numbers."
  },
  {
    "objectID": "Intro_map.html#making-use-of-map-when-vectorization-fails",
    "href": "Intro_map.html#making-use-of-map-when-vectorization-fails",
    "title": "15  Higher-order functions and mapping",
    "section": "15.4 Making use of map when vectorization fails",
    "text": "15.4 Making use of map when vectorization fails\nOne perhaps obvious criticism of map as we have used it is that its use was not really needed. Early on, we learned that when simple functions are applied to a vector of values, they get applied element-wise. This is called vectorization, and it is a very useful property of R. So (1:10)^2, in our case, achieves the same thing as map_dbl(1:10, function(x) x^2). Similarly, if we simply write cos(1:100), we get the cosine of all integers between 1 and 100 without having to type out map_dbl(1:100, cos). So why bother with map then?\nThe answer is that map can handle cases where vectorization is not available. Most simple functions in R are vectorized, but there are plenty of non-vectorizable operations. To give an example, let us start from a simple dataset: the PlantGrowth table we looked at before, but without the control ctrl group. This leaves just the two treatment groups trt1 and trt2:\n\nplantTrt &lt;- filter(PlantGrowth, group != \"ctrl\")\nprint(plantTrt)\n\n   weight group\n1    4.81  trt1\n2    4.17  trt1\n3    4.41  trt1\n4    3.59  trt1\n5    5.87  trt1\n6    3.83  trt1\n7    6.03  trt1\n8    4.89  trt1\n9    4.32  trt1\n10   4.69  trt1\n11   6.31  trt2\n12   5.12  trt2\n13   5.54  trt2\n14   5.50  trt2\n15   5.37  trt2\n16   5.29  trt2\n17   4.92  trt2\n18   6.15  trt2\n19   5.80  trt2\n20   5.26  trt2\n\n\nWe might want to perform a Wilcoxon rank sum test on these data, but with a number of different confidence levels. A naive approach would be to supply the required confidence levels as a vector:\n\nwilcox.test(weight ~ group, data = plantTrt,\n            conf.int = TRUE, conf.level = c(0.8, 0.9, 0.95, 0.99))\n\nError in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): 'conf.level' must be a single number between 0 and 1\n\n\nThis generates an error: because of the way wilcox.test is internally implemented in R, it does not allow or recognize a vector input in place of conf.level. It must be a single number instead. This, however, can be overcome if we just use map:\n\nc(0.8, 0.9, 0.95, 0.99) |&gt; # The vector of confidence levels\n  map(function(confLevel) wilcox.test(weight ~ group, data = plantTrt,\n                            conf.int = TRUE, conf.level = confLevel))\n\n[[1]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n80 percent confidence interval:\n -1.33 -0.56\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[2]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n90 percent confidence interval:\n -1.43 -0.44\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[3]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -1.50 -0.31\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[4]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n99 percent confidence interval:\n -1.70 -0.03\nsample estimates:\ndifference in location \n                -0.945 \n\n\nNotice that we used map and not map_dbl or map_chr, because wilcox.test returns a complex model object which cannot be coerced into a vector. This is precisely when map, which generates a list whose entries can be arbitrary, is especially useful. (Try the above with map_dbl; it will throw an error.) As a final comment, it would of course have been possible to define a function separately, instead of using the anonymous function above:\n\nwilcoxConf &lt;- function(confLevel) {\n  wilcox.test(weight ~ group, data = plantTrt,\n              conf.int = TRUE, conf.level = confLevel)\n}\n\nc(0.8, 0.9, 0.95, 0.99) |&gt; map(wilcoxConf)\n\n[[1]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n80 percent confidence interval:\n -1.33 -0.56\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[2]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n90 percent confidence interval:\n -1.43 -0.44\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[3]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -1.50 -0.31\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[4]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n99 percent confidence interval:\n -1.70 -0.03\nsample estimates:\ndifference in location \n                -0.945"
  },
  {
    "objectID": "Intro_map.html#exercises",
    "href": "Intro_map.html#exercises",
    "title": "15  Higher-order functions and mapping",
    "section": "15.5 Exercises",
    "text": "15.5 Exercises\n\nStart from the following sequence of values: values &lt;- seq(-5*pi, 5*pi, by = 0.01) (a vector with values going from \\(-5\\pi\\) to \\(5\\pi\\), in steps of 0.01). Now do the following:\n\nDefine a new vector, x, which contains the cosine (cos) of each value in values. Use map_dbl.\nNow define another vector, y, and again using map_dbl, compute the function sin(t) + cos(14 * t / 5) / 5 for every value t in values. You can either define this function separately to use inside map_dbl, or create it anonymously.\nFinally, create a tibble whose two columns are x and y, and plot them against each other using geom_path. See what you get!\n\nHow does the integral of \\(\\cos(x)\\) change if the lower limit of integration is fixed at zero, but the upper limit gradually increases from \\(0\\) to \\(2\\pi\\)? Define a sequence of upper limits upper &lt;- seq(0, 2*pi, by = 0.1). Then, using map_dbl, create a vector integrals whose entries are the integral of \\(\\cos(x)\\) from zero to each upper limit. Finally, plot integrals against upper, using geom_point or geom_line. What is the function you see? (Note: the integrate function returns a complicated list object instead of just a single number. To access just the value of the integral, you can use integrate(...)$value, where ... means all the arguments to the function you are supposed to write when solving the problem.)"
  },
  {
    "objectID": "Intro_map.html#footnotes",
    "href": "Intro_map.html#footnotes",
    "title": "15  Higher-order functions and mapping",
    "section": "",
    "text": "Functions that return a function as their output are also called higher-order functions. For an example which both takes functions as arguments and produces a function as its output, check out the compose function from the purrr package (part of the tidyverse).↩︎\nThe error is included in the output because R’s integration routine is purely numeric, so it algorithmically approximates the integral, and such procedures always have finite precision.↩︎"
  },
  {
    "objectID": "Multiple_analysis.html#motivating-example",
    "href": "Multiple_analysis.html#motivating-example",
    "title": "16  Nested data and multiple analysis",
    "section": "16.1 Motivating example",
    "text": "16.1 Motivating example\nThis chapter is on performing statistical (or other) analyses en masse. To motivate the problem, let us start from a dataset, fruit_fly_wings.csv, that has been adapted from Bolstad et al. (2015):\n\nlibrary(tidyverse)\n\nfly &lt;- read_csv(\"fruit_fly_wings.csv\")\nprint(fly)\n\n# A tibble: 10,327 × 6\n   Species   ID          Date      Sex   WingSize L2Length\n   &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 D_acutila ACU1006.TIF 24_Jul_01 F        0.131    0.497\n 2 D_acutila ACU1009.TIF 24_Jul_01 F        0.136    0.488\n 3 D_acutila ACU1010.TIF 24_Jul_01 F        0.195    0.540\n 4 D_acutila ACU1013.TIF 24_Jul_01 F        0.277    0.646\n 5 D_acutila ACU1018.TIF 24_Jul_01 F        0.152    0.498\n 6 D_acutila ACU1021.TIF 24_Jul_01 F        0.175    0.492\n 7 D_acutila ACU1048.TIF 24_Jul_01 F        0.230    0.600\n 8 D_acutila ACU1049.TIF 24_Jul_01 F        0.174    0.546\n 9 D_acutila ACU1054.TIF 05_Sep_01 F        0.108    0.429\n10 D_acutila ACU1059.TIF 05_Sep_01 F        0.205    0.580\n# ℹ 10,317 more rows\n\n\nThe data contain measurements on individual fruit flies, belonging to various species and either sex as indicated by the Species and Sex columns. Each individual is uniquely identified (ID), and the date of the measurement has also been recorded (Date). Most importantly, the length of the wing (WingSize) and the length of the L2 vein that runs across the wing (L2Length) have been recorded.\nWhat is the distribution of wing sizes across species and sexes? To begin answering this question, we can start with a plot:\n\nggplot(fly) +\n  aes(x = WingSize, y = Species, colour = Sex, fill = Sex) +\n  geom_boxplot(alpha = 0.2) +\n  scale_colour_manual(values = c(\"steelblue\", \"goldenrod\")) +\n  scale_fill_manual(values = c(\"steelblue\", \"goldenrod\")) +\n  xlab(\"Wing size\") +\n  theme_bw()\n\n\n\n\nLooking at this figure, it does appear as if females often had larger wings than males within the same species. The main question in this chapter is how we can test this—for instance, how would it be possible to perform a Wilcoxon rank sum test for all 55 species in the data?"
  },
  {
    "objectID": "Multiple_analysis.html#nested-data-frames",
    "href": "Multiple_analysis.html#nested-data-frames",
    "title": "16  Nested data and multiple analysis",
    "section": "16.2 Nested data frames",
    "text": "16.2 Nested data frames\nWe are by now used to the idea that the columns of tibbles can hold numbers, character strings, logical values, or even factors. But here is an interesting question: can a column of a tibble hold other tibbles?\nThe short answer is yes. The somewhat longer answer is that this is possible, but not directly so. Instead, one has to make the column into a list (Section 14.2.1).1 While this could be done by hand using the list function, there are other options in the tidyverse which facilitate creating tibbles which have other tibbles in their columns. One of these is called nest. This function receives a name first, which will become the name of the newly-created column holding the sub-tibbles. Then, after an equality sign, one lists the columns, in a vector, which one would like to package into those sub-tibbles. For example, to keep Species as a separate column and wrap everything else into sub-tibbles, we can do:\n\nfly |&gt; nest(data = c(ID, Date, Sex, WingSize, L2Length))\n\n# A tibble: 55 × 2\n   Species      data              \n   &lt;chr&gt;        &lt;list&gt;            \n 1 D_acutila    &lt;tibble [205 × 5]&gt;\n 2 D_algonqu    &lt;tibble [237 × 5]&gt;\n 3 D_texana     &lt;tibble [215 × 5]&gt;\n 4 Z_Sg.Anaprio &lt;tibble [200 × 5]&gt;\n 5 D_athabasca  &lt;tibble [79 × 5]&gt; \n 6 D_bifasci    &lt;tibble [217 × 5]&gt;\n 7 D_busckii    &lt;tibble [105 × 5]&gt;\n 8 I_crucige    &lt;tibble [211 × 5]&gt;\n 9 Det_nigro    &lt;tibble [15 × 5]&gt; \n10 H_duncani    &lt;tibble [219 × 5]&gt;\n# ℹ 45 more rows\n\n\nWhat do we see? We ended up with a tibble that has two columns. One is Species and has type character string. The other is data and has the type of list, as indicated by the &lt;list&gt; tag. The entries in this column are tibbles, with varying numbers of rows (as many as the number of individuals for the given species), and five columns; namely, those that we specified we wanted to nest. We can check and see what is inside these tibbles. For example, the contents of the first row (species: D. acutila) are:\n\nfly |&gt;\n  nest(data = c(ID, Date, Sex, WingSize, L2Length)) |&gt;\n  pull(data) |&gt; # Get contents of just the \"data\" column\n  pluck(1) # Take the first of all those tibbles\n\n# A tibble: 205 × 5\n   ID          Date      Sex   WingSize L2Length\n   &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 ACU1006.TIF 24_Jul_01 F        0.131    0.497\n 2 ACU1009.TIF 24_Jul_01 F        0.136    0.488\n 3 ACU1010.TIF 24_Jul_01 F        0.195    0.540\n 4 ACU1013.TIF 24_Jul_01 F        0.277    0.646\n 5 ACU1018.TIF 24_Jul_01 F        0.152    0.498\n 6 ACU1021.TIF 24_Jul_01 F        0.175    0.492\n 7 ACU1048.TIF 24_Jul_01 F        0.230    0.600\n 8 ACU1049.TIF 24_Jul_01 F        0.174    0.546\n 9 ACU1054.TIF 05_Sep_01 F        0.108    0.429\n10 ACU1059.TIF 05_Sep_01 F        0.205    0.580\n# ℹ 195 more rows\n\n\nSo this sub-table contains the information that pertains to just D. acutila individuals.\nWhen choosing which columns to wrap into sub-tibbles with nest, all the tidy selection conventions and functionalities apply that one can use with the select function (Section 5.1.1). So the above nesting could be equivalently and more simply be performed with:\n\nfly |&gt; nest(data = !Species)\n\n# A tibble: 55 × 2\n   Species      data              \n   &lt;chr&gt;        &lt;list&gt;            \n 1 D_acutila    &lt;tibble [205 × 5]&gt;\n 2 D_algonqu    &lt;tibble [237 × 5]&gt;\n 3 D_texana     &lt;tibble [215 × 5]&gt;\n 4 Z_Sg.Anaprio &lt;tibble [200 × 5]&gt;\n 5 D_athabasca  &lt;tibble [79 × 5]&gt; \n 6 D_bifasci    &lt;tibble [217 × 5]&gt;\n 7 D_busckii    &lt;tibble [105 × 5]&gt;\n 8 I_crucige    &lt;tibble [211 × 5]&gt;\n 9 Det_nigro    &lt;tibble [15 × 5]&gt; \n10 H_duncani    &lt;tibble [219 × 5]&gt;\n# ℹ 45 more rows\n\n\nThat is: apply the nesting to all columns that are not called Species. This can be used in more complicated cases as well. For example, if we wish to nest data pertaining to particular species-sex combinations, we can do the following:\n\nfly |&gt; nest(data = !Species & !Sex)\n\n# A tibble: 110 × 3\n   Species      Sex   data              \n   &lt;chr&gt;        &lt;chr&gt; &lt;list&gt;            \n 1 D_acutila    F     &lt;tibble [104 × 4]&gt;\n 2 D_acutila    M     &lt;tibble [101 × 4]&gt;\n 3 D_algonqu    F     &lt;tibble [144 × 4]&gt;\n 4 D_algonqu    M     &lt;tibble [93 × 4]&gt; \n 5 D_texana     F     &lt;tibble [108 × 4]&gt;\n 6 D_texana     M     &lt;tibble [107 × 4]&gt;\n 7 Z_Sg.Anaprio F     &lt;tibble [95 × 4]&gt; \n 8 Z_Sg.Anaprio M     &lt;tibble [105 × 4]&gt;\n 9 D_athabasca  F     &lt;tibble [20 × 4]&gt; \n10 D_athabasca  M     &lt;tibble [59 × 4]&gt; \n# ℹ 100 more rows\n\n\nwhere nest(data = !Species & !Sex) (nest all columns that are not Species and not Sex) is equivalent to the longer nest(data = c(ID, Date, WingSize, L2Length)).\nFinally, columns holding nested data can be unnested, meaning that their contents are expanded back into the original data frame. The function to do this with is called unnest:\n\nfly |&gt; nest(data = !Species & !Sex) |&gt; unnest(data)\n\n# A tibble: 10,327 × 6\n   Species   Sex   ID          Date      WingSize L2Length\n   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 D_acutila F     ACU1006.TIF 24_Jul_01    0.131    0.497\n 2 D_acutila F     ACU1009.TIF 24_Jul_01    0.136    0.488\n 3 D_acutila F     ACU1010.TIF 24_Jul_01    0.195    0.540\n 4 D_acutila F     ACU1013.TIF 24_Jul_01    0.277    0.646\n 5 D_acutila F     ACU1018.TIF 24_Jul_01    0.152    0.498\n 6 D_acutila F     ACU1021.TIF 24_Jul_01    0.175    0.492\n 7 D_acutila F     ACU1048.TIF 24_Jul_01    0.230    0.600\n 8 D_acutila F     ACU1049.TIF 24_Jul_01    0.174    0.546\n 9 D_acutila F     ACU1054.TIF 05_Sep_01    0.108    0.429\n10 D_acutila F     ACU1059.TIF 05_Sep_01    0.205    0.580\n# ℹ 10,317 more rows"
  },
  {
    "objectID": "Multiple_analysis.html#performing-statistical-tests-using-nested-data",
    "href": "Multiple_analysis.html#performing-statistical-tests-using-nested-data",
    "title": "16  Nested data and multiple analysis",
    "section": "16.3 Performing statistical tests using nested data",
    "text": "16.3 Performing statistical tests using nested data\nHow can we test for all 55 fly species in this dataset whether there is a significant difference between average male and female wing lengths? The answer is to first nest the data using fly |&gt; nest(data = !Species), so that we end up with a table which has one row per each species. We then need to run a Wilcoxon rank sum test on each of them. But this we know how to do from Chapter 15: we can rely on the map function.\n\nfly |&gt;\n  nest(data = !Species) |&gt;\n  mutate(test = map(data, function(x) wilcox.test(L2Length ~ Sex, data = x)))\n\n# A tibble: 55 × 3\n   Species      data               test   \n   &lt;chr&gt;        &lt;list&gt;             &lt;list&gt; \n 1 D_acutila    &lt;tibble [205 × 5]&gt; &lt;htest&gt;\n 2 D_algonqu    &lt;tibble [237 × 5]&gt; &lt;htest&gt;\n 3 D_texana     &lt;tibble [215 × 5]&gt; &lt;htest&gt;\n 4 Z_Sg.Anaprio &lt;tibble [200 × 5]&gt; &lt;htest&gt;\n 5 D_athabasca  &lt;tibble [79 × 5]&gt;  &lt;htest&gt;\n 6 D_bifasci    &lt;tibble [217 × 5]&gt; &lt;htest&gt;\n 7 D_busckii    &lt;tibble [105 × 5]&gt; &lt;htest&gt;\n 8 I_crucige    &lt;tibble [211 × 5]&gt; &lt;htest&gt;\n 9 Det_nigro    &lt;tibble [15 × 5]&gt;  &lt;htest&gt;\n10 H_duncani    &lt;tibble [219 × 5]&gt; &lt;htest&gt;\n# ℹ 45 more rows\n\n\nAnd voilà: we now have the Wilcoxon rank sum test results in the column test, for each species! All we need to do is retrieve this information.\nDoing so is not completely straightforward, because wilcox.test does not return a data frame or tibble. Instead, it returns a complicated model fit object which cannot be unnested into the outer table. If we try, we get an error:\n\nfly |&gt;\n  nest(data = !Species) |&gt;\n  mutate(test = map(data, function(x) wilcox.test(L2Length~Sex, data=x))) |&gt;\n  unnest(test)\n\nError in `list_sizes()`:\n! `x[[1]]` must be a vector, not a &lt;htest&gt; object.\n\n\nFortunately, there is an easy way to convert the output of the Wilcoxon rank sum test into a tibble. The broom package is designed to do exactly this. It is part of the tidyverse, though it does not get automatically loaded with it. We load this package first:\n\nlibrary(broom)\n\nThe function in this package that creates a tibble out of the results of statistical models (almost any model in fact, not just the Wilcoxon rank sum test) is called tidy. Let us see how it works. If we do a Wilcoxon rank sum test between females and males for the whole data (without distinguishing between species), we get:\n\nwilcox.test(WingSize ~ Sex, data = fly, conf.int = TRUE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  WingSize by Sex\nW = 16695507, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n 0.07910817 0.09418589\nsample estimates:\ndifference in location \n            0.08664608 \n\n\nBy applying the tidy function to this result, it gets converted into a tibble:\n\nwilcox.test(WingSize ~ Sex, data = fly, conf.int = TRUE) |&gt; tidy()\n\n# A tibble: 1 × 7\n  estimate statistic   p.value conf.low conf.high method             alternative\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;      \n1   0.0866  16695507 2.54e-109   0.0791    0.0942 Wilcoxon rank sum… two.sided  \n\n\nSo we can insert a step into our analysis pipeline which converts the test column into a list of data frames:\n\nfly |&gt;\n  nest(data = !Species) |&gt;\n  mutate(test = map(data, function(x) wilcox.test(L2Length~Sex, data=x))) |&gt;\n  mutate(test = map(test, tidy)) |&gt;\n  unnest(test)\n\n# A tibble: 55 × 6\n   Species      data               statistic  p.value method         alternative\n   &lt;chr&gt;        &lt;list&gt;                 &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;      \n 1 D_acutila    &lt;tibble [205 × 5]&gt;      9640 5.03e-25 Wilcoxon rank… two.sided  \n 2 D_algonqu    &lt;tibble [237 × 5]&gt;     12494 2.34e-29 Wilcoxon rank… two.sided  \n 3 D_texana     &lt;tibble [215 × 5]&gt;      9189 7.55e-14 Wilcoxon rank… two.sided  \n 4 Z_Sg.Anaprio &lt;tibble [200 × 5]&gt;      6330 1.03e- 3 Wilcoxon rank… two.sided  \n 5 D_athabasca  &lt;tibble [79 × 5]&gt;       1116 3.13e- 9 Wilcoxon rank… two.sided  \n 6 D_bifasci    &lt;tibble [217 × 5]&gt;     11447 2.56e-33 Wilcoxon rank… two.sided  \n 7 D_busckii    &lt;tibble [105 × 5]&gt;      1654 7.63e- 2 Wilcoxon rank… two.sided  \n 8 I_crucige    &lt;tibble [211 × 5]&gt;      5238 4.63e- 1 Wilcoxon rank… two.sided  \n 9 Det_nigro    &lt;tibble [15 × 5]&gt;         20 8.51e- 1 Wilcoxon rank… two.sided  \n10 H_duncani    &lt;tibble [219 × 5]&gt;     10195 2.25e-19 Wilcoxon rank… two.sided  \n# ℹ 45 more rows\n\n\nAnd now we have the results. As an example, we can check the distribution of p-values: how often is there a statistically significant sex difference? Let us visualize this, by ordering the species based on p-values:\n\nfly |&gt;\n  nest(data = !Species) |&gt;\n  mutate(test = map(data, function(x) wilcox.test(L2Length~Sex, data=x))) |&gt;\n  mutate(test = map(test, tidy)) |&gt;\n  unnest(test) |&gt;\n  arrange(p.value) |&gt;\n  mutate(Species = as_factor(Species)) |&gt;\n  ggplot(aes(x = p.value, y = Species)) +\n  geom_col(colour = \"steelblue\", fill = \"steelblue\", alpha = 0.3) +\n  scale_x_continuous(name = \"p-value\", limits = c(0, 1)) +\n  theme_bw()\n\n\n\n\nThis graph shows that most species have very small p-values, but that there are four clear outliers. We can extract the names of these outlier species:\n\noutlierSpecies &lt;- fly |&gt;\n  nest(data = !Species) |&gt;\n  mutate(test = map(data, function(x) wilcox.test(L2Length~Sex, data=x))) |&gt;\n  mutate(test = map(test, tidy)) |&gt;\n  unnest(test) |&gt;\n  arrange(desc(p.value)) |&gt;\n  slice(1:4) |&gt; # Choose first 4 rows from the sorted table\n  pull(Species) # Get the 4 species names\n\nprint(outlierSpecies)\n\n[1] \"Det_nigro\" \"N_sordida\" \"I_crucige\" \"D_busckii\"\n\n\nAnd then we can plot the wing length data for just these ones:\n\nfly |&gt;\n  filter(Species %in% outlierSpecies) |&gt;\n  ggplot() +\n  aes(x = Sex, y = WingSize) +\n  geom_boxplot(colour = \"steelblue\", fill = \"steelblue\",\n               alpha = 0.2, outlier.shape = NA) +\n  geom_jitter(colour = \"steelblue\", alpha = 0.4) +\n  facet_grid(. ~ Species) +\n  theme_bw()\n\n\n\n\nFor D_busckii, I_crucige, and N_sordida, it is difficult not to conclude that the low p-values indicate the lack of a meaningful sex difference in wing length. For Det_nigro on the other hand, there are very few sampled individuals. Therefore one would most likely need more data to say.\nIn summary, a very powerful way of analyzing data is to perform many analyses at once. To do so, one first has to nest the data. Then the analysis can be performed for every row with the help of the map function. Finally, one unnests the data and interprets the results. Often, an overview of the data will lead to insights that would have been difficult to gain otherwise. In our case, we saw that all but a handful of species exhibit a significant sex difference in wing length. For the few outliers, we saw that one of them lacks sufficient data, and therefore conclusions about this species should be postponed until more data are acquired."
  },
  {
    "objectID": "Multiple_analysis.html#exercises",
    "href": "Multiple_analysis.html#exercises",
    "title": "16  Nested data and multiple analysis",
    "section": "16.4 Exercises",
    "text": "16.4 Exercises\n\nOur analysis on the sex differences in the wing length of fruit flies (fruit_fly_wings.csv) revealed whether there was a significant difference within each species. However, it did not say anything about which sex tends to have a longer wing. Perform this analysis here.\n\nCreate a graph with the difference between mean female and mean male wing lengths along the x-axis, and the species along the y-axis. You can represent the difference of means in each species by a point (geom_point).\nHow many species are there where females have longer wings on average? How many where males do?\nWhich species shows the largest degree of sexual dimorphism?\nList the species where males have, on average, longer wings than females.\n\nThe goal of the original study by Bolstad et al. (2015) was to see the allometric relationship between wing length and the length of the L2 vein that runs across the wing, in different species of fruit flies.\n\nObtain the slope from a linear regression between wing size and L2 vein length (which has been logged in the data) for each species and sex.\nCreate a histogram of the regression slopes. What is the (approximate) distribution of the slopes?\nWhat is the mean and the standard deviation of the distribution of slopes? What is their range? Are the slopes all positive, all negative, or vary between the two? What does this tell you about the relationship between wing size and L2 vein length in general?\nPlot your results, with the regression slopes along the x-axis and the species-sex combination along the y-axis, sorted in the order of the regression slopes. Which species-sex combination has the largest slope? Which one has the smallest?\n\nThe gapminder package contains information on the population size, average life expectancy, and per capita GDP for 142 countries, from 1952 to 2007 (in steps of 5 years). Download and install this package via install.packages(\"gapminder\"), then load it with library(gapminder). If you now type gapminder in the console, you should see a table with six columns. Here we will be focusing on the columns country, continent, year, and pop (the population size of the country in the given year). Now do the following exercises.\n\nLet us see if and when population growth has been exponential in these countries. If the growth of the population size is exponential, then the growth of its logarithm is linear. Therefore, as a first step, take the logarithms of all population sizes in the pop column.\nNest the data by country and continent, and obtain a linear fit of log population size against year for each.\nExtract from this, not the slope or p-value, but a different measure of the model’s quality: the proportion of variance explained (R2). Hint: you can do this with the glance function, which is part of the broom package. It works much in the same way as tidy, but extracts information about model quality instead of model parameters. The R2 value is contained in the column called r.squared.\nMake a plot with R2 along the x-axis and country along the y-axis, showing the R2 values by points. Colour the points based on the continent of the country.\nMake the same plot but first reorder the countries by r.squared.\nWhich handful of countries stand out as having a particularly poor fit with the linear model? Make a plot of just the seven countries with the lowest R2 values. Let year be along the x-axis, the log population size along the y-axis, and the different countries be in separate facets. Bonus exercise: alongside these population curves, display also the predictions from the linear regressions.\nWhich are the countries with the worst model fit? Do they tend to come from a particular continent or region? Given your knowledge of recent history, can you speculate on what the reasons could be for their deviations from exponential growth?\n\nIn this exercise, we explore the goodness-of-fit of linear models between sepal and petal lengths in the iris dataset.\n\nFirst, visualize the data. Create a plot of the iris dataset, using points whose x-coordinate is sepal length and y-coordinate is petal length. Let them be colored by species. Finally, show linear regressions on the points belonging to each species, using geom_smooth.\nObtain the slope of the fit for each species, and the associated p-value. Do this by first nesting the data by species, then fitting a linear model to each of them (with map), and extracting slopes and p-values by applying the tidy function in the broom package. Finally, unnest the data. What are the slopes? And are they significantly different from zero?\n\n\n\n\n\n\nBolstad, Geir H., Jason A. Cassara, Eladio Márquez, Thomas F. Hansen, Kim van der Linde, David Houle, and Christophe Pélabon. 2015. “Complex constraints on allometry revealed by artificial selection on the wing of Drosophila melanogaster.” Proceedings of the National Academy of Sciences 112 (43): 13284–89. https://doi.org/10.1073/pnas.1505357112."
  },
  {
    "objectID": "Multiple_analysis.html#footnotes",
    "href": "Multiple_analysis.html#footnotes",
    "title": "16  Nested data and multiple analysis",
    "section": "",
    "text": "The reason is that columns of tibbles must always hold elementary pieces of data. The way lists work is that they do not actually hold the information corresponding to their entries. Instead, they only contain references, or pointers, to where the information can be found in memory. Since lists only store these pointers (which can be represented by simple numbers) instead of the tibbles or other data structures inside them, they can be used without problems within tibbles.↩︎"
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "17  References",
    "section": "",
    "text": "Anscombe, Francis J. 1973. “Graphs in\nStatistical Analysis.” American Statistician 27\n(1): 17–21. https://doi.org/10.1080/00031305.1973.10478966.\n\n\nBarabás, György, Christine Parent, Andrew Kraemer, Frederik Van de\nPerre, and Frederik De Laender. 2022. “The\nevolution of trait variance creates a tension between species diversity\nand functional diversity.” Nature Communications\n13 (2521): 1–10. https://doi.org/10.1038/s41467-022-30090-4.\n\n\nBolstad, Geir H., Jason A. Cassara, Eladio Márquez, Thomas F. Hansen,\nKim van der Linde, David Houle, and Christophe Pélabon. 2015.\n“Complex constraints on allometry revealed by\nartificial selection on the wing of Drosophila\nmelanogaster.” Proceedings of the National Academy of\nSciences 112 (43): 13284–89. https://doi.org/10.1073/pnas.1505357112.\n\n\nCedergreen, Nina, and Tom Vindbæk Madsen. 2002. “Nitrogen uptake by the floating macrophyte Lemna\nminor.” New Phytologist 155 (2): 285–92. https://doi.org/10.1046/j.1469-8137.2002.00463.x.\n\n\nColquhoun, David. 2014. “An investigation of\nthe false discovery rate and the misinterpretation of\np-values.” Royal Society Open Science 1 (3):\n140216. https://doi.org/10.1098/rsos.140216.\n\n\nFauchald, Per, Taejin Park, Hans Tømmervik, Ranga Myneni, and Vera\nHelene Hausner. 2017. “Arctic greening from\nwarming promotes declines in caribou populations.”\nScience Advances 3 (4): e1601365. https://doi.org/10.1126/sciadv.1601365.\n\n\nGalton, Francis. 1886. “Regression Towards\nMediocrity in Hereditary Stature.” Journal of the\nAnthropological Institute of Great Britain and Ireland 15: 246–63.\n\n\nGillespie, John H. 2004. Population Genetics: A Concise Guide.\nBaltimore, MD, USA: Johns Hopkins University Press.\n\n\nGoldberg, Emma E., Joshua R. Kohn, Russell Lande, Kelly A. Robertson,\nStephen A. Smith, and Boris Igić. 2010. “Species Selection\nMaintains Self-Incompatibility.” Science 330\n(6003): 493–95. https://doi.org/10.1126/science.1194513.\n\n\nKraemer, Andrew C., C. W. Philip, A. M. Rankin, and C. E. Parent. 2018.\n“Trade-Offs Direct the Evolution of Coloration in\nGalápagos Land Snails.” Proceedings\nof the Royal Society B 286: 20182278.\n\n\nKraemer, Andrew C., Yannik E. Roell, Nate F. Shoobs, and Christine E.\nParent. 2022. “Does island ontogeny dictate\nthe accumulation of both species richness and functional\ndiversity?” Global Ecology and Biogeography 31\n(1): 123–37. https://doi.org/10.1111/geb.13420.\n\n\nParent, C. E., and B. J. Crespi. 2009. “Ecological Opportunity in\nAdaptive Radiation of Galápagos Endemic Land\nSnails.” American Naturalist 174: 898–905.\n\n\nSmith, Felisa A., S. Kathleen Lyons, S. K. Morgan Ernest, Kate E. Jones,\nDawn M. Kaufman, Tamar Dayan, Pablo A. Marquet, James H. Brown, and John\nP. Haskell. 2003. “Body Mass of Late\nQuaternary Mammals.” Ecology 84 (12): 3403. https://doi.org/10.1890/02-9003.\n\n\nWilkinson, Leland. 2006. The Grammar of\nGraphics. Secaucus, NJ, USA: Springer Science & Business\nMedia."
  }
]