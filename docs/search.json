[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to data analysis and visualization with R",
    "section": "",
    "text": "Welcome\nDo you have any comments, suggestions for improvement, or errata? Feel free to send them to me at dysordys@protonmail.com.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "01_Intro_R_RStudio.html",
    "href": "01_Intro_R_RStudio.html",
    "title": "1  Introduction to R and RStudio",
    "section": "",
    "text": "1.1 Overview\nThis chapter introduces R and RStudio. R is a free and open-source programming language for statistics, graphing, and modeling, originally developed by statisticians. In recent years, R has become extremely popular among biologists, and you will almost certainly encounter it as part of real-world research projects. In this book we will be learning some of the ways in which R can be used for efficient data analysis and visualization.\nRStudio is an “integrated development environment” (IDE) for R, which means it is a software application that lets you write, run, and interact graphically with programs. RStudio integrates a text editor, the R console (where you run R commands), package management, plotting, help, and more.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "01_Intro_R_RStudio.html#installing-r-and-rstudio",
    "href": "01_Intro_R_RStudio.html#installing-r-and-rstudio",
    "title": "1  Introduction to R and RStudio",
    "section": "1.2 Installing R and RStudio",
    "text": "1.2 Installing R and RStudio\nYou can download the most up-to-date R distribution for free here:\nhttp://www.r-project.org\nRun the installer as directed and you should be set to go. We will not interact with the installed R application directly, but the R software components you install will be used by RStudio under the hood.\nTo install RStudio on your computer, download it from here:\nhttps://www.rstudio.com/products/rstudio/download/\nOn a Mac, open the downloaded disk image and drag the RStudio application into your Applications folder. On Windows, run the installer you downloaded.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "01_Intro_R_RStudio.html#getting-around-rstudio",
    "href": "01_Intro_R_RStudio.html#getting-around-rstudio",
    "title": "1  Introduction to R and RStudio",
    "section": "1.3 Getting Around RStudio",
    "text": "1.3 Getting Around RStudio\nRStudio should be available in the usual places: the Applications folder (on a Mac) or the Start menu (on Windows). When you start it up, you will see four sections of the screen. The most important items found in each are:\n\nUpper-left: an area for viewing and editing text files\nLower-left: Console, where you send commands to R\nUpper-right: Environment, for loading, saving, and examining data\nLower-right:\n\nFiles: a list of files in the “working directory” (more on this later)\nPlots: where the plots (graphs) you draw show up\nPackages: an area for managing installed R packages\nHelp: access to all the official documentation for R\n\n\n\n\n\nRStudio starting screen\n\n\n\n1.3.1 A Simple Calculation\nEven if you don’t know R, you can start by typing some simple calculations into the console. The &gt; symbol indicates that R is waiting for you to type something. Click on the console, type 2 + 2, and hit Enter (or Return on a Mac). You should see that R produces the right answer:\n\n&gt; 2 + 2\n[1] 4\n&gt;\n\nNow, press the Up arrow on the keyboard. You will notice that the 2 + 2 you typed before shows up. You can use the Up and Down arrows to go back and forth through past commands you have typed, which can save a lot of repetitive typing when you are trying things out. You can change the text in these historical commands: change the 2 to a 3 and press Enter (Return, on a Mac) again:\n\n&gt; 2 + 3\n[1] 5\n&gt;\n\n(The [1] at the beginning of the result just means that the following number is at position 1 of a vector. In this case, the vector only has one element, but when R needs to print out a long vector, it splits it into multiple lines tells you at the beginning of each line what position you are at.)\nIn case you press Enter/Return before completing a calculation, the prompt symbol &gt; changes to a + (which looks like an addition symbol but does not actually represent addition or any other operation). This lets you know that R is awaiting the completion of an expression instead of fresh input. For example, if we multiply 2 with 3 but press Enter after the multiplication symbol, we see this:\n\n&gt; 2 *\n+\n\nYou now have two options. First, you can press the Escape key to cancel the current calculation altogether. In that case, the prompt will return to being &gt;, and the fact that you wanted to multiply 2 with something is forgotten. Otherwise, you can complete the formula and simply enter 3, which will then be interpreted as the continuation of 2 *:\n\n&gt; 2 *\n+ 3\n[1] 6\n&gt;\n\nThe result, 6, is now displayed in the console, and the new prompt &gt; beneath means that the console is ready for a fresh input.\nBefore going deeper into R programming, we need to discuss a few things to enable you to get around R and RStudio more easily.\n\n\n1.3.2 Writing R scripts\nThe upper left part of RStudio is a simple text editor where you can write R code. But, instead of having to enter it one line at a time as we did in the console above, you can string long sequences of R instructions together that build on one another. You can save such a text file (Ctrl-S on Windows; Cmd-S on a Mac), giving it an appropriate name. It is then known as an R script, a text file containing R code that can be run from within R.\nAs an example, enter the following code. Do not worry about how or why it works just yet. It is a simple simulation and visualization of regulated (logistic) population growth:\n\ntime &lt;- 1:15\ngrowthFun &lt;- function(x, y, lambda = 1.8) lambda * x * (1 - x)\npop &lt;- Reduce(growthFun, rep(0.01, times = length(time)), accumulate = TRUE)\nplot(time, pop, xlab = \"time\", ylab = \"population density\", type = \"b\")\n\nAfter having typed this, highlight all lines. You can do this either with a mouse, or by pressing Ctrl-A on Windows / Cmd-A on a Mac, or by using the arrow keys while holding the Shift key down. Then, to send these instructions to R for processing, press Ctrl-Enter (Cmd-Return on a Mac). If all went well, the lower right screen section should have jumped to the Plots panel, showing the following graph:\n\n\n\n\n\n\n\n\n\n\n\n1.3.3 Setting the Working Directory\nWhen you ask R to run a program or load data from a file, it needs to know where to find the file. Unless you specify the complete path to the file on your machine, it assumes that file names are relative to what is called the “working directory”.\nThe first thing you should do when starting a project is to create a directory (folder) on your computer to store all the files related to the project, and then tell R to set the working directory to that location.\nThe R function setwd(\"/path/to/directory\") is used to set the working directory, where you substitute in the actual path and directory name in place of path/to/directory. In turn, getwd() tells you what the current working directory is. The path can be found using File Explorer on a Windows PC, but you will need to change backslashes to forward slashes (\\ to /). On a Mac, you can find the path by selecting a folder and choosing File &gt; Get Info (the path is under “Where:”).\nThere is also a convenient graphical way to set the working directory in RStudio. In the Files panel, you can navigate around the computer’s file space. You can do this either in the panel itself, or using the ellipsis (…) to bring up the system-standard file browser. Looking around there does not immediately set the working directory, but you can set it by clicking the “Session” menu point at the top menu bar, hovering over “Set Working Directory” with the mouse, and choosing “To Files Pane Location” from the list of sub-options when they appear.\n\n\n\n\n\n\nWarning\n\n\n\nIt is worth repeating: finding the appropriate directory in the Files panel is not enough. It will not set the working directory automatically. You need to actually click the “Session” menu and then click on “To Files Pane Location” under “Set Working Directory”.\n\n\nYou may have also noticed that you don’t actually need to type getwd(): the RStudio Console panel shows the current working directory below the word “Console”.\n\n\n1.3.4 Packages\nOne of the primary reasons ecologists use R is the availability of hundreds of free, user-contributed pieces of software, called packages. Packages are generally created by people who wanted to solve a particular problem for their own research and then realized that other people might find their code useful. Take a moment to browse the packages available on the main R site:\nhttp://cran.r-project.org/web/packages/\nTo install a package, you take its name, put it in quotes, and put it in between the parentheses of install.packages(). For example, to install the package tidyverse (which we will be relying on later), you type:\n\ninstall.packages(\"tidyverse\")\n\nand press Enter (Return, on a Mac). Note that some packages can take quite a while to install. If you are installing tidyverse for instance, it could take anywhere between five minutes to an hour (!) depending on your computer setup. This is normal, and the good news is that you only need to do this once on a computer. Once the package is installed, it will stick around.\n\n\n\n\n\n\nNote\n\n\n\nIt is possible for the installation of packages to fail. The most common reason is that the package relies on some external software (e.g., curl, ffmpeg, a C++ compiler, etc.) which is not installed on your computer. In such cases, make sure the required software is installed first, then try installing the package again.\n\n\nTo actually use a previously installed package in an R session, you need to load it from your disk directly into the computer’s memory. That can be done like this:\n\nlibrary(tidyverse)\n\nNote the lack of quotation marks when loading a package.\nRStudio also makes package management a bit easier. In the Packages panel (top line of lower right portion of the screen) you can see a list of all installed packages. You can also load and unload packages simply by checking a checkbox, and you can install new packages using a graphical interface (although you will still need to know the name of the package you want to install).",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "01_Intro_R_RStudio.html#additional-reading",
    "href": "01_Intro_R_RStudio.html#additional-reading",
    "title": "1  Introduction to R and RStudio",
    "section": "1.4 Additional Reading",
    "text": "1.4 Additional Reading\nR:\n\nR website\nCRAN package index\n\nRStudio:\n\nRStudio documentation",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "01_Intro_R_RStudio.html#sec-intro-exercises",
    "href": "01_Intro_R_RStudio.html#sec-intro-exercises",
    "title": "1  Introduction to R and RStudio",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\n\nCreate a folder named data-with-R on your computer and set it as the working directory in R. Make certain that the working directory has indeed been set properly.\nUse the RStudio file browser to set the working directory somewhere else on your hard drive, and then set it back to the data-with-R folder you created earlier. Make sure it is being set properly at each step.\nInstall an R package called vegan, using install.packages as discussed in Section 1.3.4. (The vegan package contains various utilities for community ecology.)\nLoad the vegan package invoking library(vegan). Afterwards, try unloading and then loading the vegan package again, using the Packages panel in RStudio this time.\nCreate a simple text file (you can do this via File \\(\\blacktriangleright\\) New File \\(\\blacktriangleright\\) Text File from the main menu bar at the top) and put the following secret message in it:\nA cnydm, z fqnrr, zmc z rbnqd\nPktr sgqdd shldr sgd rptzqd qnns ne entq\n  Dhuhcdc ax rdudm\n  Pktr ehud shldr dkdudm\nIr mhmd rptzqdc zmc mns z ahs lnqd.\nNow save this file as message.txt in the data-with-R folder you created earlier.\nCreate a new R script file (File \\(\\blacktriangleright\\) New File \\(\\blacktriangleright\\) R Script) and enter the following:\n\nreadLines(\"message.txt\") |&gt;\n  chartr(paste(letters, collapse = \"\"),\n         paste(c(letters[-1], \"a\"), collapse = \"\"),\n         x = _) |&gt;\n  writeLines()\n\nSave the file as read-message.R in the same folder (data-with-R). Run it and see what happens. If all goes well, it should decipher the message from the previous exercise and print it on your screen. (Hint: if you run into trouble, you might need to set the working directory appropriately.)",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "02_R_programming_basics.html",
    "href": "02_R_programming_basics.html",
    "title": "2  R programming basics",
    "section": "",
    "text": "2.1 Using R as a calculator\nAs we have seen in Section 1.3.1, R can be used as a glorified pocket calculator. Elementary operations work as expected: + and - are symbols for addition and subtraction, while * and / are multiplication and division. Thus, we can enter things such as 3 * 4 - 6 / 2 + 1 n the console, and press Enter (Return, on a Mac) to get the result:\n3 * 4 - 6 / 2 + 1\n\n[1] 10\nOne even has exponentiation, denoted by the symbol ^. To raise 2 to the 5th power, we enter\n2^5\n\n[1] 32\nFurthermore, one is not restricted to integers. It is possible to calculate with fractional numbers:\n1.62 * 34.56\n\n[1] 55.9872\nR also has many basic mathematical functions built into it. For example, sqrt is the square root function; cos is the cosine function, log is the (natural) logarithm, exp is the exponential function, and so on. The following tables summarize the symbols for various arithmetic operations and basic mathematical functions built into R:\nExpressions built from these basic blocks can be freely combined. Try to calculate \\(3^{\\log(4)} - \\sin(\\text{e}^2)\\) for instance. To do so, we simply type the following and press Enter to get the result:\n3^log(4) - sin(exp(2))\n\n[1] 3.692108\nNow obtain \\(\\text{e}^{1.3} (4 - \\sin(\\pi / 3))\\). Notice the parentheses enclosing \\(4 - \\sin(\\pi /3)\\). This means, as usual, that this expression is evaluated first, before any of the other computations. It can be implemented in R the same way, by using parentheses:\nexp(1.3) * (4 - sin(3.14159 / 3))\n\n[1] 11.49948\nNote also that you do need to indicate the symbol for multiplication between closing and opening parentheses: omitting this results in an error. Try it: entering exp(1.3)(4 - sin(3.14159/3)) instead of exp(1.3)*(4 - sin(3.14159/3)) throws an error message. Also, be mindful that exp(1.3)*(4 - sin(3.14159/3)) is not the same as exp(1.3)*4 - sin(3.14159/3). This is because multiplication takes precedence over addition and subtraction, meaning that multiplications and divisions are performed first, and additions/subtractions get executed only afterwards—unless, of course, we override this behaviour with parentheses. In general, whenever you are uncertain about the order of execution of operations, it can be useful to explicitly use parentheses, even if it turns out they aren’t really necessary. For instance, you might be uncertain whether 3 * 6 + 2 first multiplies 3 by 6 and then adds 2 to the result, or if it first adds 2 to 6 and then multiplies that by 3. In that case, if you want to be absolutely sure that you perform the multiplication first, just write (3 * 6) + 2, explicitly indicating with the parentheses that the multiplication should be performed first—even though doing so would not be strictly necessary in this case.\nIncidentally, you do not need to type out 3.14159 to approximate \\(\\pi\\) in the mathematical expressions above. R has a built-in constant, pi, that you can use instead. Therefore, exp(1.3)*(4 - sin(pi/3)) produces the same result as our earlier exp(1.3)*(4 - sin(3.14159/3)).\nAnother thing to note is that the number of spaces between various operations is irrelevant. 4*(9-6) is the same as 4*(9 - 6), or 4 * (9 - 6), or, for that matter, 4   * (9-    6). To the machine, they are all the same—it is only us, the human users, who might get confused by that last form…\nIt is possible to get help on any function from the system itself. Type either help(asin) or the shorter ?asin in the console to get information on the function asin, for instance. Whenever you are not sure how to use a certain function, just ask the computer.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R programming basics</span>"
    ]
  },
  {
    "objectID": "02_R_programming_basics.html#using-r-as-a-calculator",
    "href": "02_R_programming_basics.html#using-r-as-a-calculator",
    "title": "2  R programming basics",
    "section": "",
    "text": "Note\n\n\n\nIn line with Anglo-Saxon tradition, R uses decimal points instead of commas. A common mistake for people coming from other traditions is to type 1,62 * 34,56. This will throw an error.\n\n\n\n\n\n\nSymbol\nMeaning\nExample\nForm in R\n\n\n\n\n+\naddition\n\\(1 + 3\\)\n1 + 3\n\n\n-\nsubtraction\n\\(5 - 1\\)\n5 - 1\n\n\n*\nmultiplication\n\\(2 \\cdot 2\\)\n2 * 2\n\n\n/\ndivision\n\\(8 / 2\\)\n8 / 2\n\n\n^\nraise to power\n\\(2^2\\)\n2 ^ 2\n\n\n\n\n\n\nFunction\nMeaning\nExample\nForm in R\n\n\n\n\nlog\nnatural log\n\\(\\log(4)\\)\nlog(4)\n\n\nexp\nexponential\n\\(\\text{e}^4\\)\nexp(4)\n\n\nsqrt\nsquare root\n\\(\\sqrt{4}\\)\nsqrt(4)\n\n\nlog2\nbase-2 log\n\\(\\log_2(4)\\)\nlog2(4)\n\n\nlog10\nbase-10 log\n\\(\\log_{10}(4)\\)\nlog10(4)\n\n\nsin\nsine (radians!)\n\\(\\sin(4)\\)\nsin(4)\n\n\nabs\nabsolute value\n\\(|-4|\\)\nabs(-4)",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R programming basics</span>"
    ]
  },
  {
    "objectID": "02_R_programming_basics.html#variables-and-types",
    "href": "02_R_programming_basics.html#variables-and-types",
    "title": "2  R programming basics",
    "section": "2.2 Variables and types",
    "text": "2.2 Variables and types\n\n2.2.1 Numerical variables and variable names\nYou can assign a value to a named variable, and then whenever you call on that variable, the assigned value will be substituted. For instance, to obtain the square root of 9, you can simply type sqrt(9); or you can assign the value 9 to a variable first:\n\nx &lt;- 9\nsqrt(x)\n\n[1] 3\n\n\nThis will calculate the square root of x, and since x was defined as 9, we get sqrt(9), or 3. The assignment symbol &lt;- consists of a less-than symbol &lt; and a minus sign - written next to each other. Together, they look like an arrow pointing towards the left, meaning that we assign the value on the right to the variable on the left of the arrow. A keyboard shortcut in RStudio to create this symbol is to press Alt and - together.\nThe name for a variable can be almost anything, but a few restrictions apply. First, the name must consist only of letters, numbers, the period (.), and the underscore (_) character. Second, the variable’s name cannot start with a number or an underscore. So one_result or one.result are fine variable names, but 1_result or _one_result are not. Similarly, the name crowns to $ is not valid because of the spaces and the dollar ($) symbol, neither of which are numbers, letters, period, or the underscore.\nAdditionally, there are a few reserved words which have a special meaning in R, and therefore cannot be used as variable names. Examples are: if, NA, TRUE, FALSE, NULL, and function. You can see the complete list by typing ?Reserved.\nHowever, one can override all these rules and give absolutely any name to a variable by enclosing it in backward tick marks (` `). So while crowns to $ and function are not valid variable names, `crowns to $` and `function` are. For instance, you could type\n\n`crowns to $` &lt;- 0.09 # Approximate SEK-to-USD exchange rate\nmy_money &lt;- 123 # Assumed to be given in Swedish crowns\nmy_money_in_USD &lt;- my_money * `crowns to $`\nprint(my_money_in_USD)\n\n[1] 11.07\n\n\nto get our money’s worth in US dollars. Note that the freedom of naming our variables whatever we wish comes at the price of having to always include them between back ticks to refer to them. It is entirely up to you whether you would like to use this feature or avoid it; however, be sure to recognize what it means when looking at R code written by others.\nNotice also that the above chunk of code includes comments, prefaced by the hash (#) symbol. Anything that comes after the hash symbol on a line is ignored by R; it is only there for other humans to read.\n\n\n\n\n\n\nWarning\n\n\n\nThe variable my_money_in_USD above was defined in terms of the two variables my_money and `crowns to $`. You might be wondering: if we change my_money to a different value by executing my_money &lt;- 1000 (say), does my_money_in_USD also get automatically updated? The answer is no: the value of my_money_in_USD will remain unchanged. In other words, variables are not automatically recalculated the way Excel formula cells are. To recompute my_money_in_USD, you will need to execute my_money_in_USD &lt;- my_money * `crowns to $` again. This leads to a recurring theme in programming: while assigning variables is convenient, it also carries some dangers, in case we forget to appropriately update them. In this book we will be emphasizing a style of programming which avoids relying on (re-)assigning variables as much as possible.\n\n\n\n\n2.2.2 Strings\nSo far we have worked with numerical data. R can also work with textual information. In computer science, these are called character strings, or just strings for short. To assign a string to a variable, one has to enclose the text in quotes. For instance,\n\ns &lt;- \"Hello World!\"\n\nassigns the literal text Hello World! to the variable s. You can print it to screen either by just typing s at the console and pressing Enter, or typing print(s) and pressing Enter.\nOne useful function that works on strings is paste, which makes a single string out of several ones (in computer lingo, this is known as string concatenation). For example, try\n\ns1 &lt;- \"Hello\"\ns2 &lt;- \"World!\"\nmessage &lt;- paste(s1, s2)\nprint(message)\n\n[1] \"Hello World!\"\n\n\nThe component strings are separated by a space, but this can be changed with the optional sep argument to the paste function:\n\nmessage &lt;- paste(s1, s2, sep = \"\")\nprint(message)\n\n[1] \"HelloWorld!\"\n\n\nThis results in message becoming HelloWorld!, without the space in between. Between the quotes, you can put any character (including nothing, like above), which will be used as a separator when merging the strings s1 and s2. So specifying sep = \"-\" would have set message equal to Hello-World! (try it out and see how it works).\nIt is important to remember that quotes distinguish information to be treated as text from information to be treated as numbers. Consider the following two variable assignments:\n\na &lt;- 6.7\nb &lt;- \"6.7\"\n\nAlthough they look superficially similar, a is the number 6.7 while b is the string “6.7”, and the two are not equal! For instance, executing 2 * a results in 13.4, but 2 * b throws an error, because it does not make sense to multiply a bunch of text by 2.\n\n\n2.2.3 Logical values\nLet us type the following into the console, and press Enter:\n\n2 &gt; 1\n\n[1] TRUE\n\n\nWe are asking the computer whether 2 is larger than 1. And it returns the answer: TRUE. By contrast, if we ask whether two is less than one, we get FALSE:\n\n2 &lt; 1\n\n[1] FALSE\n\n\nSimilar to “greater than” and “less than”, there are other logical operations as well, such as “greater than or equal to”, “equal to”, “not equal to”, and others. The table below lists the most common options.\n\n\n\nSymbol\nMeaning\nExample in R\nResult\n\n\n\n\n&lt;\nless than\n1 &lt; 2\nTRUE\n\n\n&gt;\ngreater than\n1 &gt; 2\nFALSE\n\n\n&lt;=\nless than or equal\n2 &lt;= 5.3\nTRUE\n\n\n&gt;=\ngreater than or equal\n4.2 &gt;= 3.6\nTRUE\n\n\n==\nequal to\n5 == 6\nFALSE\n\n\n!=\nnot equal to\n5 != 6\nTRUE\n\n\n%in%\nis element of set\n2 %in% c(1, 2, 3)\nTRUE\n\n\n!\nlogical NOT\n!(1 &gt; 2)\nTRUE\n\n\n&\nlogical AND\n(1 &gt; 2) & (1 &lt; 2)\nFALSE\n\n\n|\nlogical OR\n(1 &gt; 2) | (1 &lt; 2)\nTRUE\n\n\n\nThe == and != operators can also be used with strings: \"Hello World\" == \"Hello World!\" returns FALSE, because the two strings are not exactly identical, differing in the final exclamation mark. Similarly, \"Hello World\" != \"Hello World!\" returns TRUE, because it is indeed true that the two strings are unequal.\nLogical values can either be TRUE or FALSE, with no other options.1 This is in contrast with numbers and character strings, which can take on a myriad different values. Note that TRUE and FALSE must be capitalized: true, False, or anything other than the fully capitalized forms will result in an error. Just like in the case of strings and numbers, logical values can be assigned to variables:\n\nlgl &lt;- 3 &gt; 4 # Since 3 &gt; 4 is FALSE, lgl will be assigned FALSE\nprint(!lgl) # lgl is FALSE, so !lgl (\"not lgl\") will be TRUE\n\n[1] TRUE\n\n\nThe function ifelse takes advantage of logical values, doing different things depending on whether some condition is TRUE or FALSE (“if the condition is true then do something, else do some other thing”). It takes three arguments: the first is a condition, the second is the expression that gets executed only if the condition is true, and the third is the expression that executes only if the condition is false. To illustrate its use, we can apply it in a program that simulates a coin toss. R will generate n random numbers between 0 and 1 by invoking runif(n). Here runif is a shorthand for “random-uniform”, randomly generated numbers from a uniform distribution between 0 and 1. The function call runif(1) therefore produces a single random number, and we can interpret values less than 0.5 as having tossed heads, and other values as having tossed tails. The following lines implement this:\n\ntoss &lt;- runif(1)\ncoin &lt;- ifelse(toss &lt; 0.5, \"heads\", \"tails\")\nprint(coin)\n\n[1] \"heads\"\n\n\nThis time we happened to have tossed heads, but try re-running the above three lines over and over again, to see that the results keep coming up at random.\n\n\n\n\n\n\nTip\n\n\n\nIn scientific applications, one often wants to create a sequence of random values that are repeatable. That is, while the sequence of \"heads\" and \"tails\" is random, the exact same sequence of values will be generated by any user who runs the R script. One can force such repeatability by setting the random generator seed. This seed is an integer value, and every choice will generate a different (but perfectly repeatable) random sequence. For instance, the random number generator can be seeded with the value 71 by typing set.seed(71).\nTo illustrate the difference between not seeding versus seeding the random number generator, here we run the coin-tossing program again, this time with eight tosses:\n\ntoss &lt;- runif(8)\ncoin &lt;- ifelse(toss &lt; 0.5, \"heads\", \"tails\")\nprint(coin)\n\n[1] \"heads\" \"heads\" \"tails\" \"tails\" \"heads\" \"heads\" \"heads\" \"heads\"\n\n\nIf we now execute the same three lines again, we will naturally get a different result because the coin tosses are, after all, random:\n\ntoss &lt;- runif(8)\ncoin &lt;- ifelse(toss &lt; 0.5, \"heads\", \"tails\")\nprint(coin)\n\n[1] \"heads\" \"tails\" \"tails\" \"tails\" \"tails\" \"tails\" \"tails\" \"heads\"\n\n\nTo ensure repeatability, we have to set the random number generator seed:\n\nset.seed(71) # Set the random number generator seed\ntoss &lt;- runif(8)\ncoin &lt;- ifelse(toss &lt; 0.5, \"heads\", \"tails\")\nprint(coin)\n\n[1] \"heads\" \"tails\" \"heads\" \"heads\" \"heads\" \"tails\" \"tails\" \"tails\"\n\n\nIf we run the above four lines together again, we get the exact same sequence of tosses as before:\n\nset.seed(71) # Set the random number generator seed\ntoss &lt;- runif(8)\ncoin &lt;- ifelse(toss &lt; 0.5, \"heads\", \"tails\")\nprint(coin)\n\n[1] \"heads\" \"tails\" \"heads\" \"heads\" \"heads\" \"tails\" \"tails\" \"tails\"",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R programming basics</span>"
    ]
  },
  {
    "objectID": "02_R_programming_basics.html#vectors",
    "href": "02_R_programming_basics.html#vectors",
    "title": "2  R programming basics",
    "section": "2.3 Vectors",
    "text": "2.3 Vectors\nA vector is simply a sequence of variables of the same type. That is, the sequence may consist of numbers or strings or logical values, but one cannot intermix them. The c function will create a vector in the following way:\n\nx &lt;- c(2, 5, 1, 6, 4, 4, 3, 3, 2, 5)\n\nThis is a vector of numbers. If, after entering this line, you type x or print(x) and press Enter, all the values in the vector will appear on screen:\n\nx\n\n [1] 2 5 1 6 4 4 3 3 2 5\n\n\nWhat can you do if you want to display only the third entry? The way to do this is by applying brackets:\n\nx[3]\n\n[1] 1\n\n\nNever forget that vectors and its elements are simply variables! To show this, calculate the value of x[1] * (x[2] + x[3]), but before pressing Enter, guess what the result will be. Then check if you were correct. You can also try typing x * 2:\n\nx * 2\n\n [1]  4 10  2 12  8  8  6  6  4 10\n\n\nWhat happened? Now you performed an operation on the vector as a whole, i.e., you multiplied each element of the vector by two. Remember: you can perform all the elementary operations on vectors as well, and then the result will be obtained by applying the operation on each element separately.\nCertain functions are specific to vectors. Try mean(x) and max(x) for instance (if you are not sure what these do, just ask by typing ?mean or ?max). Some others to try: min, median, sum, prod, and length.\nOne can quickly generate vectors of sequences of values, using one of two ways. First, the notation 1:10 generates a vector of integers ranging from 1 to 10 (inclusive), in steps of 1. Similarly, 2:7 generates the same vector as c(2, 3, 4, 5, 6, 7), and so on. Second, the function seq generates sequences, starting with the first argument, ending with the second, in steps defined with the by argument. So calling\n\nseq(0, 3, by = 0.2)\n\n [1] 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0\n\n\ncreates a vector of numbers ranging from 0 to 3, in steps of 0.2.\nJust as one can create a vector of numerical values, it is also possible to create a vector of character strings or of logical values. For example:\n\nstrVec &lt;- c(\"I am the first string\", \"I am the second\", \"And I am the 3rd\")\n\nNow strVec[1] is simply equal to the string \"I am the first string\", strVec[2] is equal to \"I am the second\", and so on. Similarly, defining\n\nlogicVec &lt;- c(TRUE, FALSE, TRUE, TRUE)\n\ngives a vector whose second entry, logicVec[2], is equal to FALSE, and its other three entries are TRUE.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R programming basics</span>"
    ]
  },
  {
    "objectID": "02_R_programming_basics.html#exercises",
    "href": "02_R_programming_basics.html#exercises",
    "title": "2  R programming basics",
    "section": "2.4 Exercises",
    "text": "2.4 Exercises\n\nWhich of the variable names below are valid, and why?\n\nfirst.result.of_computation\n2nd.result.of_computation\ndsaqwerty\ndsaq werty\n`dsaq werty`\nbreak\nis this valid?...\n`is this valid?...`\nis_this_valid?...\n\nIn a single line of R code, calculate the number of seconds in a leap year.\nLoad and display the limerick that you worked with in Exercises 5 and 6 of Section 1.5. Follow its instructions and verify that the result is indeed what the verse claims it should be. (Hint: one dozen is 12, one gross is a dozen dozens, or 144, and one score is 20.)\nThe Hardy-Weinberg law of population genetics (e.g., Gillespie 2004) establishes a connection between the frequency of alleles and that of genotypes within a population.2 Assume there are two alleles of a gene, A and a, in a human population. People with the AA genotype have brown eyes; those with Aa have green eyes, and those with aa have blue eyes. What fraction of people will have these three genotypes, assuming that the frequency of A is 18%? How about 37%? And 57%?\nApplying the same rule backwards: what are the frequencies of the alleles A and a if 49% of the population has brown eyes, 42% have green eyes, and 9% have blue eyes?\nThe breeder’s equation states that the response to selection (how much the average trait of a population changes in one generation) is equal to the selection differential (mean difference of the selected parents’ traits from the population average) times the heritability of the trait (a number between 0 and 1, documenting the fraction of the variation in the trait due to genetic vs. other, non-heritable factors). In terms of an equation: \\(R = S \\cdot h^2\\), where \\(R\\) is the response to selection, \\(S\\) is the selection differential, and \\(h^2\\) is the heritability. Assume now that a constant selection differential of \\(S = 0.5\\) is applied to wing length in a population of crickets, where the mean wing length is initially 8 millimeters. The heritability is \\(h^2 = 0.74\\). What will be the mean wing length in the population after one generation? After two generations? After three generations?\nCreate a vector called z, with entries 1.2, 5, 3, 13.7, 6.66, and 4.2 (in that order). Then, by applying functions to this vector, obtain:\n\nIts smallest entry.\nIts largest entry.\nThe sum of all its entries.\nThe number of entries in the vector.\nThe vector’s entries sorted in increasing order (Hint: look up the function sort).\nThe vector’s entries sorted in decreasing order.\nThe product of the fourth entry with the difference of the third and sixth entries. Then take the absolute value of the result.\n\nDefine a vector of strings, called s, with the three entries \"the fat cat\", \"sat on\", and \"the mat\".\n\nCombine these three strings into a single string, and print it on the screen. (Hint: look up the help page for the paste function, in particular its collapse argument.)\nReverse the entries of s, so they come in the order \"the mat\", \"sat on\", and \"the fat cat\" (hint: check out the rev function). Then merge the three strings again into a single one, and print it on the screen.\n\n\n\n\n\n\nGillespie, John H. 2004. Population Genetics: A Concise Guide. Baltimore, MD, USA: Johns Hopkins University Press.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R programming basics</span>"
    ]
  },
  {
    "objectID": "02_R_programming_basics.html#footnotes",
    "href": "02_R_programming_basics.html#footnotes",
    "title": "2  R programming basics",
    "section": "",
    "text": "Technically, there is a third option: a logical value could be equal to NA, indicating missing data. Numerical and string variables can also be NA to show that their values are missing.↩︎\nThe law assumes random mating and the absence of selection, genetic drift, or gene flow into the population. This “law” is really just an application of elementary logic: if a gene has two alleles (call them A and a) and their proportions in the population are \\(p\\) and \\(q\\) respectively, then random mating means that the proportion of AA genotypes is \\(p^2\\) (i.e., the probability that an allele A meets another allele A), the proportion of Aa genotypes is \\(2pq\\) by a similar logic (the 2 is there because of the two possible allele combinations Aa and aA), and that of aa genotypes is \\(q^2\\). Naturally, since every allele is either A or a, we have \\(p + q = 1\\).↩︎",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R programming basics</span>"
    ]
  },
  {
    "objectID": "03_Functions.html",
    "href": "03_Functions.html",
    "title": "3  Functions",
    "section": "",
    "text": "3.1 User-defined functions\nA function in R can be thought of as a black box which receives inputs and, depending on those inputs, produces some output. Vending machines provide a good working model of what a “function” is in computer science: depending on the inputs they receive (in the form of coins of various denomination, plus the buttons you press for a particular item) they give you some output (Mars bars, Coke, and the like). It’s just that computer scientists like to refer to the inputs as “function arguments” or simply “arguments” instead of coins, and to the output as the “return value”. Arguments are also often referred to as “parameters” to the function. The general workings of a function are illustrated below:\nWe have already seen some functions at work in R: for example, sqrt, log, and exp are all functions that take numbers and return some other number. In turn, the paste function (Section 2.2.2) takes one or more character strings and returns a single character string.\nWhen we ask a function to do something, we are calling the function. The arguments of functions are always enclosed in parentheses. For example, executing sqrt(9), calls the built-in square root function. Its argument (or input, or parameter) is 9, and its return value is the square root of 9, which is 3.\nJust like vending machines, functions may not be as straightforward as described above, but could do other things than simply return a value. Vending machines happen to impose a decrease in the amount of available money in your pocket after they are used. This is a very real effect with implications for our future possibilities in the real world, but has nothing to do with the pressing of buttons and the items delivered by the machine. Similarly, functions in R may do other things than simply return a value: they may print messages on the screen, change the working directory, or load packages. We have already seen examples of functions having these side effects: print will display the value of a variable on screen, setwd sets your working directory and is not used for the value it returns, and library is used for its effect of loading R packages.\nWhile such effects are important to keep in mind (and can be useful), below we will focus on functions which simply receive an input and produce an output based on those inputs, without any other effects.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "03_Functions.html#user-defined-functions",
    "href": "03_Functions.html#user-defined-functions",
    "title": "3  Functions",
    "section": "",
    "text": "3.1.1 How to define functions\nThus far, we have been using many built-in functions in R, such as exp, log, sqrt, seq, paste, and others. However, it is also possible to define our own functions, which can then be used just like any built-in function. The way to do this is to first give a name to the function. The naming rules for functions are exactly the same as the naming rules for variables (Section 2.2.1). Then we assign to this name the function keyword, followed by the function’s arguments in parentheses, and then the R code comprising the function’s body enclosed in curly braces {}. For example, here is a function which calculates the area of a circle with given radius:\n\ncircleArea &lt;- function(radius) {\n  area &lt;- radius^2 * pi\n  return(area)\n}\n\nThe function implements the formula that the area of a circle is equal to \\(\\pi\\) times its radius squared. The return keyword determines what result the function will output when it finishes executing. In this case, the function returns the value of area that is created within the function. After running the above lines, the computer now knows and remembers the function. Calling circleArea(3) will, for example, calculate the area of a circle with radius 3, which is approximately 28.27433.\nOne very important property of functions is that any variables defined within them (such as area above) are local to that function. This means that they are not visible from outside: even after calling the function, the variable area will not be accessible to the rest of the program, despite the fact that it was declared in the function. This helps us create programs with modular structure, where functions operate as black boxes: we can use them without looking inside.\nTwo things are good to be aware of. First, the return keyword is optional in functions. If omitted, the final expression evaluated within the body of the function automatically becomes the return value. Second, the curly braces used in defining functions are not in fact specific to functions per se. Instead, the rule is as follows: an arbitrary block of code between curly braces, potentially made up of several expressions, is always treated as if it was one single expression. Its value is then by definition the final evaluated expression within the braces. This also means that the curly braces are superfluous whenever they contain only a single expression.\nThis means that the definition of circleArea could be written in alternative forms as well, some being shorter than the one above. For starters, since a variable evaluates to the value it contains (naturally), one could simply remove return and have the same effect:\n\ncircleArea &lt;- function(radius) {\n  area &lt;- radius^2 * pi\n  area\n}\n\nBut then, one may also think that it is not worth defining area just for the sake of being able to return it. One might as well simply return radius^2 * pi to begin with:\n\ncircleArea &lt;- function(radius) {\n  radius^2 * pi # Or, equivalently: return(radius^2 * pi)\n}\n\nWhile the previous version of the function has better self-documentation (because by having named the intermediate variable area, we clarified its intended meaning and purpose), the latest one is more concise. Often, it is a question of taste which version one prefers. But there is an opportunity to compress even further. Since the purpose of the curly braces is to pretend that several expressions are just one single expression (its value being the last evaluated expression inside the braces), but the above function consists of just one expression anyway, the braces can be omitted:\n\ncircleArea &lt;- function(radius) radius^2 * pi\n\nThe four versions of circleArea are all equivalent from the point of view of the computer.\n\n\n3.1.2 Functions with more than one argument\nOne can define functions with more than one argument. For instance, here is a function that calculates the volume of a cylinder with given radius and height:\n\ncylinderVol &lt;- function(radius, height) {\n  baseArea &lt;- circleArea(radius)\n  volume &lt;- baseArea * height\n  return(volume)\n}\n\nHere we used the fact that the volume of a cylinder is the area of its base circle, times its height. Notice also that we made use of our earlier circleArea function within the body of cylinderVol. While this was not a necessity and we could have simply written volume &lt;- radius^2 * pi * height above, this is generally speaking good practice: by constructing functions to solve smaller problems, one can write slightly more complicated functions which make use of those simpler ones. Then, one will be able to write even more complex functions using the slightly more complex ones in turn—and so on. We will discuss this principle in more detail below, in Section 3.2.\nWhen calling a function with multiple arguments, the default convention is that their order should follow the same pattern as in the function’s definition. The function call cylinderVol(2, 3) means that radius will be set to 2 and height to 3, because that is the order in which the arguments were defined in function(radius, height). But one can override this default ordering by explicitly naming arguments, as explained below.\nIt is optional but possible to name the arguments explicitly. This means that calling circleArea(3) is the same as calling circleArea(radius = 3), and calling cylinderVol(2, 3) is the same as calling cylinderVol(radius = 2, height = 3). Even more is true: since naming the arguments removes any ambiguity about which argument is which, one may even call cylinderVol(height = 3, radius = 2), with the arguments in reverse order, and this will still be equivalent to cylinderVol(2, 3).\nWhile naming arguments this way is optional, doing so can increase the clarity of our programs. To give an example from a built-in function in R, take log(5, 3). Does this function compute the base-5 logarithm of 3, or the base-3 logarithm of 5? While reading the documentation reveals that it is the latter, one can clarify this easily, because the second argument of log is called base, as seen from reading the help after typing ?log. We can then write log(5, base = 3), which is now easy to interpret: it is the base-3 logarithm of 5.\n\nlog(5, base = 3)\n\n[1] 1.464974\n\n\nIt is also possible to define default values for one or more of the arguments to any function. If defaults are given, the user does not have to specify the value for that argument. It will then automatically be set to the default value instead. For example, one could rewrite the cylinderVol function to specify default values for radius and height. Making these defaults be 1 means we can write:\n\ncylinderVol &lt;- function(radius = 1, height = 1) {\n  baseArea &lt;- circleArea(radius)\n  volume &lt;- baseArea * height\n  return(volume)\n}\n\nIf we now call cylinderVol() without specifying arguments, the defaults will be substituted for radius and height. Since both are equal to 1, the cylinder volume will simply be \\(\\pi\\) (about 3.14159), which is the result we get back. Alternatively, if we call cylinderVol(radius = 2), then the function returns \\(4\\pi\\) (approximately 12.56637), because the default value of 1 is substituted in place of the unspecified height argument. Importantly, if we don’t define default values and yet omit to specify one or more of those parameters, we get back an error message. For example, our earlier circleArea function had no default value for its argument radius, so leaving it unspecified throws an error:\n\ncircleArea()\n\nError in circleArea(): argument \"radius\" is missing, with no default",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "03_Functions.html#sec-funccomp",
    "href": "03_Functions.html#sec-funccomp",
    "title": "3  Functions",
    "section": "3.2 Function composition",
    "text": "3.2 Function composition\nA function is like a vending machine: we give it some input(s), and it produces some output. The output itself may then be fed as input to another function—which in turn produces an output, which can be fed to yet another function, and so on. Chaining functions together in this manner is called the composition of functions. For example, we might need to take the square root of a number, then calculate the logarithm of the output, and finally, obtain the cosine of the result. This is as simple as writing cos(log(sqrt(9))), if the number we start with is 9. More generally, one might even define a new function (let us call it cls, after the starting letters of cos, log, and sqrt) like this:\n\ncls &lt;- function(x) {\n  return(cos(log(sqrt(x))))\n}\n\nA remarkable property of composition is that the composed function (in this case, cls) is in many ways just like its constituents: it is also a black box which takes a single number as input and produces another number as its output. Putting it differently, if one did not know that the function cls was defined manually as the composition of three more “elementary” functions, and instead claimed it was just another elementary built-in function in R, there would be no way to tell the difference just based on the behaviour of the function itself. The composition of functions thus has the important property of self-similarity: if we manage to solve a problem through the composition of functions, then that solution itself will behave like an “elementary” function, and so can be used to solve even more complex problems via composition—and so on.\nIf we conceive of a program written in R as a large lego building, then one can think of functions as the lego blocks out of which the whole construction is made up. Lego pieces are designed to fit well together, one can always combine them in various ways. Furthermore, any combination of lego pieces itself behaves like a more elementary lego piece: it can be fitted together with other pieces in much the same way. The composition of functions is analogous to building larger lego blocks out of simpler ones. Remarkably, just as the size of a lego block does not hamper our ability to stick them together, the composability of functions is retained regardless of how many more elementary pieces each of them consist of. Thus, the composition of functions is an excellent way of keeping the complexity of larger programs in hand.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "03_Functions.html#sec-pipes",
    "href": "03_Functions.html#sec-pipes",
    "title": "3  Functions",
    "section": "3.3 Function piping",
    "text": "3.3 Function piping\nOne problem with composing many functions together is that the order of application must be read backwards. An expression such as sqrt(sin(cos(log(1)))) means: “take the square root of the sine of the cosine of the natural logarithm of 1”. But it is more convenient for the human brain to think of it the other way round: we first take the log of 1, then the cosine of the result, then the sine of what we got, and finally the square root. The problem of interpreting composed functions gets more difficult when the functions have more than one argument. Even something as relatively simple as\n\nexp(mean(log(seq(-3, 11, by = 2)), na.rm = TRUE))\n\n[1] 4.671655\n\n\nmay cause one to stop and have to think about what this expression actually does—and it only involves the composition of four simple functions. One can imagine the difficulties of having to parse the composition of dozens of functions in this style.\nThe expression exp(mean(log(seq(-3, 11, by = 2)), na.rm = TRUE)) generates the numeric sequence -3, -1, 1, …, 11 (jumping in steps of 2), and computes their geometric mean. To do so, it takes the logarithms of each value, takes their mean, and finally, exponentiates the result back. The problem is that the logarithm of a negative number does not exist,1 and therefore, log(-3) and log(-1) both produce undefined results. Thus, when taking the mean of the logarithms, we must remove any such undefined values. This can be accomplished via an extra argument to mean, called na.rm (“NA-remove”). By default, this is set to FALSE, but by changing it to TRUE, undefined values are simply ignored when computing the mean. For example mean(c(1, 2, 3, NA)) returns NA, because of the undefined entry in the vector; but mean(c(1, 2, 3, NA), na.rm = TRUE) returns 2, the result one gets after discarding the NA entry.\nBut all this is quite difficult to see when looking at the expression\n\nexp(mean(log(seq(-3, 11, by = 2)), na.rm = TRUE))\n\nPart of the reason is the awkward backwards order of function applications which makes it hard to see which function the argument na.rm = TRUE belongs to. Fortunately, there is a simple operator in R called a pipe (written |&gt;), which allows one to write the same code in a more streamlined way. The pipe allows one to write function application in reverse order (first the argument and then the function), making the code more transparent. Formally, x |&gt; f() is equivalent to f(x) for any function f. For example, sqrt(9) can also be written 9 |&gt; sqrt(). Thus, sqrt(sin(cos(log(1)))) can be written as 1 |&gt; log() |&gt; cos() |&gt; sin() |&gt; sqrt(), which reads straightforwardly as “start with the number 1; then take its log; then take the cosine of the result; then take the sine of that result; and then, finally, take the square root to obtain the final output”. In general, it helps to pronounce |&gt; as “then”.2\nThe pipe also works for functions with multiple arguments. In that case, x |&gt; f(y, ...) is equivalent to f(x, y, ...). That is, the pipe refers to the function’s first argument (though, as we will see in Section 14.7, it is possible to override this). Instead of the awkward and hard-to-read mean(log(seq(-3, 11, by = 2)), na.rm = TRUE), we can therefore write:\n\nseq(-3, 11, by = 2) |&gt;\n  log() |&gt;\n  mean(na.rm = TRUE) |&gt;\n  exp()\n\n[1] 4.671655\n\n\nThis is fully equivalent to the traditional form, but is much more readable, because the functions are written in the order in which they actually get applied. Moreover, even though the program is built only from the composition of functions, it reads straightforwardly as if it was a sequence of imperative instructions: we start from the vector of integers c(-3, -1, 1, 3, 5, 7, 9, 11); then we take the logarithm of each; then we take their average, discarding any invalid entries (produced in this case by taking the logarithm of negative numbers); and then, finally, we exponentiate the result back to obtain the geometric mean.\n\n\n\n\n\n\nTip\n\n\n\nYou can create the pipe symbol |&gt; with the keyboard shortcut Ctrl+Shift+M (Cmd+Shift+M on a Mac). In case you get the symbol %&gt;% instead, do the following: in RStudio, click on the Tools menu at the top, and select Global options. A new menu will pop up. Click on Code in the panel on the left, and tick the box Use native pipe operator, |&gt; (requires R 4.1+). (In case you cannot see this option, then you are likely using a version of R earlier than 4.1; you should upgrade your R installation.)",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "03_Functions.html#exercises",
    "href": "03_Functions.html#exercises",
    "title": "3  Functions",
    "section": "3.4 Exercises",
    "text": "3.4 Exercises\n\nWrite a function called inc which takes a single number as its input and increments it by 1. That is, inc(0) should return 1, inc(4) should return 5, and so on.\nAssume you have a population of some organism in which one given allele of some gene is the only one available in the gene pool. If a new mutant organism with a different, selectively advantageous allele appears, it would be reasonable to conclude that the new allele will fix in the population and eliminate the original one over time. This, however, is not necessarily true, because a very rare allele might succumb to being eliminated by chance, regardless of how advantageous it is. According to the famous formula of Motoo Kimura (1924-1994), the probability of such a new allele eventually fixing in the population is given as: \\[ P = \\frac{1 - \\text{e}^{-s}}{1 - \\text{e}^{-2Ns}} \\] (e.g., Gillespie 2004). Here P is the probability of eventual fixation, s is the selection differential (the degree to which the new allele is advantageous over the original one), and N is the effective population size.\n\nWrite a function that implements this formula. It should take the selection differential s and the effective population size N as parameters, and return the fixation probability as its result. (Hint: the mathematical notation \\(\\text{e}^{x}\\) can be written exp(x) in R.)\nA selection differential of 0.5 is very strong (though not unheard of). What is the likelihood that an allele with that level of advantage will fix in a population of 1000 individuals? Interpret the result.\n\nWrite a function called contrary which takes a character string as input, and prepends it with the string \"un\". That is, contrary(\"satisfactory\") should return \"unsatisfactory\", contrary(\"kind\") should return \"unkind\", and so on. (Hint: for merging two strings into a single string, check out the paste function.)\nModify the function contrary you wrote above. In general, it should prepend \"un\" to the input as before. However, if the input string already starts with \"un\", it should return the string unchanged. That is, contrary(\"disclosed\") should still return \"undisclosed\", but contrary(\"undisclosed\") should return \"undisclosed\" instead of \"unundisclosed\". (Hint: look up the substr function, and use ifelse.)\nA text is palindromic if it reads backwards the same as it reads forwards. For example, “deified”, “racecar”, and “step on no pets” are palindromes. Assume that you are given some text, in the form of a character string, in all lowercase. For instance, you could be given the string \"noon\" (a palindrome) or \"hello\" (not a palindrome). Here is an outline of a function which checks whether the string is a palindrome:\n\nisPalindrome &lt;- function(text) {\n  textAsVector &lt;- strsplit(text, split = \"\")[[1]]\n  ...\n}\n\nThe strsplit(text, split = \"\")[[1]] part takes the string represented by the input variable text and breaks it up into a vector of individual characters. Try it out: for example, strsplit(\"madam\", split = \"\")[[1]] takes the word \"madam\" and splits it into its individual characters c(\"m\", \"a\", \"d\", \"a\", \"m\"). (Feel free to look up the help page of strsplit, but otherwise, you do not need to worry about how it does its magic.) After defining textAsVector, the rest of the function’s definition is missing. Your task is to complete the function, so that it returns TRUE if the supplied text is a palindrome and FALSE otherwise. (Hint: reverse the vector textAsVector, collapse both the original and the reversed vectors into single strings, and then compare them using logical equality.)\nModify the isPalindrome function to allow for both upper- and lowercase text, treating case as irrelevant (i.e., \"A\" is treated to be equal to \"a\" when evaluating whether the text is palindromic). One simple way to do this is to convert each character of the text into lowercase, and use this standardized text for reversing and comparing with. Look up the function tolower, and implement this improvement in your palindrome checker function.\n\n\n\n\n\nGillespie, John H. 2004. Population Genetics: A Concise Guide. Baltimore, MD, USA: Johns Hopkins University Press.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "03_Functions.html#footnotes",
    "href": "03_Functions.html#footnotes",
    "title": "3  Functions",
    "section": "",
    "text": "More precisely, it is not a real number. Just as with square roots, negative numbers have complex logarithms. In particular, \\(\\log(-1) = \\log(\\text{e}^{\\text{i}\\pi}) = \\text{i}\\pi\\) and \\(\\sqrt{-1} = \\text{i}\\). Both are rigorously provable propositions, and they have profound if strange-looking consequences. For instance, \\(\\log(-1)\\) divided by \\(\\sqrt{-1}\\) is exactly \\(\\pi\\). This has prompted the 19th-century English mathematician Augustus De Morgan (1806-1871) to exclaim: “Imagine a person with a gift of ridicule [who might say:] first that a negative quantity has no logarithm; secondly that a negative quantity has no square root; thirdly that the first non-existent is to the second as the circumference of a circle is to the diameter.” (Note: it is possible to compute with complex numbers in R. Try executing log(-1 + 0i) / sqrt(-1 + 0i) to verify De Morgan’s result.)↩︎\nThis built-in pipe |&gt; has only been available since R 4.1. Upgrade your R version if necessary. Alternatively, you can use another pipe operator that is provided by the magrittr package. It is written %&gt;% instead of |&gt;, but otherwise works identically (at least at this level; we will be mentioning some of the subtler differences in Section 14.7). To use the magrittr pipe, install the package first with install.packages(\"magrittr\") and then load it with library(magrittr). Now you can use %&gt;%. Incidentally, the package name magrittr is an allusion to Belgian surrealist artist René Magritte (1898-1967) because of his famous painting La trahison des images.↩︎",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "04_Data_reading.html",
    "href": "04_Data_reading.html",
    "title": "4  Reading tabular data from disk",
    "section": "",
    "text": "4.1 The tidyverse package suite\nA suite of R packages, sharing the same design philosophy, are collected under the name tidyverse. The tidyverse describes itself as “an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.” The packages in this ecosystem work well together, making it easy to combine their elements to perform a wide range of data analysis tasks. In case it is not yet installed on your computer, type\ninstall.packages(\"tidyverse\")\nat the R console and press Enter. After the package is installed, we can load it via the function call\nlibrary(tidyverse)\nLooking at the message generated by executing the above line, we see that nine packages are now loaded.1 They are called ggplot2, tibble, and so on. We will get to know these in more detail throughout the book.\nThere are even more packages that are part of the tidyverse. Typing and executing tidyverse_packages() will show all such packages. Of all these, only eight are loaded by default when invoking library(tidyverse). The others must be loaded separately. For example, readxl is a tidyverse package for loading Excel files in R. To use it, run library(readxl).",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading tabular data from disk</span>"
    ]
  },
  {
    "objectID": "04_Data_reading.html#the-tidyverse-package-suite",
    "href": "04_Data_reading.html#the-tidyverse-package-suite",
    "title": "4  Reading tabular data from disk",
    "section": "",
    "text": "Tip\n\n\n\nIn general, it is a good idea to load all necessary packages at the top of your R script, instead of loading them wherever the need to use them first arises. There are two reasons for this. First, if you close and then later reopen RStudio, the packages do not get automatically reloaded—one must execute the calls to library all over again. Second, often other users will run the scripts you write on their own computers, and they will not be able to do so unless the proper packages are loaded first. It is then helpful to others if the necessary packages are all listed right at the top, showing what is needed to run your program.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading tabular data from disk</span>"
    ]
  },
  {
    "objectID": "04_Data_reading.html#data-file-formats",
    "href": "04_Data_reading.html#data-file-formats",
    "title": "4  Reading tabular data from disk",
    "section": "4.2 Data file formats",
    "text": "4.2 Data file formats\nOne of the packages loaded by default with the tidyverse is called readr. This package contains tools for loading data files and writing them to disk. We will explore how it works using an example dataset. Before turning to the data however, it is worth mentioning a few things about data file formats.\nTabular data (i.e., data which can be organized into a table with rows and named columns) may be stored in many different forms. By far the most popular choice is to use Excel spreadsheets. While these may have many good properties, they are decidedly not recommended for scientific use. Instead, in science we strive to rely on plain text files to store our data. Plain text files have several advantages. First, they can be read by any machine without the need for special or proprietary software (such as Excel). Second, they will never go obsolete: one will always be able to open plain text files on any machine. By contrast, there is no guarantee that an Excel file saved today will still open ten years from now in future versions of Excel. And third, file formats such as Excel’s .xls and .xlsx contain a lot of metadata about formatting, font types, locale, etc. that are obscured from the user, but which have the potential to be in the way when performing scientific analyses. By contrast, plain text files have nothing up their sleeve: what you see they contain is exactly what you get; no more, no less. For these reasons, this book will emphasize workflows that rely on data stored in plain text files. However, due to the popularity of Excel, we will also learn how to read data from Excel files into R (Section 4.6), even though their use is otherwise not recommended.\n\n4.2.1 The CSV file format\nHow can one store data in plain text files? There are various solutions, but the one we will be relying on is called a delimited file. These files contain information as in a spreadsheet, with contents from every row in a different line. The column entries are separated by some delimiter—a character that represents the end of the information in one column and the start of the next. As an example, let us consider some actual data on land snail species from the Galápagos Islands.2 The data file is available by clicking on this link: islands-FL.csv.3 Set the working directory in RStudio to the folder where you have saved it. As a reminder, one can do this by executing setwd(\"/path/to/files\"), where one should substitute in one’s own path in place of /path/to/files. Then one can open islands-FL.csv in RStudio by clicking on it in the Files panel in the lower right part of the RStudio window, and then choosing the option “View file” (ignore the other option called “Import dataset…”). Having done this, a new tab opens in the editor panel (upper left region) where something like the following should appear:\nhabitat,species,size,shape\nhumid,ustulatus,17.088,-0.029\nhumid,ustulatus,20.06,-0.001\nhumid,ustulatus,16.34,0.014\narid,calvus,13.734,-0.043\nhumid,nux,21.898,-0.042\nhumid,ustulatus,16.848,-0.023\nhumid,ustulatus,19.162,0.014\nhumid,ustulatus,16.017,0.042\narid,galapaganus,18.894,0.011\nhumid,nux,26.59,0\nAnd so on. Here the first row does not contain data, but instead contains the names of the different columns. Both the names in the first row and the data in subsequent ones are separated by a comma, which is our delimiter that separates information belonging in different columns. This also explains the strange-looking extension .csv to the file: this stands for comma-separated values. Comma-separated value files are some of the most often used ones in science, and we will be relying on them frequently throughout the book.\n\n\n4.2.2 The Galápagos land snail data\nIt is worth explaining what the dataset in the file island-FL.csv represents, as we will be using it repeatedly in subsequent examples. Each row contains information on one land snail individual sampled from the Galápagos Islands. To simplify the data, the individuals have been restricted to just those that are from Floreana Island in the Galápagos. The columns in the data are:\n\nhabitat: This can be either “arid” or “humid”.\nspecies: Which species the individual belongs to. The Genus is always Naesiotus, from the family Bulimulidae. The seven species in the data are Naesiotus calvus, N. galapaganus, N. invalidus, N. nux, N. rugulosus, N. unifasciatus, and N. ustulatus.\nsize: A measurement of the absolute volume of the snail’s shell. It is given in units we are not concerned with here, but larger values correspond to larger shells. See Parent and Crespi (2009), Kraemer et al. (2018), and Kraemer et al. (2022) for more information.\nshape: A measurement for the shell’s shape. Again, the units are not important for us here, but small values represent bulky and round shells, while large values represent long and slender shells.\n\nBelow are some pictures of these snails (courtesy of Dr. Christine E. Parent):\n\n\n\nNaesiotus ustulatus – a land snail from Floreana Island, Galápagos. Photo credits: Dr. Christine E. Parent.\n\n\n\n\n\nN. ochsneri – a land snail from Santa Cruz Island, Galápagos. Photo credits: Dr. Christine E. Parent.\n\n\n\n\n\nN. chemnitzioides – a land snail from San Cristóbal Island, Galápagos. Photo credits: Dr. Christine E. Parent.\n\n\nShell morphology is an important indicator of microhabitat specialization in these snails: species with long slender shells are adaptations to arid environments due to their better surface-to-volume ratios (see the picture of N. chemnitzioides above for an example), whereas species with round and bulky shells are better adapted to humid environments.\n\n\n4.2.3 Tabulator-separated and other delimited files\nAbove we have discussed comma-separated files. Another type of file uses tabulators instead of commas as the column separator. These are called tab-separated value (TSV) files. Just like CSV files, TSV files are also very widely used. An example is provided by the file islands-FL.tsv. The data in this file are exactly identical to the one in islands-FL.csv. The only difference is that the commas are replaced by tabulators. Opening the file in RStudio, the first few lines look like this:\nhabitat species size    shape\nhumid   ustulatus   17.088  -0.029\nhumid   ustulatus   20.06   -0.001\nhumid   ustulatus   16.34   0.014\narid    calvus  13.734  -0.043\nhumid   nux 21.898  -0.042\nhumid   ustulatus   16.848  -0.023\nhumid   ustulatus   19.162  0.014\nhumid   ustulatus   16.017  0.042\narid    galapaganus 18.894  0.011\nhumid   nux 26.59   0\nCommas and tabulators are not the only possible delimiters however, and in principle any character could play that role. The only thing to be wary of is that choosing some “regular” character as the delimiter might create unintended side effects. For instance, if the character 4 is the delimiter, then a number such as 0.143 will no longer be interpreted as a single number, but as two numbers in separate columns (0.1 and 3, respectively). For this reason, only those characters should be used as delimiters which we can be certain will never be confused with the actual data.\nTo give an example: it is perfectly possible and legal (even if not common) to use a “percent-separated value” representation where the symbol % is the delimiter. Here is what that would look like with the same land snail data (one can download it from islands-FL.psv):\nhabitat%species%size%shape\nhumid%ustulatus%17.088%-0.029\nhumid%ustulatus%20.06%-0.001\nhumid%ustulatus%16.34%0.014\narid%calvus%13.734%-0.043\nhumid%nux%21.898%-0.042\nhumid%ustulatus%16.848%-0.023\nhumid%ustulatus%19.162%0.014\nhumid%ustulatus%16.017%0.042\narid%galapaganus%18.894%0.011\nhumid%nux%26.59%0\n\n\n\n\n\n\nWarning\n\n\n\nThe common file extensions .csv and .tsv are useful and suggestive: they can indicate that the data are comma- or tab-separated. However, one must bear in mind that these are mere naming conventions which are not forced in any way. That is, while not necessarily a good idea, it is perfectly possible to save comma-separated data in a file called mydata.tsv, or tab-separated data in mydata.csv. Furthermore, plain text files often have extensions such as .txt (“text”) or .dat (“data”), neither of which reveal what form the data were stored in. To be absolutely sure, one must open the file in a text editor such as RStudio’s top left editor panel, and see what was used as a delimiter. If you open a file like this, make absolutely sure not to modify and save it accidentally. Doing so compromises your hard-won data, which then no longer accurately capture your true observations.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading tabular data from disk</span>"
    ]
  },
  {
    "objectID": "04_Data_reading.html#the-tibble-data-structure",
    "href": "04_Data_reading.html#the-tibble-data-structure",
    "title": "4  Reading tabular data from disk",
    "section": "4.3 The tibble data structure",
    "text": "4.3 The tibble data structure\nThe above raw formats are not yet amenable to processing within R. To make it so, we first need to import them. For delimited files there is a convenient function, read_delim, which makes this especially simple:4\n\nread_delim(\"island-FL.csv\", delim = \",\")\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nThe function read_delim takes two inputs: the name of the file (with a path if needed), and delim, which is the delimiter used to separate the columns in the file. We will discuss how it works in greater detail in Section 4.4.\nLook at the output that was produced by read_delim(\"island-FL.csv\", delim = \",\") above, starting with # A tibble: 223 x 4. A tibble (or data frame5) is the R-equivalent of an Excel-style spreadsheet. In this case, it has 223 rows and 4 columns (hence the 223 x 4). The way to conceive of a tibble is as a collection of vectors, each arranged in a column, glued together side-by-side to form a table of data. Importantly, although each vector must consist of entries of the same type as usual (e.g., they can be vectors of numbers, vectors of strings, or vectors of logical values), the different columns need not share types. For example, in the above table, the first and second columns consist of character strings but the third and fourth ones consist of numerical values. This can be seen right below the header information. Below habitat and species you can see &lt;chr&gt;, which stands for “character string”. Below size and shape we have &lt;dbl&gt; which, confusing as it may look at first, refers simply to ordinary numbers.6 In turn, columns comprising of logical values would have the tag &lt;lgl&gt; underneath them (in this case though, we don’t have such a column). The point is that by looking at the type information below the header, you can see how R has interpreted each of the columns at a glance.\nThe fact that the individual columns are simply vectors can be made explicit, by relying on the $-notation. To access a given column of the table as a vector, we write the name of the table, followed by the $ symbol, followed by the name of the column in question. To make the illustration easier, let us first assign the tibble to a variable called snailDat:\n\nsnailDat &lt;- read_delim(\"island-FL.csv\", delim = \",\")\n\nAnd now one can access e.g. the size column from the snailDat table as a vector of numbers like this:\n\nsnailDat$size\n\n  [1] 17.088 20.060 16.340 13.734 21.898 16.848 19.162 16.017 18.894 26.590\n [11] 17.865 13.857 21.706 15.354 21.369 22.714 16.530 16.604 16.292 16.075\n [21] 18.154 21.025 12.895 25.046 23.227 17.284 16.701 23.515 21.903 17.314\n [31] 23.891 21.349 24.946 14.122 28.467 17.010 16.831 16.641 20.244 22.042\n [41] 18.665 18.680 18.949 16.933 20.399 22.907 18.265 24.269 20.704 16.553\n [51] 19.246 24.894 23.610 14.131 15.617 18.605 18.343 19.340 17.017 16.868\n [61] 17.814 16.311 17.066 15.386 23.167 21.272 16.799 17.383 15.178 17.195\n [71] 19.027 15.011 23.723 17.569 20.091 18.773 22.885 13.661 33.792 19.390\n [81] 19.513 18.246 17.637 19.865 18.812 17.903 16.614 28.810 27.675 31.672\n [91] 17.209 21.618 26.420 19.135 20.493 15.142 17.195 17.382 15.502 14.404\n[101] 22.678 16.210 30.946 14.347 14.541 20.546 20.994 20.103 31.923 24.465\n[111] 24.451 19.188 20.610 16.193 15.989 25.314 16.610 13.507 16.744 21.330\n[121] 20.480 19.246 19.125 18.642 18.250 17.152 18.814 21.081 15.852 16.743\n[131] 16.602 14.879 12.299 21.646 20.539 21.798 19.211 18.168 24.985 19.938\n[141] 20.176 14.846 23.488 24.120 17.320 16.118 18.924 15.928 15.478 21.282\n[151] 15.838 19.506 24.212 18.066 17.427 15.213 20.748 20.504 20.311 17.444\n[161] 20.846 19.351 18.679 24.111 17.906 16.322 21.967 15.992 22.788 17.920\n[171] 18.804 24.760 25.766 23.452 16.145 16.040 18.868 17.528 16.556 22.199\n[181] 24.424 18.800 21.335 20.191 19.225 23.424 15.601 27.679 29.221 20.373\n[191] 16.037 17.577 21.852 18.969 16.933 19.290 24.102 21.398 16.369 19.649\n[201] 25.137 18.394 23.102 32.001 16.946 19.054 18.527 18.646 21.361 14.362\n[211] 15.858 17.201 19.886 19.264 21.503 21.728 23.312 20.423 21.912 18.158\n[221] 16.859 20.488 22.369\n\n\nHere snailDat$size is really just a vector, and can be treated as such. For example, to get the 9th entry of this vector, we can use the usual bracket notation:\n\nsnailDat$size[9]\n\n[1] 18.894\n\n\nThe result is an ordinary numerical value.\nFinally, let us take one more look at the output again:\n\nprint(snailDat)\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nWhen displaying large tibbles, R will not dump all the data at you. Instead, it will display the first 10 rows, with a message indicating how many more rows remain (in our case, we have ...with 213 more rows written at the end of the printout). The system is still aware of the other rows; it just does not show them. To get a full view of a tibble in a more digestible, spreadsheet-like style, one can use the view function. Try running view(snailDat) and see what happens!",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading tabular data from disk</span>"
    ]
  },
  {
    "objectID": "04_Data_reading.html#sec-delim",
    "href": "04_Data_reading.html#sec-delim",
    "title": "4  Reading tabular data from disk",
    "section": "4.4 Reading delimited files",
    "text": "4.4 Reading delimited files\nWe have seen that the function read_delim works by taking two inputs: the name of the file to read, and the delimiter character. So to read a comma-separated file, we can write\n\nread_delim(\"island-FL.csv\", delim = \",\")\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nSimilarly, in case we want to read a percent-separated file such as islands-FL.psv, all we need to do is change the delimiter:\n\nread_delim(\"island-FL.psv\", delim = \"%\")\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nThe output is exactly as before, since the file contained the same information (just represented differently due to the different delimiter).\nTo read a tab-separated file, remember not to simply press the tabulator key between the quotes of the delim argument. Instead, R uses the character string \"\\t\" to represent a single press of the tabulator:\n\nread_delim(\"island-FL.tsv\", delim = \"\\t\")\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nWhat happens if the wrong delimiter is specified—for instance, if we type read_delim(\"island-FL.csv\", delim = \"%\")? In that case R will look for the % character to separate the columns, but since these do not occur anywhere, each row will be interpreted as a single column entry in its entirety:\n\nread_delim(\"island-FL.csv\", delim = \"%\")\n\n# A tibble: 223 × 1\n   `habitat,species,size,shape` \n   &lt;chr&gt;                        \n 1 humid,ustulatus,17.088,-0.029\n 2 humid,ustulatus,20.06,-0.001 \n 3 humid,ustulatus,16.34,0.014  \n 4 arid,calvus,13.734,-0.043    \n 5 humid,nux,21.898,-0.042      \n 6 humid,ustulatus,16.848,-0.023\n 7 humid,ustulatus,19.162,0.014 \n 8 humid,ustulatus,16.017,0.042 \n 9 arid,galapaganus,18.894,0.011\n10 humid,nux,26.59,0            \n# ℹ 213 more rows\n\n\nAs seen, we have a tibble with a single column called `habitat,species,size,shape1,shape2`, which has the type &lt;chr&gt; (character string). One must beware of such mistakes, because the computer will not signal any errors: for all it cares, we did intend % to be the delimiter. However, one will not be able to properly work with the data subsequently if it gets loaded in this mistaken form.\nThe CSV and TSV file formats are so common that there are shorthands available for loading them. Instead of writing read_delim(\"island-FL.csv\", delim = \",\"), one can equivalently type read_csv(\"island-FL.csv\") for the same effect:\n\nread_csv(\"island-FL.csv\")\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nSimilarly, read_tsv(\"island-FL.tsv\") is an equivalent shorthand to read_delim(\"island-FL.tsv\", delim = \"\\t\"):\n\nread_tsv(\"island-FL.tsv\")\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading tabular data from disk</span>"
    ]
  },
  {
    "objectID": "04_Data_reading.html#sec-rename",
    "href": "04_Data_reading.html#sec-rename",
    "title": "4  Reading tabular data from disk",
    "section": "4.5 Naming (and renaming) columns",
    "text": "4.5 Naming (and renaming) columns\nAll of the files we have looked at so far contained a header as their first row: instead of containing data, they contained the names of the data columns. Having such a header is good practice. Sometimes however, the header may be missing. For example, the file islands-FL-nohead.csv contains the same land snail data as the other files above, but it lacks a header. The first few lines of the file look like this:\nhumid,ustulatus,17.088,-0.029\nhumid,ustulatus,20.06,-0.001\nhumid,ustulatus,16.34,0.014\narid,calvus,13.734,-0.043\nhumid,nux,21.898,-0.042\nhumid,ustulatus,16.848,-0.023\nhumid,ustulatus,19.162,0.014\nhumid,ustulatus,16.017,0.042\narid,galapaganus,18.894,0.011\nhumid,nux,26.59,0\nIf one tries to read this file either with read_delim(\"island-FL-nohead.csv\", delim = \",\") or the shorthand read_csv(\"island-FL-nohead.csv\"), we get the following:\n\nread_delim(\"island-FL-nohead.csv\", delim = \",\")\n\n# A tibble: 222 × 4\n   humid ustulatus   `17.088` `-0.029`\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n 1 humid ustulatus       20.1   -0.001\n 2 humid ustulatus       16.3    0.014\n 3 arid  calvus          13.7   -0.043\n 4 humid nux             21.9   -0.042\n 5 humid ustulatus       16.8   -0.023\n 6 humid ustulatus       19.2    0.014\n 7 humid ustulatus       16.0    0.042\n 8 arid  galapaganus     18.9    0.011\n 9 humid nux             26.6    0    \n10 arid  calvus          17.9   -0.024\n# ℹ 212 more rows\n\n\nThe fact that a header is missing is obvious to a human reading the file—but not to the computer, which simply took the first row to be the header names anyway and interpreted the data in there as if they were column names. To avoid doing this, one can pass col_names = FALSE as an argument to either read_delim, read_csv, or read_tsv. The col_names argument is set by default to TRUE; to override it, we must explicitly change it like this:\n\nread_delim(\"island-FL-nohead.csv\", delim = \",\", col_names = FALSE)\n\n# A tibble: 223 × 4\n   X1    X2             X3     X4\n   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid ustulatus    17.1 -0.029\n 2 humid ustulatus    20.1 -0.001\n 3 humid ustulatus    16.3  0.014\n 4 arid  calvus       13.7 -0.043\n 5 humid nux          21.9 -0.042\n 6 humid ustulatus    16.8 -0.023\n 7 humid ustulatus    19.2  0.014\n 8 humid ustulatus    16.0  0.042\n 9 arid  galapaganus  18.9  0.011\n10 humid nux          26.6  0    \n# ℹ 213 more rows\n\n\nWhile this works, the column names now default to the moderately informative labels X1, X2, and so on. In fact, one can use the col_names argument to explicitly specify a vector of character strings, which are then interpreted as the names to be given to the columns in case no header information exists within the file itself. For example:\n\nread_delim(\"island-FL-nohead.csv\", delim = \",\",\n           col_names = c(\"habitat\", \"species\", \"size\", \"shape\"))\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nAnd now these data are identical to our earlier snailDat tibble.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading tabular data from disk</span>"
    ]
  },
  {
    "objectID": "04_Data_reading.html#sec-excel",
    "href": "04_Data_reading.html#sec-excel",
    "title": "4  Reading tabular data from disk",
    "section": "4.6 Excel tables",
    "text": "4.6 Excel tables\nAlthough their use is discouraged in science, one should know how to read data from an Excel spreadsheet. To do this, one needs to load the readxl package. This package is part of the tidyverse, but does not get automatically attached when running library(tidyverse). Therefore, we first load the package:\n\nlibrary(readxl)\n\nWe can now load Excel files with the function read_excel(). At the start, we downloaded an Excel version of the land snail data, called island-FL.xlsx. It holds the exact same data as the original CSV file, just saved in Excel format for instructive purposes. Let us load this file:\n\nread_excel(\"island-FL.xlsx\", sheet = 1)\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nThe sheet argument specifies which sheet of an Excel file we are importing (this file has only one of them, so technically the sheet = 1 argument would have been optional). It can also be given as a character string, specifying the name of the sheet to be imported. The function read_excel has several further options as well—check its help page via ?read_excel for more information.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading tabular data from disk</span>"
    ]
  },
  {
    "objectID": "04_Data_reading.html#writing-data-to-files",
    "href": "04_Data_reading.html#writing-data-to-files",
    "title": "4  Reading tabular data from disk",
    "section": "4.7 Writing data to files",
    "text": "4.7 Writing data to files\nFinally, data can not only be read from a file, but also written out to one. Then, instead of read_delim, read_csv, read_tsv and the like, one uses write_delim, write_csv, write_tsv, and so on. For instance, to save some tibble called dat in CSV form, one can do either\n\nwrite_delim(dat, file = \"/path/to/file.csv\", delim = \",\")\n\nor the equivalent but shorter\n\nwrite_csv(dat, file = \"/path/to/file.csv\")\n\nwhere /path/to/file.csv should be replaced by the path and file name with which the data ought to be saved.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading tabular data from disk</span>"
    ]
  },
  {
    "objectID": "04_Data_reading.html#sec-reading-exercises",
    "href": "04_Data_reading.html#sec-reading-exercises",
    "title": "4  Reading tabular data from disk",
    "section": "4.8 Exercises",
    "text": "4.8 Exercises\n\nGoldberg et al. (2010) collected data on self-incompatibility in the family Solanaceae (nightshades). It contains a list of 356 species, along with a column determining self-incompatibility status (0: self-incompatible; 1: self-compatible; 2-5: more complicated selfing scenarios). The data are in the file Goldberg2010_data.csv. An equivalent version is available in Goldberg2010_data.xlsx, with the only difference that it is saved in Excel format.\n\nRead the file Goldberg2010_data.csv using read_delim.\nRead the file Goldberg2010_data.csv using read_csv.\nRead the Excel file Goldberg2010_data.xlsx using read_excel from the readxl package.\nRead the file Goldberg2010_data.csv using read_csv, and assign it to a variable called goldbergDat.\nExtract the species names (first column, called Species) from goldbergDat as a vector of character strings, using the $ notation. What are the 42nd and 137th entries of this vector?\n\nThe data file ladybugs.csv contains information on whether ladybugs sampled from rural versus industrial areas tend to be red or black morphs. Black morphs are thought to be an adaptation to more polluted industrial environments where the dark coloration provides better camouflage. The file is comma-separated and lacks headers. The meanings of the columns are, in order: habitat type, site code where the sampling was done, morph color, and the number of individuals sampled.\n\nLoad this file into a tibble in R, and give the following names to its columns: habitat, site, morph, and number.\nExtract the last column (number) as a vector and calculate its mean and variance.\n\nSmith et al. (2003) compiled a database of the body masses of mammals of the late Quaternary period. The data are in the file Smith2003_data.txt. The column names are not specified in the file, but they are, in order: Continent (AF=Africa, etc.), Status (extinct, historical, introduction, or extant), Order, Family, Genus, Species, Base-10 Log Mass, Combined Mass (grams), and Reference (numbers, referring to a numerically ordered list of published works—no need to worry about the details).\n\nWhat is the delimiter in this data file?\nLoad the data and give appropriate names to its columns, based on the information above.\n\n\n\n\n\n\nBarabás, György, Christine Parent, Andrew Kraemer, Frederik Van de Perre, and Frederik De Laender. 2022. “The evolution of trait variance creates a tension between species diversity and functional diversity.” Nature Communications 13 (2521): 1–10. https://doi.org/10.1038/s41467-022-30090-4.\n\n\nGoldberg, Emma E., Joshua R. Kohn, Russell Lande, Kelly A. Robertson, Stephen A. Smith, and Boris Igić. 2010. “Species Selection Maintains Self-Incompatibility.” Science 330 (6003): 493–95. https://doi.org/10.1126/science.1194513.\n\n\nKraemer, Andrew C., C. W. Philip, A. M. Rankin, and C. E. Parent. 2018. “Trade-Offs Direct the Evolution of Coloration in Galápagos Land Snails.” Proceedings of the Royal Society B 286: 20182278.\n\n\nKraemer, Andrew C., Yannik E. Roell, Nate F. Shoobs, and Christine E. Parent. 2022. “Does island ontogeny dictate the accumulation of both species richness and functional diversity?” Global Ecology and Biogeography 31 (1): 123–37. https://doi.org/10.1111/geb.13420.\n\n\nParent, C. E., and B. J. Crespi. 2009. “Ecological Opportunity in Adaptive Radiation of Galápagos Endemic Land Snails.” American Naturalist 174: 898–905.\n\n\nSmith, Felisa A., S. Kathleen Lyons, S. K. Morgan Ernest, Kate E. Jones, Dawn M. Kaufman, Tamar Dayan, Pablo A. Marquet, James H. Brown, and John P. Haskell. 2003. “Body Mass of Late Quaternary Mammals.” Ecology 84 (12): 3403. https://doi.org/10.1890/02-9003.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading tabular data from disk</span>"
    ]
  },
  {
    "objectID": "04_Data_reading.html#footnotes",
    "href": "04_Data_reading.html#footnotes",
    "title": "4  Reading tabular data from disk",
    "section": "",
    "text": "If you are using a tidyverse version earlier than 2.0.0, only eight packages are loaded—lubridate, a package for working with dates, is missing from the list.↩︎\nThe data have been adapted from Barabás et al. (2022).↩︎\nThis and other data files used throughout the book will be accessible in a similar way. They are compressed in a zip file; extract them to obtain the data.↩︎\nWarning: there exists a similarly-named function called read.delim which is part of base R. It does much the same thing as read_delim; however, its use is clunkier and less flexible. You can think of read_delim as the tidyverse upgrade to the original read.delim. My recommendation is to stick with using just read_delim—it is simpler and at the same time more powerful than its predecessor.↩︎\nData frames (a feature of base R) and tibbles (a tidyverse construct) are equivalent for most practical purposes. Tibbles offer some features that are absent from data frames and omit quirks of data frames which tend to get in the way. Like with read_delim and read.delim, tibbles can be thought of as a slightly upgraded and more user-friendly version of data frames. You do not need to be overly concerned with the precise differences between the two. In this book we will be using tibbles almost exclusively.↩︎\nThe abbreviation &lt;dbl&gt; happens to stand for double-precision numerical value, a standard way of representing numbers on computers.↩︎",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reading tabular data from disk</span>"
    ]
  },
  {
    "objectID": "05_Basic_data_wrangling.html",
    "href": "05_Basic_data_wrangling.html",
    "title": "5  Basic data manipulation",
    "section": "",
    "text": "5.1 Important functions for transforming data\nLet us start by loading the tidyverse:\nlibrary(tidyverse)\nAs seen from the message output above, the dplyr package is part of the tidyverse and gets loaded by default. It allows one to arrange and manipulate data efficiently. Some of the functions manipulate the data based on its columns, and others do so based on its rows. The basic column-manipulating functions one should know are select, rename, and mutate. The row-manipulating ones are filter, slice, arrange, and distinct. Each of these functions work similarly in that:\nWe now explain each of these below. We will use the island-FL.csv data file we worked with in Chapter 4:\nsnails &lt;- read_delim(\"island-FL.csv\", delim = \",\")\nprint(snails)\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic data manipulation</span>"
    ]
  },
  {
    "objectID": "05_Basic_data_wrangling.html#important-functions-for-transforming-data",
    "href": "05_Basic_data_wrangling.html#important-functions-for-transforming-data",
    "title": "5  Basic data manipulation",
    "section": "",
    "text": "The first argument they take is the data, in the form of a tibble;\nTheir other arguments are given in terms of the column names of the data;\nThey return another tibble.\n\n\n\n\n5.1.1 select\nThis function chooses columns of the data. The second and subsequent arguments of the function are the columns which should be retained. For example, select(snails, species) will keep only the species column of snails:\n\nselect(snails, species)\n\n# A tibble: 223 × 1\n   species    \n   &lt;chr&gt;      \n 1 ustulatus  \n 2 ustulatus  \n 3 ustulatus  \n 4 calvus     \n 5 nux        \n 6 ustulatus  \n 7 ustulatus  \n 8 ustulatus  \n 9 galapaganus\n10 nux        \n# ℹ 213 more rows\n\n\nIt is also possible to deselect columns, by prepending an exclamation mark (!) in front of the column names. To drop the species column, we can type:\n\nselect(snails, !species)\n\n# A tibble: 223 × 3\n   habitat  size  shape\n   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid    17.1 -0.029\n 2 humid    20.1 -0.001\n 3 humid    16.3  0.014\n 4 arid     13.7 -0.043\n 5 humid    21.9 -0.042\n 6 humid    16.8 -0.023\n 7 humid    19.2  0.014\n 8 humid    16.0  0.042\n 9 arid     18.9  0.011\n10 humid    26.6  0    \n# ℹ 213 more rows\n\n\nNow we are left with only the columns habitat, size, and shape.\nThere are several other options within select, which mostly help with selecting several columns at a time fulfilling certain criteria. The collection of these options and methods is called tidy selection. First of all, it is possible to select a range of columns using the colon (:) operator. The following selection will choose the columns species and shape, and everything in between (which in this case is the column size):\n\nselect(snails, species:shape)\n\n# A tibble: 223 × 3\n   species      size  shape\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 ustulatus    17.1 -0.029\n 2 ustulatus    20.1 -0.001\n 3 ustulatus    16.3  0.014\n 4 calvus       13.7 -0.043\n 5 nux          21.9 -0.042\n 6 ustulatus    16.8 -0.023\n 7 ustulatus    19.2  0.014\n 8 ustulatus    16.0  0.042\n 9 galapaganus  18.9  0.011\n10 nux          26.6  0    \n# ℹ 213 more rows\n\n\nSecond, tidy selection allows one to specify, as character strings, what the column names should start or end with, using starts_with and ends_with:\n\nselect(snails, starts_with(\"s\"))\n\n# A tibble: 223 × 3\n   species      size  shape\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 ustulatus    17.1 -0.029\n 2 ustulatus    20.1 -0.001\n 3 ustulatus    16.3  0.014\n 4 calvus       13.7 -0.043\n 5 nux          21.9 -0.042\n 6 ustulatus    16.8 -0.023\n 7 ustulatus    19.2  0.014\n 8 ustulatus    16.0  0.042\n 9 galapaganus  18.9  0.011\n10 nux          26.6  0    \n# ℹ 213 more rows\n\nselect(snails, starts_with(\"sh\"))\n\n# A tibble: 223 × 1\n    shape\n    &lt;dbl&gt;\n 1 -0.029\n 2 -0.001\n 3  0.014\n 4 -0.043\n 5 -0.042\n 6 -0.023\n 7  0.014\n 8  0.042\n 9  0.011\n10  0    \n# ℹ 213 more rows\n\nselect(snails, ends_with(\"e\"))\n\n# A tibble: 223 × 2\n    size  shape\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1  17.1 -0.029\n 2  20.1 -0.001\n 3  16.3  0.014\n 4  13.7 -0.043\n 5  21.9 -0.042\n 6  16.8 -0.023\n 7  19.2  0.014\n 8  16.0  0.042\n 9  18.9  0.011\n10  26.6  0    \n# ℹ 213 more rows\n\nselect(snails, ends_with(\"pe\"))\n\n# A tibble: 223 × 1\n    shape\n    &lt;dbl&gt;\n 1 -0.029\n 2 -0.001\n 3  0.014\n 4 -0.043\n 5 -0.042\n 6 -0.023\n 7  0.014\n 8  0.042\n 9  0.011\n10  0    \n# ℹ 213 more rows\n\n\nSimilarly, the function contains can select columns which contain some string anywhere in their names. For example, select(snails, ends_with(\"pe\")) above only selected the shape column, but select(snails, contains(\"pe\")) additionally selects species:\n\nselect(snails, contains(\"pe\"))\n\n# A tibble: 223 × 2\n   species      shape\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 ustulatus   -0.029\n 2 ustulatus   -0.001\n 3 ustulatus    0.014\n 4 calvus      -0.043\n 5 nux         -0.042\n 6 ustulatus   -0.023\n 7 ustulatus    0.014\n 8 ustulatus    0.042\n 9 galapaganus  0.011\n10 nux          0    \n# ℹ 213 more rows\n\n\nThird, one can combine these selection methods using the & (“and”), | (“or”), and ! (“not”) logical operators. To select columns called either size or shape (i.e., selecting those two and no others):\n\nselect(snails, size | shape)\n\n# A tibble: 223 × 2\n    size  shape\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1  17.1 -0.029\n 2  20.1 -0.001\n 3  16.3  0.014\n 4  13.7 -0.043\n 5  21.9 -0.042\n 6  16.8 -0.023\n 7  19.2  0.014\n 8  16.0  0.042\n 9  18.9  0.011\n10  26.6  0    \n# ℹ 213 more rows\n\n\nTo select all columns which start with \"s\" but do not contain the letter \"z\" in their names:\n\nselect(snails, starts_with(\"s\") & !contains(\"z\"))\n\n# A tibble: 223 × 2\n   species      shape\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 ustulatus   -0.029\n 2 ustulatus   -0.001\n 3 ustulatus    0.014\n 4 calvus      -0.043\n 5 nux         -0.042\n 6 ustulatus   -0.023\n 7 ustulatus    0.014\n 8 ustulatus    0.042\n 9 galapaganus  0.011\n10 nux          0    \n# ℹ 213 more rows\n\n\nThe following selects columns that either contain \"ha\" in their names, or end with the letter \"s\":\n\nselect(snails, contains(\"ha\") | ends_with(\"s\"))\n\n# A tibble: 223 × 3\n   habitat  shape species    \n   &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;      \n 1 humid   -0.029 ustulatus  \n 2 humid   -0.001 ustulatus  \n 3 humid    0.014 ustulatus  \n 4 arid    -0.043 calvus     \n 5 humid   -0.042 nux        \n 6 humid   -0.023 ustulatus  \n 7 humid    0.014 ustulatus  \n 8 humid    0.042 ustulatus  \n 9 arid     0.011 galapaganus\n10 humid    0     nux        \n# ℹ 213 more rows\n\n\nOne can even combine range selection with the logical operations above. For instance, to select the range from habitat to species, as well as any columns whose name contains the letter \"z\":\n\nselect(snails, habitat:species | contains(\"z\"))\n\n# A tibble: 223 × 3\n   habitat species      size\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;\n 1 humid   ustulatus    17.1\n 2 humid   ustulatus    20.1\n 3 humid   ustulatus    16.3\n 4 arid    calvus       13.7\n 5 humid   nux          21.9\n 6 humid   ustulatus    16.8\n 7 humid   ustulatus    19.2\n 8 humid   ustulatus    16.0\n 9 arid    galapaganus  18.9\n10 humid   nux          26.6\n# ℹ 213 more rows\n\n\nFinally, there is a way to select all columns, with the everything helper function:\n\nselect(snails, everything())\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\n\n\n5.1.2 rename\nThe rename function simply gives new names to existing columns. The first argument, as always, is the tibble in which the column(s) should be renamed. The subsequent arguments follow the pattern new_name = old_name in replacing column names. For example, in the land snail data, the arid and humid habitats are often referred to as arid or humid zones. To rename habitat to zone, we simply write:\n\nrename(snails, zone = habitat)\n\n# A tibble: 223 × 4\n   zone  species      size  shape\n   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid ustulatus    17.1 -0.029\n 2 humid ustulatus    20.1 -0.001\n 3 humid ustulatus    16.3  0.014\n 4 arid  calvus       13.7 -0.043\n 5 humid nux          21.9 -0.042\n 6 humid ustulatus    16.8 -0.023\n 7 humid ustulatus    19.2  0.014\n 8 humid ustulatus    16.0  0.042\n 9 arid  galapaganus  18.9  0.011\n10 humid nux          26.6  0    \n# ℹ 213 more rows\n\n\nMultiple columns can also be renamed. To change all column names to start with capital letters:\n\nrename(snails,\n       Habitat = habitat, Species = species, Size = size, Shape = shape)\n\n# A tibble: 223 × 4\n   Habitat Species      Size  Shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\n\n\n5.1.3 mutate\nThe mutate function allows us to create new columns from existing ones. We may apply any function or operator we learned about to existing columns, and the result of the computation will go into the new column. We do this in the second argument of mutate (the first, as always, is the data tibble) by first giving a name to the column, then writing =, and then the desired computation. For example, we could create a new column indicating whether a snail is “large” (has a shell size above some threshold—say, 25) or “small”. We can do this using the ifelse function within mutate:\n\nmutate(snails, shellSize = ifelse(size &gt; 25, \"large\", \"small\"))\n\n# A tibble: 223 × 5\n   habitat species      size  shape shellSize\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    \n 1 humid   ustulatus    17.1 -0.029 small    \n 2 humid   ustulatus    20.1 -0.001 small    \n 3 humid   ustulatus    16.3  0.014 small    \n 4 arid    calvus       13.7 -0.043 small    \n 5 humid   nux          21.9 -0.042 small    \n 6 humid   ustulatus    16.8 -0.023 small    \n 7 humid   ustulatus    19.2  0.014 small    \n 8 humid   ustulatus    16.0  0.042 small    \n 9 arid    galapaganus  18.9  0.011 small    \n10 humid   nux          26.6  0     large    \n# ℹ 213 more rows\n\n\nThe original columns of the data are retained, but we now also have the additional shellSize column.\nOne very common transformation on quantities such as size and shape is to standardize them. There are various methods for standardization, but all of them entail shifting the data in some way, and making them unitless. One example for such standardization is to change the measurement scale such that all measured values fall in the range from 0 to 1. This can be achieved in two steps: first, we subtract off the minimum value in the data from every entry (this ensures that no point falls below 0), and then we divide each of these shifted values by the difference between the maximum and the minimum values (which squeezes the points between 0 and 1). Here is how one can perform this standardization with mutate:\n\nmutate(\n  snails,\n  stdSize  = (size  - min(size))  / (max(size)  - min(size)),\n  stdShape = (shape - min(shape)) / (max(shape) - min(shape))\n)\n\n# A tibble: 223 × 6\n   habitat species      size  shape stdSize stdShape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029  0.223     0.311\n 2 humid   ustulatus    20.1 -0.001  0.361     0.409\n 3 humid   ustulatus    16.3  0.014  0.188     0.462\n 4 arid    calvus       13.7 -0.043  0.0668    0.262\n 5 humid   nux          21.9 -0.042  0.447     0.266\n 6 humid   ustulatus    16.8 -0.023  0.212     0.332\n 7 humid   ustulatus    19.2  0.014  0.319     0.462\n 8 humid   ustulatus    16.0  0.042  0.173     0.559\n 9 arid    galapaganus  18.9  0.011  0.307     0.451\n10 humid   nux          26.6  0      0.665     0.413\n# ℹ 213 more rows\n\n\n\n\n5.1.4 filter\nWhile select chooses columns, filter chooses rows from the data. As with all these functions, the first argument of filter is the tibble to be filtered. The second argument is a logical condition on the columns. Those rows which satisfy the condition are retained; the rest are dropped. Thus, filter keeps only those rows of the data which fulfill some condition.\nFor example, to retain only those individuals from snails whose shell size is at least 29:\n\nfilter(snails, size &gt;= 29)\n\n# A tibble: 6 × 4\n  habitat species       size  shape\n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 humid   unifasciatus  33.8 -0.07 \n2 humid   unifasciatus  31.7 -0.115\n3 humid   unifasciatus  30.9 -0.074\n4 humid   unifasciatus  31.9 -0.071\n5 humid   nux           29.2 -0.01 \n6 humid   unifasciatus  32.0 -0.087\n\n\nThe filtered data have only 6 rows instead of the original 223—this is the number of snail individuals with a very large shell size. As seen, five of these belong in the species Naesiotus unifasciatus, and only one in the species Naesiotus nux.\n\n\n5.1.5 slice\nWith slice, one can choose rows of the data, just like with filter. Unlike with filter however, slice receives a vector of row indices to retain instead of a condition to be tested on each row. So, for example, if one wanted to keep only the first, second, and fifth rows, then one can do so with slice:\n\nslice(snails, c(1, 2, 5))\n\n# A tibble: 3 × 4\n  habitat species    size  shape\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 humid   ustulatus  17.1 -0.029\n2 humid   ustulatus  20.1 -0.001\n3 humid   nux        21.9 -0.042\n\n\n(Note: the numbers in front of the rows in the output generated by tibbles always pertain to the row numbers of the current table, not the one from which they were created. So the row labels 1, 2, 3 above simply enumerate the rows of the sliced data. The actual rows still correspond to rows 1, 2, and 5 in the original snails.)\n\n\n5.1.6 arrange\nThis function rearranges the rows of the data, in increasing order of the column given as the second argument. For example, to arrange in increasing order of size, we write:\n\narrange(snails, size)\n\n# A tibble: 223 × 4\n   habitat species  size  shape\n   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 arid    calvus   12.3 -0.019\n 2 arid    calvus   12.9 -0.039\n 3 arid    calvus   13.5 -0.012\n 4 arid    calvus   13.7 -0.018\n 5 arid    calvus   13.7 -0.043\n 6 arid    calvus   13.9  0.01 \n 7 arid    calvus   14.1 -0.027\n 8 arid    calvus   14.1 -0.016\n 9 arid    calvus   14.3 -0.011\n10 arid    calvus   14.4  0.002\n# ℹ 213 more rows\n\n\nTo arrange in decreasing order, there is a small helper function called desc. Arranging by desc(size) instead of size will arrange the rows in decreasing order of size:\n\narrange(snails, desc(size))\n\n# A tibble: 223 × 4\n   habitat species       size  shape\n   &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   unifasciatus  33.8 -0.07 \n 2 humid   unifasciatus  32.0 -0.087\n 3 humid   unifasciatus  31.9 -0.071\n 4 humid   unifasciatus  31.7 -0.115\n 5 humid   unifasciatus  30.9 -0.074\n 6 humid   nux           29.2 -0.01 \n 7 humid   unifasciatus  28.8 -0.075\n 8 humid   unifasciatus  28.5 -0.088\n 9 humid   nux           27.7 -0.05 \n10 humid   unifasciatus  27.7 -0.047\n# ℹ 213 more rows\n\n\nIt is also perfectly possible to arrange by a column whose type is character string. In that case, the system will sort the rows in alphabetical order—or reverse alphabetical order in case desc is applied. For example, to sort in alphabetical order of species names:\n\narrange(snails, species)\n\n# A tibble: 223 × 4\n   habitat species  size  shape\n   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 arid    calvus   13.7 -0.043\n 2 arid    calvus   17.9 -0.024\n 3 arid    calvus   13.9  0.01 \n 4 arid    calvus   16.5 -0.004\n 5 arid    calvus   16.6 -0.006\n 6 arid    calvus   16.1  0.01 \n 7 arid    calvus   18.2 -0.003\n 8 arid    calvus   12.9 -0.039\n 9 arid    calvus   17.3  0.002\n10 arid    calvus   14.1 -0.027\n# ℹ 213 more rows\n\n\nAnd to sort in reverse alphabetical order:\n\narrange(snails, desc(species))\n\n# A tibble: 223 × 4\n   habitat species    size  shape\n   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus  17.1 -0.029\n 2 humid   ustulatus  20.1 -0.001\n 3 humid   ustulatus  16.3  0.014\n 4 humid   ustulatus  16.8 -0.023\n 5 humid   ustulatus  19.2  0.014\n 6 humid   ustulatus  16.0  0.042\n 7 humid   ustulatus  15.4 -0.016\n 8 humid   ustulatus  16.3 -0.017\n 9 humid   ustulatus  16.7 -0.034\n10 humid   ustulatus  17.0 -0.018\n# ℹ 213 more rows\n\n\nNotice that when we sort the rows by species, there are many ties—rows with the same value of species. In those cases, arrange will not be able to decide which rows should come earlier, and so any ordering that was present before invoking arrange will be retained. In case we would like to break the ties, we can give further sorting variables, as the third, fourth, etc. arguments to arrange. To sort the data by species, and to resolve ties in order of increasing size, we write:\n\narrange(snails, species, size)\n\n# A tibble: 223 × 4\n   habitat species  size  shape\n   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 arid    calvus   12.3 -0.019\n 2 arid    calvus   12.9 -0.039\n 3 arid    calvus   13.5 -0.012\n 4 arid    calvus   13.7 -0.018\n 5 arid    calvus   13.7 -0.043\n 6 arid    calvus   13.9  0.01 \n 7 arid    calvus   14.1 -0.027\n 8 arid    calvus   14.1 -0.016\n 9 arid    calvus   14.3 -0.011\n10 arid    calvus   14.4  0.002\n# ℹ 213 more rows\n\n\nThis causes the table to be sorted primarily by species, but in case there are ties (equal species between multiple rows), they will be resolved in priority of size—first the smallest and then increasingly larger individuals.\n\n\n5.1.7 distinct\nWhile not as important as the previous six functions, distinct can also be useful. It takes as its input a tibble, and removes all rows that contain exact copies of any other row. For example, we might wonder how many different species there are in snails. One way to answer this is to select the species column only, and then apply distinct to remove duplicated entries:\n\ndistinct(select(snails, species))\n\n# A tibble: 7 × 1\n  species     \n  &lt;chr&gt;       \n1 ustulatus   \n2 calvus      \n3 nux         \n4 galapaganus \n5 unifasciatus\n6 invalidus   \n7 rugulosus   \n\n\nSo each individual in the data comes from one of the above seven species.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic data manipulation</span>"
    ]
  },
  {
    "objectID": "05_Basic_data_wrangling.html#using-pipes-to-our-advantage",
    "href": "05_Basic_data_wrangling.html#using-pipes-to-our-advantage",
    "title": "5  Basic data manipulation",
    "section": "5.2 Using pipes to our advantage",
    "text": "5.2 Using pipes to our advantage\nLet us take a slightly more complicated version of the above task. We are still looking for all the species in the data, but we want to display their names with the genus name Naesiotus added, and then sort the names in alphabetical order. We can break down the solution into the following steps that should be executed in sequence:\n\nChoose the species column from the data and discard all the others. This can be done using select.\nRemove duplicate rows with distinct, so that each species appears only once.\nAdd \"Naesiotus\" to the name of each species, e.g., using the paste function (Section 2.2.2) within mutate.\nFinally, sort the names in alphabetical order with arrange.\n\nSince select, mutate, arrange, etc. are just ordinary functions, they do not “modify” data. They merely take a tibble as input (plus other arguments) and return another tibble. They do not do anything to the original input data. In order for R not to forget their result immediately after they are computed, they have to be stored in variables. So one way of implementing the solution might rely on repeated assignments, as below:\n\nsnailSpecies &lt;- select(snails, species)\nsnailUnique &lt;- distinct(snailSpecies)\nsnailNamesFull &lt;- mutate(snailUnique,\n                   species = paste(\"Naesiotus\", species))\nsnailNamesSorted &lt;- arrange(snailNamesFull, species)\nprint(snailNamesSorted)\n\n# A tibble: 7 × 1\n  species               \n  &lt;chr&gt;                 \n1 Naesiotus calvus      \n2 Naesiotus galapaganus \n3 Naesiotus invalidus   \n4 Naesiotus nux         \n5 Naesiotus rugulosus   \n6 Naesiotus unifasciatus\n7 Naesiotus ustulatus   \n\n\nAs seen, this solution works. However, it requires inventing arbitrary variable names at every step. For such a short example, this is not problematic, but doing the same for a long pipeline of dozens of steps could get confusing. One solution to this problem could be to not use different names, but instead keep overwriting snails at every step. But this can lead to other problems arising from the fact that the value of snails might not be what we expect at any given time. While the solution with repeated assignments is viable, there ought to be better options out there.\nAnother possible solution is to rely on function composition (Section 3.2). Applying repeated composition is straightforward—in principle. In practice, when composing many functions together, things can get unwieldy quite quickly. Let us see what such a solution looks like:\n\narrange(\n  mutate(\n    distinct(\n      select(snails, species)\n    ),\n    species = paste(\"Naesiotus\", species)\n  ),\n  species\n)\n\n# A tibble: 7 × 1\n  species               \n  &lt;chr&gt;                 \n1 Naesiotus calvus      \n2 Naesiotus galapaganus \n3 Naesiotus invalidus   \n4 Naesiotus nux         \n5 Naesiotus rugulosus   \n6 Naesiotus unifasciatus\n7 Naesiotus ustulatus   \n\n\nThe expression is highly unpleasant: to a human reader, it is not at all obvious what is happening above. It would be nice to clarify this workflow if possible.\nIt turns out that one can do this by making use of the pipe operator |&gt; from Section 3.3. As a reminder: for any function f and function argument x, f(x, y, ...) is the same as x |&gt; f(y, ...), where the ... denote potential further arguments to f. That is, the first argument of the function can be moved from the argument list to in front of the function, before the pipe symbol. The tidyverse functions take a tibble as their first argument and always return another tibble. This means that the use of pipes allow us to very conveniently chain together multiple steps of data analysis. In our case, we can rewrite the above (quite confusing) code block in a much more transparent way:\n\nsnails |&gt;\n  select(species) |&gt;\n  distinct() |&gt;\n  mutate(species = paste(\"Naesiotus\", species)) |&gt;\n  arrange(species)\n\nAgain, the pipe |&gt; should be pronounced then. We take the data, then we select one of its columns only, then we remove all duplicated entries in that column, then we modify the entries of that column to include the genus name, and then we rearrange the rows in alphabetical order. In performing these steps, each function both receives and returns data. Thus, by starting out with the original snails, we no longer need to write out the data argument of the functions explicitly. Instead, the pipe takes care of that automatically for us, making the functions receive as their first input the piped-in data, and in turn producing transformed data as their output—which becomes the input for the next function in line.\nIn fact, there is no need to even assign snails. The pipe can just as well start with read_delim to load the dataset:\n\nread_delim(\"island-FL.csv\", delim = \",\") |&gt;\n  select(species) |&gt;\n  distinct() |&gt;\n  mutate(species = paste(\"Naesiotus\", species)) |&gt;\n  arrange(species)\n\n# A tibble: 7 × 1\n  species               \n  &lt;chr&gt;                 \n1 Naesiotus calvus      \n2 Naesiotus galapaganus \n3 Naesiotus invalidus   \n4 Naesiotus nux         \n5 Naesiotus rugulosus   \n6 Naesiotus unifasciatus\n7 Naesiotus ustulatus",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic data manipulation</span>"
    ]
  },
  {
    "objectID": "05_Basic_data_wrangling.html#sec-wrangling-exercises",
    "href": "05_Basic_data_wrangling.html#sec-wrangling-exercises",
    "title": "5  Basic data manipulation",
    "section": "5.3 Exercises",
    "text": "5.3 Exercises\n\nThe Smith2003_data.txt dataset we worked with in Section 4.8 occasionally has the entry -999 in its last three columns. This stands for unavailable data. As discussed in Section 3.3, in R there is a built-in way of referring to such information: by setting a variable to NA. Modify these columns using mutate so that the entries which are equal to -999 are replaced with NA.\nAfter replacing -999 values with NA, remove all rows from the data which contain one or more NA values (hint: look up the function drop_na). How many rows are retained? And what was the original number of rows?\n\nThe iris dataset is a built-in table in R. It contains measurements of petal and sepal characteristics from three flower species belonging to the genus Iris (I. setosa, I. versicolor, and I. virginica). If you type iris in the console, you will see the dataset displayed. In solving the problems below, feel free to use the all-important dplyr cheat sheet.\n\nThe format of the data is not a tibble, but a data.frame. As mentioned in Chapter 4, the two are basically the same for practical purposes, though internally tibbles do offer some advantages. Convert the iris data frame into a tibble. (Hint: look up the as_tibble function.)\nVerify that there are indeed three distinct species in the data (hint: combine select and distinct in an appropriate way).\nSelect the columns containing petal and sepal length, and species identity.\nGet those rows of the data with petal length less than 4 cm, but sepal length greater than 4 cm.\nSort the data in reverse alphabetical order of species names, but within each species, sort them by increasing petal width. After you are done, add petal length as yet another sorting variable that breaks ties with respect to petal width. If all went well, your result should begin with the following rows:\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n6.0\n2.2\n5.0\n1.5\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n4.9\n2.5\n4.5\n1.7\nvirginica\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n\nCreate a new column called MeanLength. It should contain the sum of the petal and sepal length for each individual flower, divided by two (that is, the average of the petal and sepal length for each flower).\nPerform the operations from exercises 5-8 sequentially, in a single long function call, chaining the different functions together using pipes.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic data manipulation</span>"
    ]
  },
  {
    "objectID": "06_Summaries_normalization.html",
    "href": "06_Summaries_normalization.html",
    "title": "6  Summary statistics and tidy data",
    "section": "",
    "text": "6.1 Statistical concepts and terminology\nChapter 5 introduced the basics of manipulating data. Here we continue by learning how one can efficiently compute summary statistics over a dataset. Additionally, we will take a look at the concept of tidy data. The two are not independent: as we will see, tidy data admit to calculating summaries much more efficiently than non-tidy data.\nWhen given a dataset – such as the snail data in Section 4.2.2 – it is difficult or even impossible to look at the entire set and understand the information it provides. It is therefore important to be able to describe aspects of the data, such as the different variables and observations, through the use of statistical measurements.\nWe will be relying on the file pop_data.csv. Download the file and set your working directory to the folder where you saved it. This is a comma-separated file (CSV), so you can load it using read_delim, with the delimiter specified as the comma.\nAs usual, we first load the tidyverse package:\nAnd now we may use the tidyverse functionalities, such as read_delim:\nThe data we just loaded contain population densities of three species at two spatial patches (A and B) at various points in time, ranging from 1 to 50 in steps of 1:\nThis dataset is a sample from a larger population,1 and while it is not clear how the units in the sample (the different time points) have been selected, we can assume that they have been randomly selected from the population of all possible time points.\nIt would be impossible, if not very expensive, to collect data from all units in a population. This means that in practice we work with samples of data. We are still interested in making conclusions about the population, but these will be made from a smaller set of units that we can actually collect data from. The information we collect will depend on the units selected for the sample, and any two samples will contain different units if the sampling procedure is done with any kind of randomization.\nProperties or descriptive statistics calculated on the population are called parameters (parameter in singular), and these will have the same value because the population contains all units we are interested in. In the sample we instead calculate statistics (statistic in singular) which is an estimate of the population parameter. The value of a statistic will also be different depending on the sample drawn and this inherent randomness is a vital aspect of statistical inference which will be covered in a later chapter.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics and tidy data</span>"
    ]
  },
  {
    "objectID": "06_Summaries_normalization.html#statistical-concepts-and-terminology",
    "href": "06_Summaries_normalization.html#statistical-concepts-and-terminology",
    "title": "6  Summary statistics and tidy data",
    "section": "",
    "text": "6.1.1 Variable types\nThe pop dataset contains 5 variables of different types. A variable type is used to describe what type of information we can find within it and determines how we can further analyze the variable. We can separate variables into two main groups: qualitative and quantitative variables.\nQualitative variables are those describing categories—for example nationalities, sex, blood type, etc. In our pop data the variable patch is qualitative, as it describes a label used for patches of land. If we would choose to label the two patches as 1 and 2, the variable itself would still be qualitative as the numbers do not have a numeric meaning.\nQuantitative variables are those describing real numbers, for example height, weight, or final times in a 100 m race. The numbers represent real numeric values instead of just labels. There are two sub-types of quantitative variables; discrete and continuous. A discrete variable may only take on integer (i.e., whole number) values, or a limited amount of decimals. The number of siblings would be a discrete variable, as one cannot have 2.32451 siblings and cannot measure the value with an infinite amount of decimals. Continuous variables on the other hand can be measured with this amount of detail. An example would be a person’s height.\nIn our dataset the three species’ density variables (number of individuals per unit area) are considered continuous quantitative variables, as they represent a real numeric value that can be measured with infinite amount of decimals. Even though the loaded dataset contain values with only two decimals, this is only done for rounding purposes and does not prevent the variable itself to be measured with infinite amount of decimals.\n\n\n\n\n\n\nNote\n\n\n\nNote that we differentiate from the variable types used within programming in earlier chapters and types used to describe variables within statistics. Some terminology is similar but the biggest difference is that within programming we differentiate between various types of quantitative (numeric) variables based on the amount of information they store on the disk.\nA quick conversion between the two terminologies would be:\n\n\n\nType in R\nType in statistics\n\n\n\n\ncharacter\nqualitative\n\n\nfactor\nqualitative\n\n\nlogical\nqualitative\n\n\nnumeric\nquantitative (discrete or continuous)\n\n\ninteger\ndiscrete quantitative\n\n\n\n\n\n\n\n6.1.2 Variable scales\nIn addition to describing what values we can expect in a variable, we can also use a variable scale to get information how the values relate to one another. Both the type and scale of a variable are important aspects to define or learn before analyzing the variable to know which methods are suitable.\nQualitative variables can have one of two different scales: nominal and ordinal.\n\nThe nominal scale is defined by categories that cannot be ordered in any logical way. For instance it cannot be said that one patch comes “before” the other, or that it is “better” in any way.2\nThe ordinal scale is defined by categories that can be ordered. Sizes of clothes are a good example of categories that can be ordered in such a way that the size continually increases (S &lt; M &lt; L). However, we cannot define how much the difference actually is, or if the difference is the same between any two adjacent categories.3\n\nQuantitative variables can also have one of two different scales: interval and ratio.\n\nThe interval scale is an upgrade to the ordinal scale, where values are still ordered but additionally the difference between any two distinct values is also meaningful. An example would be temperature measured in Celsius degrees: the difference between any two temperature values is itself always meaningful. Note however that the actual values, by themselves, need not mean anything. For example, a temperature of 0°C does not mean that the temperature measured does not exist. In fact, one might as well define the zero point to be anywhere else, and it is purely by convention that it is set to the freezing point of water under normal atmospheric pressure.\nThe ratio scale does have a defined zero point, which creates the possibility for calculating ratios between values. We can say that the density of one species (0.2) can be twice the size of the density of another (0.1) because the value of 0 actually means that there is no presence of the species at all.\n\n\n\n\n\n\n\nImportant\n\n\n\nThere is only one instance where the scale of the variable is implemented in R; namely, when we create ordered factors following an ordinal scale (Section 8.5). Otherwise this information is not saved in an R object.\nWhat this means in practice is that it is very important to know which scale the variable follows when programming. Ignoring this makes it very easy to end up in situations where we calculate inappropriate values or use misleading visualizations.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics and tidy data</span>"
    ]
  },
  {
    "objectID": "06_Summaries_normalization.html#processing-the-data",
    "href": "06_Summaries_normalization.html#processing-the-data",
    "title": "6  Summary statistics and tidy data",
    "section": "6.2 Processing the data",
    "text": "6.2 Processing the data\nOne can now perform various manipulations on these data, by using the functions we have learned about in Chapter 5. For instance, because the three densities in the data are quantitative continuous variables following the ratio scale, we could create a new column called total which contains the total community density (sum of the three species’ population densities) at each point in time and each location:\n\nmutate(pop, total = species1 + species2 + species3)\n\n# A tibble: 100 × 6\n    time patch species1 species2 species3 total\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1     1 A         8.43     6.62    10.1   25.1\n 2     1 B        10.1      3.28     6.27  19.7\n 3     2 A         7.76     6.93    10.3   25  \n 4     2 B        10.1      3.04     6.07  19.2\n 5     3 A         7.09     7.24    10.5   24.8\n 6     3 B        10.1      2.8      5.82  18.7\n 7     4 A         6.49     7.54    10.6   24.7\n 8     4 B        10.1      2.56     5.57  18.2\n 9     5 A         5.99     7.83    10.7   24.5\n10     5 B        10.1      2.33     5.32  17.8\n# ℹ 90 more rows\n\n\nAs a reminder, this can be written equivalently using pipes as\n\npop |&gt; mutate(total = species1 + species2 + species3)\n\n# A tibble: 100 × 6\n    time patch species1 species2 species3 total\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1     1 A         8.43     6.62    10.1   25.1\n 2     1 B        10.1      3.28     6.27  19.7\n 3     2 A         7.76     6.93    10.3   25  \n 4     2 B        10.1      3.04     6.07  19.2\n 5     3 A         7.09     7.24    10.5   24.8\n 6     3 B        10.1      2.8      5.82  18.7\n 7     4 A         6.49     7.54    10.6   24.7\n 8     4 B        10.1      2.56     5.57  18.2\n 9     5 A         5.99     7.83    10.7   24.5\n10     5 B        10.1      2.33     5.32  17.8\n# ℹ 90 more rows",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics and tidy data</span>"
    ]
  },
  {
    "objectID": "06_Summaries_normalization.html#creating-summary-data",
    "href": "06_Summaries_normalization.html#creating-summary-data",
    "title": "6  Summary statistics and tidy data",
    "section": "6.3 Creating summary data",
    "text": "6.3 Creating summary data\nThere are two main ways to summarize a variable; either visualize its distribution (as we will see in Chapter 7 and Section 7.3.2), or present various descriptive statistics that provide information about the variable.\n\n6.3.1 Measures of center\nA simple way to summarize the position of a variable is a measure of center. As the name implies, it describes where on the unit scale the values are centered around, and gives an indication of the magnitude, or level, of the values.\n\n6.3.1.1 Mean\nThe most common measure of center is the mean, which can be calculated on a continuous variable. Assume that we have n samples from a population, and the measurement on the i:th unit is denoted \\(x_i\\). The mean of the sample is then calculated as \\[\n\\bar{x} = \\frac{\\sum_{i = 1}^n{x_i}}{n}\n\\tag{6.1}\\] where \\(\\bar{x}\\) (pronounced “x-bar”) is the statistic that aims to estimate the population mean \\(\\mu\\) (the Greek letter mu).\n\n\n\n\n\n\nWarning\n\n\n\nEven though it is mathematically possible to calculate a mean of a discrete variable, the resulting value would not be an actual value of the variable. For instance the mean number of siblings of a person might turn out to be 1.6, but no person will have 1 full plus another 6/10 sibling.\n\n\nTo calculate the density of each of the different species in our dataset, all we need to do is use the mean function within a summarize function:\n\npop |&gt; \n  summarize(mean_sp1 = mean(species1))\n\n# A tibble: 1 × 1\n  mean_sp1\n     &lt;dbl&gt;\n1     5.30\n\n\nHere mean_sp1 is the name of the new column to be created, and the mean function is our summary function, collapsing the data into a single number.\n\nGrouping\nSo far, this is not particularly interesting; in fact, the exact same effect would have been achieved by typing the shorter mean(pop$species1) instead. The real power of summarize comes through when combined with group_by. This groups the data based on the given grouping variables. Let us see how this works in practice:\n\npop |&gt; \n  group_by(patch)\n\n# A tibble: 100 × 5\n# Groups:   patch [2]\n    time patch species1 species2 species3\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 A         8.43     6.62    10.1 \n 2     1 B        10.1      3.28     6.27\n 3     2 A         7.76     6.93    10.3 \n 4     2 B        10.1      3.04     6.07\n 5     3 A         7.09     7.24    10.5 \n 6     3 B        10.1      2.8      5.82\n 7     4 A         6.49     7.54    10.6 \n 8     4 B        10.1      2.56     5.57\n 9     5 A         5.99     7.83    10.7 \n10     5 B        10.1      2.33     5.32\n# ℹ 90 more rows\n\n\nSeemingly nothing has happened; the only difference is the extra line of comment above, before the printed table, saying Groups: patch [2]. What this means is that the rows of the data were internally split into two groups. The first have \"A\" as their patch, and the second have \"B\". Whenever one groups data using group_by, rows which share the same unique combination of the grouping variables now belong together, and subsequent operations will act separately on each group instead of acting on the table as a whole (which is what we have been doing so far). That is, group_by does not actually alter the data; it only alters the behavior of the functions applied to the grouped data.\nIf we group not just by patch but also by time, the comment above the table will read Groups: patch, time [100]:\n\npop |&gt; \n  group_by(patch, time)\n\n# A tibble: 100 × 5\n# Groups:   patch, time [100]\n    time patch species1 species2 species3\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 A         8.43     6.62    10.1 \n 2     1 B        10.1      3.28     6.27\n 3     2 A         7.76     6.93    10.3 \n 4     2 B        10.1      3.04     6.07\n 5     3 A         7.09     7.24    10.5 \n 6     3 B        10.1      2.8      5.82\n 7     4 A         6.49     7.54    10.6 \n 8     4 B        10.1      2.56     5.57\n 9     5 A         5.99     7.83    10.7 \n10     5 B        10.1      2.33     5.32\n# ℹ 90 more rows\n\n\nThis is because there are 100 unique combinations of patch and time: two different patch values (\"A\" and \"B\"), and fifty points in time (1, 2, …, 50). So we have “patch A, time 1” as group 1, “patch B, time 1” as group 2, “patch A, time 2” as group 3, and so on until “patch B, time 50” as our group 100.\nAs mentioned, functions that are applied to grouped data will act on the groups separately. To return to the example of calculating the mean population density of species 1 in the two patches, we can write:\n\npop |&gt;\n  group_by(patch) |&gt;\n  summarize(mean_sp1 = mean(species1))\n\n# A tibble: 2 × 2\n  patch mean_sp1\n  &lt;chr&gt;    &lt;dbl&gt;\n1 A         5.29\n2 B         5.32\n\n\nOne may obtain multiple summary statistics within the same summarize function. Below we compute the mean, the minimum, and the maximum of the densities per patch:\n\npop |&gt;\n  group_by(patch) |&gt;\n  summarize(mean_sp1 = mean(species1),\n            min_sp1 = min(species1),\n            max_sp1 = max(species1))\n\n# A tibble: 2 × 4\n  patch mean_sp1 min_sp1 max_sp1\n  &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 A         5.29    4.26    8.43\n2 B         5.32    0.47   10.1 \n\n\nLet us see what happens if we calculate the mean density of species 1—but grouping by time instead of patch:\n\npop |&gt;\n  group_by(time) |&gt;\n  summarize(mean_sp1 = mean(species1))\n\n# A tibble: 50 × 2\n    time mean_sp1\n   &lt;dbl&gt;    &lt;dbl&gt;\n 1     1     9.28\n 2     2     8.94\n 3     3     8.60\n 4     4     8.3 \n 5     5     8.04\n 6     6     7.84\n 7     7     7.68\n 8     8     7.54\n 9     9     7.44\n10    10     7.35\n# ℹ 40 more rows\n\n\nThe resulting table has 50 rows—half the number of rows in the original data, but many more than the two rows we get after grouping by patch. The reason is that there are 50 unique time points, and so the average is now computed over those rows which share time. But there are only two rows per moment of time: the rows corresponding to patch A and patch B. When we call summarize after having grouped by time, the averages are computed over the densities in these two rows only, per group. That is why here we end up with a table which has a single row per point in time.\n\n\n\n\n\n\nWarning\n\n\n\nOne common mistake when first encountering grouping and summaries is to assume that if we call group_by(patch), then the subsequent summaries will be taken over patches. This is not the case, and it is important to take a moment to understand why. When we apply group_by(patch), we are telling R to treat different patch values as group indicators. Therefore, when creating a summary, only the patch identities are retained from the original data, to which the newly calculated summary statistics are added. This means that the subsequent summaries are taken over everything except the patches. This should be clear after comparing the outputs of\n\npop |&gt; \n  group_by(patch) |&gt; \n  summarize(mean_sp1 = mean(species1))\n\nand\n\npop |&gt; \n  group_by(time) |&gt; \n  summarize(mean_sp1 = mean(species1))\n\nThe first distinguishes the rows of the data only by patch, and therefore the average is taken over time. The second distinguishes the rows by time, so the average is taken over the patches. Run the two expressions again to see the difference between them!\n\n\nWe can use functions such as mutate or filter on grouped data. For example, we might want to know the difference of species 1’s density from its average in each patch. Doing the following does not quite do what we want:\n\npop |&gt; mutate(diff_sp1 = species1 - mean(species1))\n\n# A tibble: 100 × 6\n    time patch species1 species2 species3 diff_sp1\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 A         8.43     6.62    10.1     3.13 \n 2     1 B        10.1      3.28     6.27    4.83 \n 3     2 A         7.76     6.93    10.3     2.46 \n 4     2 B        10.1      3.04     6.07    4.82 \n 5     3 A         7.09     7.24    10.5     1.79 \n 6     3 B        10.1      2.8      5.82    4.82 \n 7     4 A         6.49     7.54    10.6     1.19 \n 8     4 B        10.1      2.56     5.57    4.81 \n 9     5 A         5.99     7.83    10.7     0.685\n10     5 B        10.1      2.33     5.32    4.80 \n# ℹ 90 more rows\n\n\nThis will put the difference of species 1’s density from its mean density across both time and patches into the new column diff_sp1. That is not the same as calculating the difference from the mean in a given patch—patch A for rows corresponding to patch A, and patch B for the others. To achieve this, all one needs to do is to group the data by patch before invoking mutate:\n\npop |&gt;\n  group_by(patch) |&gt;\n  mutate(diff_sp1 = species1 - mean(species1))\n\n# A tibble: 100 × 6\n# Groups:   patch [2]\n    time patch species1 species2 species3 diff_sp1\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 A         8.43     6.62    10.1     3.14 \n 2     1 B        10.1      3.28     6.27    4.81 \n 3     2 A         7.76     6.93    10.3     2.47 \n 4     2 B        10.1      3.04     6.07    4.80 \n 5     3 A         7.09     7.24    10.5     1.80 \n 6     3 B        10.1      2.8      5.82    4.80 \n 7     4 A         6.49     7.54    10.6     1.20 \n 8     4 B        10.1      2.56     5.57    4.79 \n 9     5 A         5.99     7.83    10.7     0.702\n10     5 B        10.1      2.33     5.32    4.78 \n# ℹ 90 more rows\n\n\nComparing this with the previous table, we see that the values in the diff_sp1 column are now different, because this time the differences are taken with respect to the average densities per each patch.\nFinally, since group_by changes subsequent behavior, we eventually want to get rid of the grouping in our data. This can be done with the function ungroup. For example:\n\npop |&gt;\n  group_by(patch) |&gt;\n  mutate(diff_sp1 = species1 - mean(species1)) |&gt;\n  ungroup()\n\n# A tibble: 100 × 6\n    time patch species1 species2 species3 diff_sp1\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 A         8.43     6.62    10.1     3.14 \n 2     1 B        10.1      3.28     6.27    4.81 \n 3     2 A         7.76     6.93    10.3     2.47 \n 4     2 B        10.1      3.04     6.07    4.80 \n 5     3 A         7.09     7.24    10.5     1.80 \n 6     3 B        10.1      2.8      5.82    4.80 \n 7     4 A         6.49     7.54    10.6     1.20 \n 8     4 B        10.1      2.56     5.57    4.79 \n 9     5 A         5.99     7.83    10.7     0.702\n10     5 B        10.1      2.33     5.32    4.78 \n# ℹ 90 more rows\n\n\nIt is good practice to always ungroup the data after we have calculated what we wanted using the group structure. Otherwise, subsequent calculations could be influenced by the grouping in unexpected ways.\n\n\n\n6.3.1.2 Median\nOne instance when the mean misrepresents a variable’s center is if there are outliers present in the data. An outlier is an observation that is located far away from the majority of the other observations. These observations affect the mean by moving the measure towards the direction of the outliers and thereby shifting the center away from the majority of the observations. This can be identified if we visualize the distribution alongside the mean but we can also use another measure of center as an alternative.\nThe median is much more robust against outliers than the mean, in the sense that it is less affected by them. It describes the middle observation if we were to order the values in increasing size. Any single extremely small or large value would not affect position of the middle observation, as long as there are sufficiently many data points.\nWe can calculate the position — i.e., the index of the value in a vector or the row in a dataset — of the median as (n + 1) / 2, where n is the number of data points. This results in either a whole number (if n is odd), in which case the value at that position is the median. Or it could result in a half number (if n is even), in which case the convention is to put the median halfway between the two positions obtained by rounding the position’s number up and down, respectively.\nThe process of calculating the median can be done step by step as follows:\n\n# The number of observations is the number of rows in the data:\nn &lt;- nrow(pop)\n\n# Find the position (index) of the median:\nposition &lt;- (n + 1) / 2 # The position is 50.5\n\n# Therefore we need to round down and up, to get the vector of positions c(50, 51):\npositionVec &lt;- c(floor(position), ceiling(position))\n\n# First we order the variable\npop |&gt;\n  select(species1) |&gt; # Keep only this one column\n  arrange(species1) # Order the densities from lowest to largest\n\n# A tibble: 100 × 1\n   species1\n      &lt;dbl&gt;\n 1     0.47\n 2     0.55\n 3     0.64\n 4     0.74\n 5     0.84\n 6     0.96\n 7     1.08\n 8     1.22\n 9     1.36\n10     1.51\n# ℹ 90 more rows\n\n# Next, we extract the observations located at the position vector\npop |&gt;\n  select(species1) |&gt; \n  arrange(species1) |&gt; \n  slice(positionVec) # Get the middle two values (at rows 50 and 51)\n\n# A tibble: 2 × 1\n  species1\n     &lt;dbl&gt;\n1     5.27\n2     5.29\n\n# Finally we calculate the mean of the two observations\npop |&gt;\n  select(species1) |&gt; \n  arrange(species1) |&gt; \n  slice(positionVec) |&gt; \n  summarize(median = mean(species1)) # Obtain their mean, which is the median of the variable\n\n# A tibble: 1 × 1\n  median\n   &lt;dbl&gt;\n1   5.28\n\n\nThis becomes tedious to do multiple times, but thankfully the function median does all of this for us all in one go.\n\npop |&gt; \n  summarize(median = median(species1))\n\n# A tibble: 1 × 1\n  median\n   &lt;dbl&gt;\n1   5.28\n\n\nUsing the function, the median is also 5.28.4\n\n\n6.3.1.3 Quantiles\nA generalization of the median is a quantile which aims to divide the ordered values into a specific number of parts. For example, we can consider the median a quantile where the data are split up into two equally-sized parts. Other common quantiles are quartiles which divide the data into four equal quarters, each containing 25% of the data. The first quartile (which splits the data into 25% | 75%) and the third quartile (splits 75% | 25%) can be used as additional measures to gain information about different positions.\n\n\n\n\n\n\nNote\n\n\n\nThe second quartile is actually just the median as it splits the data into two equally sized parts, 50% | 50%, the same as we defined in the median.\n\n\nThe function quantile() is used to calculate specific quantiles of a variable via the argument probs that can be given one or more numeric values. If we want to calculate the three quartiles we would give the values c(0.25, 0.50, 0.75).\n\npop |&gt; \n  summarize(quartile = quantile(species1, probs = c(0.25, 0.50, 0.75)) |&gt; t())\n\n# A tibble: 1 × 1\n  quartile[,\"25%\"] [,\"50%\"] [,\"75%\"]\n             &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1             3.72     5.28     6.04\n\n\nThe function t() is used to rotate the vector of quantiles from quantile() to one row instead of a column. With this the names of the columns in the result show the value of the first (25%), second (50%) and third (75%) quartile.\n\n\n\n6.3.2 Measures of spread or uncertainty\nThe measure of a center by itself does not provide the full picture of a variable. There is a joke saying that “if a statistician has one hand in a freezer and the other on a hot plate, they are on average comfortable”. When summarizing a variable, it is also important to describe the spread of the values, the amount of variation they have. This can give a sense of how two variables with the same mean might differ.\n\n6.3.2.1 Standard deviation\nThe standard deviation can be seen as the “average distance from the mean”—that is, how far away from the mean we expect a randomly selected value to be. The formula for the standard deviation is \\[\ns = \\sqrt{\\frac{\\sum_{i = 1}^n{(x_i - \\bar{x})^2}}{n - 1}}\n\\tag{6.2}\\] where \\(x_i\\) is the i:th observed value, \\(\\bar{x}\\) is the mean, and \\(n\\) is the number of observations.\nWe can get an understanding of why the standard deviation is seen as an average by going through each step of the formula using algebraic rules. First we calculate the part inside the parenthesis, the difference between the observed value and the mean, where values further from the mean result in a bigger difference (negative or positive):\n\npop |&gt; \n  mutate(diff = species1 - mean(species1))\n\n# A tibble: 100 × 6\n    time patch species1 species2 species3  diff\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1     1 A         8.43     6.62    10.1  3.13 \n 2     1 B        10.1      3.28     6.27 4.83 \n 3     2 A         7.76     6.93    10.3  2.46 \n 4     2 B        10.1      3.04     6.07 4.82 \n 5     3 A         7.09     7.24    10.5  1.79 \n 6     3 B        10.1      2.8      5.82 4.82 \n 7     4 A         6.49     7.54    10.6  1.19 \n 8     4 B        10.1      2.56     5.57 4.81 \n 9     5 A         5.99     7.83    10.7  0.685\n10     5 B        10.1      2.33     5.32 4.80 \n# ℹ 90 more rows\n\n\nWhen calculating the “average distance”, we need all values to be positive (as you cannot have a negative distance). So we square all the values, making them all positive:\n\npop |&gt; \n  mutate(diff = (species1 - mean(species1))^2)\n\n# A tibble: 100 × 6\n    time patch species1 species2 species3   diff\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1     1 A         8.43     6.62    10.1   9.77 \n 2     1 B        10.1      3.28     6.27 23.3  \n 3     2 A         7.76     6.93    10.3   6.03 \n 4     2 B        10.1      3.04     6.07 23.2  \n 5     3 A         7.09     7.24    10.5   3.19 \n 6     3 B        10.1      2.8      5.82 23.2  \n 7     4 A         6.49     7.54    10.6   1.41 \n 8     4 B        10.1      2.56     5.57 23.1  \n 9     5 A         5.99     7.83    10.7   0.470\n10     5 B        10.1      2.33     5.32 23.0  \n# ℹ 90 more rows\n\n\nNext, we sum all the squared differences so we get a sense of the total (squared) distance from every observation to the mean:\n\npop |&gt; \n  summarize(diff = (species1 - mean(species1))^2 |&gt; \n              sum())\n\n# A tibble: 1 × 1\n   diff\n  &lt;dbl&gt;\n1  746.\n\n\nTo get an average (squared) distance, we then divide by the number of observations or at least something that depends on the number of observations, \\(n - 1\\).5\n\npop |&gt; \n  summarize(diff = (species1 - mean(species1))^2 |&gt; \n              sum() / (length(species1) - 1))\n\n# A tibble: 1 × 1\n   diff\n  &lt;dbl&gt;\n1  7.54\n\n\nThis value can be read as an average of the squared distances, which is not simple to actually interpret.6 In order to make it interpretable, we need to take the square root of the value:\n\npop |&gt; \n  summarize(sd = ((species1 - mean(species1))^2 |&gt; \n              sum() / (length(species1) - 1)) |&gt; \n              sqrt())\n\n# A tibble: 1 × 1\n     sd\n  &lt;dbl&gt;\n1  2.75\n\n\nInstead of squared distances, we now have a representation of the “normal” distances between each observation and its mean. Thankfully, we do not need to go through all of these steps every time we want to calculate the standard deviation, because we can just use the built-in function sd:\n\npop |&gt; \n  summarize(sd = sd(species1))\n\n# A tibble: 1 × 1\n     sd\n  &lt;dbl&gt;\n1  2.75\n\n\n\n\n6.3.2.2 Interquartile range\nSimilarly to the mean, the standard deviation will overestimate the spread of the variable if there are outliers present. Since outliers are defined as observations far from the rest of the observations, they will also be far from the mean, thereby increasing the average distance from the mean and resulting in an overestimation. Instead, we can use the difference between the first and third quartile as an indication of the spread of the variable. This is called the interquartile range (IQR). For instance, the IQR for Species 1 is 2.3225 using the values found in Section 6.3.1.3.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics and tidy data</span>"
    ]
  },
  {
    "objectID": "06_Summaries_normalization.html#sec-tidy",
    "href": "06_Summaries_normalization.html#sec-tidy",
    "title": "6  Summary statistics and tidy data",
    "section": "6.4 Tidy data",
    "text": "6.4 Tidy data\nIn science, we often strive to work with tidy data. A dataset is called tidy if:\n\nEach observation is a row; each row is an observation.\nEach variable is a column; each column is a variable.\nEach value is a cell; each cell is a single value.7\n\nTidy data are suitable for performing operations, statistics, and plotting on. Furthermore, tidy data have a certain well-groomed feel to them, in the sense that their organization always follows the same general pattern regardless of the type of dataset one studies. Paraphrasing Tolstoy: tidy data are all alike; by contrast, every non-tidy dataset tends to be messy in its own unique way (Wickham 2014).\nThe tidyverse offers a simple and convenient way of putting data in tidy format. The pop table from the previous section is not tidy, because although each variable is in its own column, it is not true that each observation is in its own row. Instead, each row contains three observations: the densities of species 1, 2, and 3 at a given time and place. To tidy up these data, we create key-value pairs. We merge the columns for species densities into just two new ones. The first of these (the key) indicates whether it is species 1, or 2, or 3 which the given row refers to. The second column (the value) contains the population density of the given species. Such key-value pairs are created by the function pivot_longer:\n\npop |&gt;\n  pivot_longer(cols = species1 | species2 | species3,\n               names_to = \"species\",\n               values_to = \"density\")\n\n# A tibble: 300 × 4\n    time patch species  density\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1     1 A     species1    8.43\n 2     1 A     species2    6.62\n 3     1 A     species3   10.1 \n 4     1 B     species1   10.1 \n 5     1 B     species2    3.28\n 6     1 B     species3    6.27\n 7     2 A     species1    7.76\n 8     2 A     species2    6.93\n 9     2 A     species3   10.3 \n10     2 B     species1   10.1 \n# ℹ 290 more rows\n\n\nThe function pivot_longer takes three arguments in addition to the first (data) argument that we may pipe in, like above. First, cols is the set of columns to be converted into key-value pairs. It uses the same tidy selection mechanisms as the function select; see Section 5.1.1. (This means that cols = starts_with(\"species\") could also have been used.) Second, the argument names_to is the name of the new key column, specified as a character string. And third, values_to is the name of the new value column, also as a character string.\nThe above table is now in tidy format: each column records a single variable, and each row contains a single observation. Notice that, unlike the original pop which had 100 rows and 5 columns, the tidy version has 300 rows and 4 columns. This is natural: since the number of columns was reduced, there must be some extra rows to prevent the loss of information. And one should notice another benefit to casting the data in tidy format: it forces one to explicitly specify what was measured. By having named the value column density, we now know that the numbers 8.43, 6.62, etc. are density measurements. By contrast, it is not immediately obvious what these same numbers mean under the columns species1, species2, … in the original data.\nIt is possible to undo the effect pivot_longer. To do so, use pivot_wider:\n\npop |&gt;\n  pivot_longer(cols = starts_with(\"species\"),\n               names_to = \"species\",\n               values_to = \"density\") |&gt;\n  pivot_wider(names_from = \"species\", values_from = \"density\")\n\n# A tibble: 100 × 5\n    time patch species1 species2 species3\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 A         8.43     6.62    10.1 \n 2     1 B        10.1      3.28     6.27\n 3     2 A         7.76     6.93    10.3 \n 4     2 B        10.1      3.04     6.07\n 5     3 A         7.09     7.24    10.5 \n 6     3 B        10.1      2.8      5.82\n 7     4 A         6.49     7.54    10.6 \n 8     4 B        10.1      2.56     5.57\n 9     5 A         5.99     7.83    10.7 \n10     5 B        10.1      2.33     5.32\n# ℹ 90 more rows\n\n\nThe two named arguments of pivot_wider above are names_from (which specifies the column from which the names for the new columns will be taken), and values_from (the column whose values will be used to fill in the rows under those new columns).\nAs a remark, one could make the data even “wider”, by not only making columns out of the population densities, but the densities at a given patch. Doing so is simple: one just needs to specify both the species and patch columns from which the new column names will be compiled.\n\npop |&gt;\n  pivot_longer(cols = starts_with(\"species\"),\n               names_to = \"species\",\n               values_to = \"density\") |&gt;\n  pivot_wider(names_from = c(\"species\", \"patch\"), values_from = \"density\")\n\n# A tibble: 50 × 7\n    time species1_A species2_A species3_A species1_B species2_B species3_B\n   &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1     1       8.43       6.62      10.1        10.1       3.28       6.27\n 2     2       7.76       6.93      10.3        10.1       3.04       6.07\n 3     3       7.09       7.24      10.5        10.1       2.8        5.82\n 4     4       6.49       7.54      10.6        10.1       2.56       5.57\n 5     5       5.99       7.83      10.7        10.1       2.33       5.32\n 6     6       5.58       8.1       10.7        10.1       2.12       5.08\n 7     7       5.27       8.34      10.6        10.1       1.92       4.86\n 8     8       5.02       8.54      10.4        10.1       1.74       4.64\n 9     9       4.82       8.7       10.0        10.0       1.58       4.43\n10    10       4.66       8.82       9.66       10.0       1.43       4.23\n# ℹ 40 more rows\n\n\nFinally, it is worth noting the power of tidy data in, e.g., generating summary statistics. To obtain the mean, minimum, and maximum of the population densities for each species in each patch, all one has to do is this:\n\npop |&gt;\n  # Tidy up the data:\n  pivot_longer(cols = starts_with(\"species\"),\n               names_to = \"species\",\n               values_to = \"density\") |&gt;\n  # Group data by both species and patch:\n  group_by(patch, species) |&gt;\n  # Obtain statistics:\n  summarize(mean_dens = mean(density),\n            min_dens = min(density),\n            max_dens = max(density)) |&gt;\n  # Don't forget to ungroup the data at the end:\n  ungroup()\n\n# A tibble: 6 × 5\n  patch species  mean_dens min_dens max_dens\n  &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 A     species1      5.29     4.26     8.43\n2 A     species2      8.05     6.62     8.98\n3 A     species3      7.51     6.38    10.7 \n4 B     species1      5.32     0.47    10.1 \n5 B     species2      1.07     0.36     3.28\n6 B     species3      6.57     3.44    11.4 \n\n\n\n6.4.1 Grouped summary statistics on tidy data\nWith the help of tidy data we can now summarize each of the three species with just a small addition to the code. group_by() and summarize() applied to the pivoted data allows us to calculate the same statistic over many different variables from the original data.\n\npop |&gt; \n  pivot_longer(cols = starts_with(\"species\"),\n               names_to = \"species\",\n               values_to = \"density\") |&gt; \n  group_by(species) |&gt; \n  summarize(mean = mean(density)) |&gt;\n  ungroup()\n\n# A tibble: 3 × 2\n  species   mean\n  &lt;chr&gt;    &lt;dbl&gt;\n1 species1  5.30\n2 species2  4.56\n3 species3  7.04\n\n\nEach mean now shows the position of the center for each variable (species), with Species 1 and 2 having closer measures of center than Species 3.\nTo compare the standard deviations of the three species in the pop dataset, we can use the same type of grouped calculation we have done before, but adding on additional summary statistics within summarize():\n\npop |&gt; \n  pivot_longer(cols = starts_with(\"species\"),\n               names_to = \"species\",\n               values_to = \"density\") |&gt; \n  group_by(species) |&gt; \n  summarize(mean = mean(density),\n            stdev = sd(density)) |&gt;\n  ungroup()\n\n# A tibble: 3 × 3\n  species   mean stdev\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 species1  5.30  2.75\n2 species2  4.56  3.57\n3 species3  7.04  2.11\n\n\nSpecies 2 does indeed have a higher standard deviation than both species 1 and 3, indicating that its observations are further away from the mean.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics and tidy data</span>"
    ]
  },
  {
    "objectID": "06_Summaries_normalization.html#exercises",
    "href": "06_Summaries_normalization.html#exercises",
    "title": "6  Summary statistics and tidy data",
    "section": "6.5 Exercises",
    "text": "6.5 Exercises\nThe first set of problems relies on the iris dataset—the same that we used in the previous chapter’s exercises (Section 5.3). Convert the iris data to a tibble with the as_tibble function, and assign it to a variable.\n\nCreate a new column in the iris dataset which contains the difference of petal lengths from the average petal length of all flowers in the entire dataset.\nCreate a new column in the iris dataset which contains the difference of petal lengths from the average petal length of each species. (Hint: group_by the species and then mutate!)\nCreate a table where the rows are the three species, and the columns are: average petal length, total range of petal length (difference between the largest and smallest values), average sepal length, and total range of sepal length. Each of these should be calculated across flowers of the corresponding species.\nCreate key-value pairs in the iris dataset for the petal characteristics. In other words, have a column called Petal.Trait (whose values are either Petal.Length or Petal.Width), and another column called Petal.Value (with the length/width values).\nRepeat the same exercise, but now for sepal traits.\nFinally, do it for both petal and sepal traits simultaneously, to obtain a fully tidy form of the iris data. That is, the key column (call it Flower.Trait) will have the values Petal.Length, Petal.Width, Sepal.Length, and Sepal.Width. And the value column (which you can call Trait.Value) will have the corresponding measurements.\n\nThe subsequent exercises use the Galápagos land snail data from the previous two chapters (see Section 4.2.2 to review the description of the data).\n\nWhat is the average shell size of the whole community? What is its total range (the difference between the largest and smallest values)? How about the average and total range of shell shape?\nWhat is the average shell size and the average shell shape of the community in each habitat type (humid/arid)?\nWhat is the average shell size and the average shell shape in each unique combination of species and habitat type?\nBased on your answer to the previous question: how many species are there which live in both arid and humid environments?\nOrganize the size and shape columns in key-value pairs: instead of the original size and shape, have a column called trait (which will either be \"size\" or \"shape\") and another column called value which holds the corresponding measurement.\n\n\n\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (September): 1–23. https://doi.org/10.18637/jss.v059.i10.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics and tidy data</span>"
    ]
  },
  {
    "objectID": "06_Summaries_normalization.html#footnotes",
    "href": "06_Summaries_normalization.html#footnotes",
    "title": "6  Summary statistics and tidy data",
    "section": "",
    "text": "“Population” in the statistical sense of the word is not to be confused with “population” from ecology. It simply means the complete set of all entities out there from which we take our smaller sample.↩︎\nNote that there might exist other information, such as the size of the patches, that can be used to order the categories, but then we are ordering based on another variable and not the categories themselves.↩︎\nA quantitative variable can be thought of as following an ordinal scale if the measurements are intervals—e.g., 0-4, 5-9, 10-19, etc.↩︎\nWe would expect the same value as the function uses the same process as shown earlier.↩︎\nWe will return to why we do not use \\(n\\) directly in a later chapter.↩︎\nThe squared distance is actually a component in more advanced calculations, and is called the variance of a variable.↩︎\nThe requirement that every cell should contain a single value might sound unnecessary to spell out. Are the entries of cells not single values by definition? As it happens, later on we will see examples of tables whose cells contain more complex information than single values. For example, tables might contain cells whose entries are themselves tables. While such nested tables are not in tidy format, they are a very powerful idea, and we will learn to harness their benefits in Chapter 22.↩︎",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary statistics and tidy data</span>"
    ]
  },
  {
    "objectID": "07_Creating_figures.html",
    "href": "07_Creating_figures.html",
    "title": "7  Creating publication-grade figures",
    "section": "",
    "text": "7.1 General philosophy\nIn science, we want clear and informative plots. Each figure should make it obvious what data you are plotting, what the axes, colors, shapes, and size differences represent, and the overall message the figure is conveying. When writing a scientific paper or report, remember that your future readers are busy people. They often do not have the time to delve into the subtleties of overly refined verbal arguments. Instead, they will most often look for the figures to learn what your work is about. You will want to create figures which makes this possible for them to do.\nHere we will learn how to create accessible, publication-quality scientific graphs in a simple way. We do this using the R package ggplot2 which is a standard part of the tidyverse. The ggplot2 package follows a very special philosophy for creating figures that was originally proposed by Leland Wilkinson (2006). The essence of this view is that, just like the grammar of sentences, graphs have fixed “grammatical” components whose specification defines the plot. The grand idea is that the data ought not be changed in order to display it in different formats. For instance, the same data should be possible to represent either as a box plot, or as a histogram, without changing their format.\nThis last claim needs to be qualified somewhat. It is more accurate to say that one should not need to change the data as long as they are in tidy format. As a reminder, “tidy data” means that every variable is in its own column, and every observation is in its own row (Section 6.4). In case the data are not tidy, one should first wrangle them into such form, for example by using pivot_longer. While this step is not always required (especially for simpler graphs), it can be very useful to tidy the data before analyzing and plotting them when working with larger, more complex datasets.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Creating publication-grade figures</span>"
    ]
  },
  {
    "objectID": "07_Creating_figures.html#basic-ggplot2-usage",
    "href": "07_Creating_figures.html#basic-ggplot2-usage",
    "title": "7  Creating publication-grade figures",
    "section": "7.2 Basic ggplot2 usage",
    "text": "7.2 Basic ggplot2 usage\nTo see how ggplot2 works, let us load tidyverse, and then use the Galápagos land snail dataset to create some figures. As a reminder, here is what the data look like:\n\nlibrary(tidyverse)\n\nsnails &lt;- read_delim(\"island-FL.csv\", delim = \",\")\nprint(snails)\n\n# A tibble: 223 × 4\n   habitat species      size  shape\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 humid   ustulatus    17.1 -0.029\n 2 humid   ustulatus    20.1 -0.001\n 3 humid   ustulatus    16.3  0.014\n 4 arid    calvus       13.7 -0.043\n 5 humid   nux          21.9 -0.042\n 6 humid   ustulatus    16.8 -0.023\n 7 humid   ustulatus    19.2  0.014\n 8 humid   ustulatus    16.0  0.042\n 9 arid    galapaganus  18.9  0.011\n10 humid   nux          26.6  0    \n# ℹ 213 more rows\n\n\nLet us, as a first step, create a plot where shell shape (y-axis) is plotted against shell size (x-axis), with the points referring to different habitats shown in different colors:\n\nggplot(\n  data = snails,\n  mapping = aes(x = size, y = shape, color = habitat)\n) +\n  geom_point()\n\n\n\n\n\n\n\n\nThe ggplot function takes two inputs: the data (in this case, the snails table) and the aesthetic mappings (mapping). On top of this, we add on the geometry of how to display the data—in this case, with geom_point().\nThe aesthetic mappings are always defined via the aes helper function. What are these “aesthetic mappings”? The important thing to remember is that the aesthetic mappings are all those aspects of the figure that are governed by the data. For instance, if you wanted to set the color of all points to blue, this would not be an aesthetic mapping, because it applies regardless of what the data are (in case you want to do this, you would have to specify geom_point(color = \"blue\") in the last line).\nThe geometry of your plot governs the overall visual arrangement of your data (points, lines, histograms, etc). There are many different geom_s; we will learn about some here, but when in doubt, Google and a ggplot2 cheat sheet are your best friends.\nThe above program is often written by piping the data into the ggplot function. Then, since the mapping argument is always given via the aes function, one usually omits naming this argument:\n\nsnails |&gt;\n  ggplot(aes(x = size, y = shape, color = habitat)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis is the most common form we will write a function call to ggplot.\nNotice that the different “grammatical” components are added to the plot, using the + symbol for addition. A common source of error is to accidentally keep using the pipe operator |&gt; even within a plot. The rule of thumb is that after invoking ggplot, one must use + to compose the various graph elements, but outside of that, the usual |&gt; is used for function composition. If one uses |&gt; instead of + within a plot, R will give back an error message instead of graphics:\n\nsnails |&gt;\n  ggplot(aes(x = size, y = shape, color = habitat)) |&gt;\n  geom_point()\n\nError in `geom_point()`:\n! `mapping` must be created by `aes()`.\n✖ You've supplied a &lt;ggplot2::ggplot&gt; object.\nℹ Did you use `%&gt;%` or `|&gt;` instead of `+`?\n\n\nJust be sure to remember this so you can correct the mistake, should you accidentally run up against it.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Creating publication-grade figures</span>"
    ]
  },
  {
    "objectID": "07_Creating_figures.html#some-common-geometries",
    "href": "07_Creating_figures.html#some-common-geometries",
    "title": "7  Creating publication-grade figures",
    "section": "7.3 Some common geometries",
    "text": "7.3 Some common geometries\n\n7.3.1 Scatter plots with geom_point\nThe plot above, created using geom_point, is called a scatter plot: each row in the data is represented by a point whose x- and y-coordinates come from different columns in the data (in this case, size and shape). The scatter plot above immediately revealed some interesting facts about the data. For example, the largest shell sizes are found in the humid habitats, whereas snails with the largest shell shape parameters (indicating long and slender shells) are found only in the arid regions. This is exactly the kind of information that would have been difficult to get just from the raw data in its tabular form, but is immediately seen from an appropriate graph.\n\n\n7.3.2 Distribution of a variable\nThe distribution of a qualitative variable should be visualized using a bar plot. If the variable follows the ordinal scale, the categories on the x-axis should follow the same order. If the variable follows the nominal scale we do not have to follow any specific order of the categories, but it is usually nice to order them either alphabetically or in descending order based on the (relative) frequency of the category.\n\nsnails |&gt;\n  ggplot(aes(x = species)) +\n  geom_bar(fill = \"steelblue\") \n\n\n\n\n\n\n\n\nWhen visualizing the distribution, it is customary to use the relative frequency (%) of each category instead of their absolute frequency (count). This can be done directly in the ggplot-process without the need for additional data processing prior to the visualization. In this case, the default calculation done is counting the number of occurrences of each category and the height (y) of the bars being defined by the count. The after_stat() function allows for calculations after the counting has been completed. for example, count / sum(count) would calculate the relative frequency of each category compared to the total number of observations. If we multiply this value by 100, we can show the percent of observations of each category.\n\nsnails |&gt;\n  # The after_stat below changes the calculation of the y-axis to\n  # (count / sum(count)) * 100, instead of the default, which is count\n  ggplot(aes(x = species, y = after_stat(count / sum(count)) * 100)) +\n  geom_bar(fill = \"steelblue\") +\n  # Fix the y-axis to display percentages:\n  scale_y_continuous(name = \"percent\")\n\n\n\n\n\n\n\n\nA quantitative variable can be visualized in two different ways depending on the variable type. A discrete variable, which per definition only can assume whole (or a set number of decimals) numbers, can be visualized using a bar plot because it usually contains a finite number of unique values and at the same time cannot have values in between.\nA continuous variable can be measured with infinite amount of decimal places which means that there exist an infinite amount of unique values. Instead of having an infinite number of bars in a bar plot, we group adjacent values together into intervals and create a histogram. Another way to visualize a continuous variable is by using a box(-and-whisker) plot.\nWe do not differentiate between a variable following an interval or a ratio scale when visualizing a distribution, but it is something to take into account when interpreting the plots. For example, if a variable follows the interval scale, we could not state that values in one area of the figure are “twice as large” as values in another area.\n\n7.3.2.1 Histograms with geom_histogram\nTo look at a different kind of geometry, let us create a histogram of the shell shape measurements. This is done using geom_histogram:\n\nsnails |&gt;\n  ggplot(aes(x = shape)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nA histogram divides up the x-axis into equally-sized segments, called bins. We then count how many observations fall within the limits of each bin, and draw the bins as tall along the y-axis as that count. So bins with many data points are tall, and bins with only few observations are short.\nThe default setting for geom_histogram is to classify the data into 30 equally-sized bins, but that might not always be the best choice. In the above histogram, for example, there are some erratic-looking jumps that have more to do with having too few observations per bin than with the actual data themselves. To change the number of bins, we have two options. We can use the bins argument to geom_histogram to explicitly specify how many bins we want in the plot:\n\nsnails |&gt;\n  ggplot(aes(x = shape)) +\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n\nAnd the histogram now looks much more well-behaved. The other option is to specify the width of each individual bin, using the binwidth argument:\n\nsnails |&gt;\n  ggplot(aes(x = shape)) +\n  geom_histogram(binwidth = 0.02)\n\n\n\n\n\n\n\n\nWhat do we do to make the plot prettier—e.g., to change the color of the bins to something else? One can give the color argument to geom_histogram:\n\nsnails |&gt;\n  ggplot(aes(x = shape)) +\n  geom_histogram(bins = 15, color = \"navy\")\n\n\n\n\n\n\n\n\nAnd now the plot is navy blue - at least in outline. To change the color of the fill, one must also change the fill argument:\n\nsnails |&gt;\n  ggplot(aes(x = shape)) +\n  geom_histogram(bins = 15, color = \"navy\", fill = \"navy\")\n\n\n\n\n\n\n\n\nOne can make the fill color partially or completely transparent, too. The level of transparency is controlled by the alpha argument. It is 0 for fully transparent fills and 1 for fully opaque ones. The default value is 1, but one can reduce this for more transparency:\n\nsnails |&gt;\n  ggplot(aes(x = shape)) +\n  geom_histogram(bins = 15, color = \"navy\", fill = \"navy\", alpha = 0.6)\n\n\n\n\n\n\n\n\nReducing transparency further, we get:\n\nsnails |&gt;\n  ggplot(aes(x = shape)) +\n  geom_histogram(bins = 15, color = \"navy\", fill = \"navy\", alpha = 0.3)\n\n\n\n\n\n\n\n\nAnd, for full transparency:\n\nsnails |&gt;\n  ggplot(aes(x = shape)) +\n  geom_histogram(bins = 15, color = \"navy\", fill = \"navy\", alpha = 0)\n\n\n\n\n\n\n\n\nThe arguments color, fill, and alpha can also appear inside the aesthetic mappings. Why didn’t they here? Because, as stated, aesthetic mappings are those aspects of a plot that are governed by the data. But here, the color, fill, and transparency levels do not depend on the data: they have been chosen independently, to be constant values.\nIncidentally, there are many built-in color names in R. One can see the full list by invoking colors() in the R console - there are hundreds of them. To see what those colors actually look like, this website has a good overview.\n\n\n7.3.2.2 Box plots with geom_boxplot\nA box plot (also known as a “box-and-whisker plot”) provides a quick overview of how data are distributed: a box contains the middle 50% of all the data, whiskers at the ends of the boxes show the top and bottom 25%, and a thick line going through the box separates the top half from the bottom half of the data (the median). Additionally, points classified as outliers are shown explicitly. More detailed information on box plots will come in Section 6.3.1.3.\nOne can create box plots with geom_boxplot. Let us create one box plot of the distribution of shell shapes for each habitat (arid and humid). Putting habitat along the x-axis and shell shape along the y-axis:\n\nsnails |&gt;\n  ggplot(aes(x = habitat, y = shape)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nAs seen, the shell shape parameters in arid habitats tend to be larger than in humid ones.\nAlthough this is sufficient, one should feel free to make the plots prettier. For instance, one could use colors and fills, like before: \n\nsnails |&gt;\n  ggplot(aes(x = habitat, y = shape)) +\n  geom_boxplot(color = \"steelblue\", fill = \"steelblue\", alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\n7.3.3 Smoothing lines with geom_smooth\nLet us look at the following program and its output:\n\nsnails |&gt;\n  ggplot(aes(x = size, y = shape)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\n\nWe see two new things here. First, there are two geometries instead of one: the first generates a scatter plot with geom_point, but there is an additional geometry as well. This is perfectly legal; one can add as many as one wants. The aesthetic mappings will then apply to every geometry. Second, there is a function called geom_smooth. This generates a wiggly blue line which attempts to estimate the general trend of how the data change. There is a grey shaded region surrounding the line, which estimates the level of uncertainty in the blue line’s position.\nMost often, we want a simple linear function to estimate the overall trend in the data. This can be achieved by adding method = lm as an argument to geom_smooth:\n\nsnails |&gt;\n  ggplot(aes(x = size, y = shape)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\n\n\n\n\n(Here lm means “linear model”. It is a function in R which we will discuss in detail in Chapter 17 and subsequent chapters.) The shaded region can also be turned off using the se = FALSE argument:\n\nsnails |&gt;\n  ggplot(aes(x = size, y = shape)) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE)\n\n\n\n\n\n\n\n\nAs mentioned above, when having multiple geometries, the aesthetic mappings apply to all of them by default. This means that if, for example, we want to distinguish the habitat of the snails using colors (like we did before), then we automatically split the smoothing line into two colored lines as well:\n\nsnails |&gt;\n  ggplot(aes(x = size, y = shape, color = habitat)) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE)\n\n\n\n\n\n\n\n\nThis is a very handy feature. Sometimes however, we want to color the points by habitat but still retain a single global smoothing line for all points taken together. How can one do that? It turns out that in addition to the global aesthetic mappings (that are inside the ggplot function), one can also define local ones inside any geom_ function. So one solution could be to only define the color aesthetic inside geom_point:\n\nsnails |&gt;\n  ggplot(aes(x = size, y = shape)) +\n  geom_point(aes(color = habitat)) +\n  geom_smooth(method = lm, se = FALSE)\n\n\n\n\n\n\n\n\nOne final comment: the default color for the smoothing line is blue, but this can be changed within geom_smooth. To set the color to black, for instance:\n\nsnails |&gt;\n  ggplot(aes(x = size, y = shape)) +\n  geom_point(aes(color = habitat)) +\n  geom_smooth(method = lm, se = FALSE, color = \"black\")\n\n\n\n\n\n\n\n\nSince the black color is a constant property of the geometry, it was not defined as an aesthetic mapping.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Creating publication-grade figures</span>"
    ]
  },
  {
    "objectID": "07_Creating_figures.html#saving-plots",
    "href": "07_Creating_figures.html#saving-plots",
    "title": "7  Creating publication-grade figures",
    "section": "7.4 Saving plots",
    "text": "7.4 Saving plots\nTo save the most recently created ggplot figure, simply type\n\nggsave(filename = \"graph.pdf\", width = 4, height = 3)\n\nHere filename is the name (with path and extension) of the file you want to save the figure into. The extension is important: by having specified .pdf, the system automatically saves the figure in PDF format. To use, say, PNG instead:\n\nggsave(filename = \"graph.png\", width = 4, height = 3)\n\nPDF is a vectorized file format: the file contains the instructions for generating the plot elements instead of a pixel representation of the image. Consequently, PDF figures are arbitrarily scalable, and are therefore the preferred way of saving and handling scientific graphs.\nThe width and height parameters specify, in inches, the dimensions of the saved plot. Note that this also scales some other plot elements, such as the size of the axis labels and plot legends. This means you can play with the width and height parameters to save the figure at a size where the labels are clearly visible without being too large.\nIn case you would like to save a figure that is not the last one that was generated, you can specify the plot argument to ggsave(). to do so, first you should assign a plot to a variable. For example:\n\np &lt;- snails |&gt; # Assign the ggplot object to the variable p\n  ggplot(aes(x = size, y = shape)) +\n  geom_point(aes(color = habitat)) +\n  geom_smooth(method = lm, se = FALSE, color = \"black\")\n\nand then\n\nggsave(filename = \"graph.pdf\", plot = p, width = 4, height = 3)",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Creating publication-grade figures</span>"
    ]
  },
  {
    "objectID": "07_Creating_figures.html#sec-ggplot-exercises",
    "href": "07_Creating_figures.html#sec-ggplot-exercises",
    "title": "7  Creating publication-grade figures",
    "section": "7.5 Exercises",
    "text": "7.5 Exercises\nFauchald et al. (2017) tracked the population size of various herds of caribou in North America over time, and correlated population cycling with the amount of vegetation and sea-ice cover. The part of their data that we will use consists of two files: pop_size.tsv (data on herd population sizes), and sea_ice.tsv (on sea levels of sea ice cover per year and month).\n\nThe file sea_ice.tsv is in human-readable, wide format. Note however that the rule “each set of observations is stored in its own row” is violated. We would like to organize the data in a tidy tibble with four columns: Herd, Year, Month, and Cover. To this end, apply the function pivot_longer to columns 3-14 in the tibble, gathering the names of the months in the new column Month and the values in the new column Cover.\nUse pop_size.tsv to make a plot of herd sizes through time. Let the x-axis be Year, the y-axis be population size. Show different herds in different colors. For the geometry, use points.\nThe previous plot is actually not that easy to see and interpret. To make it better, add a line geometry as well, which will connect the points with lines.\nMake a histogram out of all population sizes in the data.\nMake the same histogram, but break it down by herd, using a different color and fill for each herd.\nInstead of a histogram, make a density plot with the same data and display (look up geom_density if needed).\nMake box plots of the population size of each herd. Along the x-axis, each herd should be separately displayed; the y-axis should be population size. The box plots should summarize population sizes across all years.\nLet us go back to sea_ice.tsv. Make the following plot. Along the x-axis, have Year. Along the y-axis, Month. Then, for each month-year pair, color the given part of the plot darker for lower ice cover and lighter for more. (Hint: look up geom_tile if needed.) Finally, make sure to do all this only for the herd with the label WAH (filter the data before plotting).\n\nThe following plotting exercises use the Galápagos land snail data (Section 4.2.2).\n\nCreate a density plot (geom_density) with shell size along the x-axis and the corresponding density of individuals along the y-axis. How much overlap is there between the sizes of the different species?\nRepeat the above exercise, but use the standardized size instead of the raw measurements. As a reminder, this means subtracting out the minimum from each entry, and dividing the results by the difference of the maximum and minimum entries (Section 5.1.3). Make sure to have clean, descriptive axis labels on the plot. What is the difference between this figure and the one obtained in the previous exercise using the non-standardized size values?\nNow do exercises 9-10 for shell shape instead of size.\nCreate a scatter plot with size along the x-axis and shape along the y-axis. Each individual should be a point on the graph.\nRepeat the same, but use the standardized size and shape measurements along the axes. Compare the figures. In what do they differ?\nCreate the same scatter plot with standardized size along the x-axis and shape along the y-axis, but with the points colored based on habitat. That is, individuals found in humid environments should all have one color, and those found in arid regions should have another. Do you see any patterns, in terms of whether certain shell sizes or shapes are more associated with a given type of habitat?\nModify the previous plot by coloring the points based on species instead of habitat. How much trait overlap is there between individuals belonging to different species? How do you interpret this result, especially in light of what you previously saw in exercises 9-11?\n\n\n\n\n\nFauchald, Per, Taejin Park, Hans Tømmervik, Ranga Myneni, and Vera Helene Hausner. 2017. “Arctic greening from warming promotes declines in caribou populations.” Science Advances 3 (4): e1601365. https://doi.org/10.1126/sciadv.1601365.\n\n\nWilkinson, Leland. 2006. The Grammar of Graphics. Secaucus, NJ, USA: Springer Science & Business Media.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Creating publication-grade figures</span>"
    ]
  },
  {
    "objectID": "08_Further_plotting_options.html",
    "href": "08_Further_plotting_options.html",
    "title": "8  Some further plotting options; introducing factors",
    "section": "",
    "text": "8.1 Themes\nIn this chapter we look at adding themes to out plots, manipulating scales (x- and y-axes, color, fill, and so on), faceting to create subplots within a bigger plot, and the plotting of summaries. We keep working with the land snail data:\nThe default appearance of a plot created with ggplot is with a gray background and white panel grids going through it to aid locating coordinates. Here is the plot we ended Chapter 7 with, where these features are clearly visible:\nsnails |&gt;\n  ggplot(aes(x = size, y = shape)) +\n  geom_point(aes(color = habitat)) +\n  geom_smooth(method = lm, se = FALSE, color = \"black\")\nThe general theme can be changed by adding one of the functions starting with theme_ to the plot. For example, theme_bw is a theme which uses a white background instead of the default gray:\nsnails |&gt;\n  ggplot(aes(x = size, y = shape)) +\n  geom_point(aes(color = habitat)) +\n  geom_smooth(method = lm, se = FALSE, color = \"black\") +\n  theme_bw()\nAnd theme_classic omits the panel grid lines, as well as the box from around the plot:\nsnails |&gt;\n  ggplot(aes(x = size, y = shape)) +\n  geom_point(aes(color = habitat)) +\n  geom_smooth(method = lm, se = FALSE, color = \"black\") +\n  theme_classic()\nFeel free to check out the built-in themes (theme_gray is the default). Additionally, the ggthemes package has many further options as well.\nFurther fine-tuning of a theme can be achieved using the theme function. For example, the default position of the color legend is at the right side of the plot. One can move the legend to another position by specifying the legend.position option within the theme function that can be added onto the plot:\nsnails |&gt;\n  ggplot(aes(x = size, y = shape)) +\n  geom_point(aes(color = habitat)) +\n  geom_smooth(method = lm, se = FALSE, color = \"black\") +\n  theme(legend.position = \"bottom\") # Or: \"top\", \"left\", \"right\", \"none\"\nSpecifying legend.position = \"none\" omits the legend altogether.\nA word of caution: in case the legend positioning is matched with a generic theme such as theme_bw(), one should put the legend position after the main theme definition. The reason is that pre-defined themes like theme_bw() override any specific theme options you might specify. The rule of thumb is: any theme() component to your plot should be added only after the generic theme definition. Otherwise the theme() component will be overridden and will not take effect. For example, this does not work as intended:\nsnails |&gt;\n  ggplot(aes(x = size, y = shape)) +\n  geom_point(aes(color = habitat)) +\n  geom_smooth(method = lm, se = FALSE, color = \"black\") +\n  theme(legend.position = \"bottom\") + # Position legend at the left\n  theme_bw() # Define general theme - and thus override the line above...\nBut this one does:\nsnails |&gt;\n  ggplot(aes(x = size, y = shape)) +\n  geom_point(aes(color = habitat)) +\n  geom_smooth(method = lm, se = FALSE, color = \"black\") +\n  theme_bw() + # This defines the general theme\n  theme(legend.position = \"bottom\") # Now override default legend positioning",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Some further plotting options; introducing factors</span>"
    ]
  },
  {
    "objectID": "08_Further_plotting_options.html#scales",
    "href": "08_Further_plotting_options.html#scales",
    "title": "8  Some further plotting options; introducing factors",
    "section": "8.2 Scales",
    "text": "8.2 Scales\nThe aesthetic mappings of a graph (x-axis, y-axis, color, fill, size, shape, alpha, …) are automatically rendered into the displayed plot, based on certain default settings within ggplot2. These defaults can be altered, however. Consider the following bare-bones plot:\n\nsnails |&gt;\n  ggplot(aes(x = habitat, y = shape)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nWe can now change, for example, how the y-axis is displayed. The component to be added to the plot is scale_y_continuous(). Here scale means we are going to change the scaling of some aesthetic mapping, y refers to the y-axis (as expected, it can be replaced with x, color, fill, etc.), and continuous means that the scaling of the axis is not via discrete values (e.g., either 1 or 2 or 3 but nothing in between), but continuous (every real number is permissible along the y-axis). The plot component scale_y_continuous() takes several arguments; take a look at its help page to see all possible options. Here we mention a few of them. First, there is the name option, which is used to relabel the axis. The limits argument accepts a vector of two values, containing the lower and upper limits of the plot. If any of them is set to NA, the corresponding limit will be determined automatically. Next, the breaks argument controls where the tick marks along the axis go. It is given as a vector, with its entries corresponding to the y-coordinates of the tick marks. Finally, labels determines what actually gets written on the axis at the tick mark points—it is therefore also a vector, its length matching that of breaks.\nAs an example, let us scale the y-axis of the previous graph in the following way. The axis label should read “Shell shape”, instead of the current “shape”. It should go from -0.17 to 0.17, with breaks at the values -0.15, 0, and 0.15. Here is how to do this:\n\nsnails |&gt;\n  ggplot(aes(x = habitat, y = shape)) +\n  geom_boxplot() +\n  scale_y_continuous(\n    name = \"Shell shape\",\n    limits = c(-0.17, 0.17),\n    breaks = c(-0.15, 0, 0.15)\n  )\n\n\n\n\n\n\n\n\nWhat should we do if we would additionally like the “0.00” in the middle to be displayed as just a “0” instead, without hte unnecessary decimals? In that case, we can add an appropriate labels option as an argument to scale_y_continuous:\n\nsnails |&gt;\n  ggplot(aes(x = habitat, y = shape)) +\n  geom_boxplot() +\n  scale_y_continuous(\n    name = \"Shell shape\",\n    limits = c(-0.17, 0.17),\n    breaks = c(-0.15, 0, 0.15),\n    labels = c(\"-0.15\", \"0\", \"0.15\")\n  )\n\n\n\n\n\n\n\n\nThe labels are given as character strings, defining the text that should be written at each tick mark specified by breaks.\nThe x-axis can be scaled similarly. One important difference though is that here the x-axis has a discrete scale. Since we are displaying habitat type along it, any value must be either arid or humid; it makes no sense to talk about what is “halfway in between arid and humid”. Therefore, one should use scale_x_discrete(). Its options are similar to those of scale_x_continuous(). For instance, let us override the axis label, writing Habitat with a capital H:\n\nsnails |&gt;\n  ggplot(aes(x = habitat, y = shape)) +\n  geom_boxplot() +\n  scale_y_continuous(\n    name = \"Shell shape\",\n    limits = c(-0.17, 0.17),\n    breaks = c(-0.15, 0, 0.15),\n    labels = c(\"-0.15\", \"0\", \"0.15\")\n  ) +\n  scale_x_discrete(name = \"Habitat\")\n\n\n\n\n\n\n\n\nOne could also redefine the labels along the x-axis in the same spirit, using capitalized starting letters:\n\nsnails |&gt;\n  ggplot(aes(x = habitat, y = shape)) +\n  geom_boxplot() +\n  scale_y_continuous(\n    name = \"Shell shape\",\n    limits = c(-0.17, 0.17),\n    breaks = c(-0.15, 0, 0.15),\n    labels = c(\"-0.15\", \"0\", \"0.15\")\n  ) +\n  scale_x_discrete(\n    name = \"Habitat\",\n    labels = c(\"Arid\", \"Humid\")\n  )\n\n\n\n\n\n\n\n\nOther aesthetic mappings can also be adjusted, such as color, fill, size, or alpha. One useful way to do it is through scale_color_manual(), scale_fill_manual(), and so on. These are like scale_color_discrete(), scale_fill_discrete() etc., except that they allow one to specify a discrete set of values by hand. Let us do this for color and fill:\n\nsnails |&gt;\n  ggplot(aes(x = habitat, y = shape, color = habitat, fill = habitat)) +\n  geom_boxplot(alpha = 0.2) +\n  scale_y_continuous(\n    name = \"Shell shape\",\n    limits = c(-0.17, 0.17),\n    breaks = c(-0.15, 0, 0.15),\n    labels = c(\"-0.15\", \"0\", \"0.15\")\n  ) +\n  scale_x_discrete(\n    name = \"Habitat\",\n    labels = c(\"Arid\", \"Humid\")\n  ) +\n  scale_color_manual(values = c(\"goldenrod\", \"steelblue\")) +\n  scale_fill_manual(values = c(\"goldenrod\", \"steelblue\"))\n\n\n\n\n\n\n\n\nThe choice of colors was not arbitrary: the dry yellow-gold color \"goldenrod\" corresponds to the arid, while the moist \"steelblue\" color to the humid habitats. As mentioned in Section 7.3.2.1, one can look at the various color options in R at https://r-charts.com/colors. Additionally, a useful R color cheat sheet can be found here, for more options and built-in color names.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Some further plotting options; introducing factors</span>"
    ]
  },
  {
    "objectID": "08_Further_plotting_options.html#sec-faceting",
    "href": "08_Further_plotting_options.html#sec-faceting",
    "title": "8  Some further plotting options; introducing factors",
    "section": "8.3 Facets",
    "text": "8.3 Facets\nPlots can be faceted (subplots created and arranged in a grid layout) based on some variable or variables. For instance, let us create histograms of shell shapes in the snails dataset:\n\nsnails |&gt;\n  ggplot(aes(x = shape)) +\n  geom_histogram(bins = 20)\n\n\n\n\n\n\n\n\nThis way, one cannot see which part of the histogram belongs to which habitat. One fix to this is to color the histogram by habitat. Another is to separate the plot into two facets, each displaying data for one of the habitats only:\n\nsnails |&gt;\n  ggplot(aes(x = shape)) +\n  geom_histogram(bins = 20) +\n  facet_wrap(~ habitat)\n\n\n\n\n\n\n\n\nThe function call facet_wrap(~ habitat) means that the data will be grouped based on habitat, and displayed separately in distinct subplots. The subplots are arranged in a regular grid, first side by side, and then wrapping around to the next row of facets if needed (hence the name facet_wrap). One can control the number of rows and/or columns of facets using the nrow and ncol arguments. For example, to display the facets on top of one another instead of side by side:\n\nsnails |&gt;\n  ggplot(aes(x = shape)) +\n  geom_histogram(bins = 20) +\n  facet_wrap(~ habitat, ncol = 1) # And / or: nrow = 2\n\n\n\n\n\n\n\n\nIn this particular case, one can argue that this way of displaying the data is better, because it makes the two shell shape distributions share the same x-axis. This makes them directly comparable. Thus, it is immediately obvious that shell shapes are larger in the arid than in the humid regions, which is not immediately visible when the facets are next to one another.\nIt is also possible to create facets where two, instead of just one, variables control what goes in each facet. The first variable then controls what gets displayed in the rows, the other controls what gets displayed in the columns. To illustrate how to make such a two-dimensional grid of facets, let us first apply pivot_longer to the snails dataset:\n\nsnails |&gt;\n  pivot_longer(cols = size | shape,\n               names_to = \"trait\",\n               values_to = \"value\")\n\n# A tibble: 446 × 4\n   habitat species   trait  value\n   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;\n 1 humid   ustulatus size  17.1  \n 2 humid   ustulatus shape -0.029\n 3 humid   ustulatus size  20.1  \n 4 humid   ustulatus shape -0.001\n 5 humid   ustulatus size  16.3  \n 6 humid   ustulatus shape  0.014\n 7 arid    calvus    size  13.7  \n 8 arid    calvus    shape -0.043\n 9 humid   nux       size  21.9  \n10 humid   nux       shape -0.042\n# ℹ 436 more rows\n\n\n(We specified the columns for pivoting via size | shape, meaning that any column called either size or shape gets selected—that is, only those two are chosen in the end. See Section 5.1.1 to review the various tidy selection mechanisms for choosing columns.) We can create a histogram of each measured trait in each habitat now, in a remarkably simple way, by using the facet_grid function:\n\nsnails |&gt;\n  pivot_longer(cols = size | shape,\n               names_to = \"trait\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 20) +\n  facet_grid(habitat ~ trait)\n\n\n\n\n\n\n\n\nHere facet_grid(habitat ~ trait) means that the data will be grouped based on columns habitat and trait, with the distinct values of habitat making up the rows and those of trait the columns of the grid of plots. Also, if one of them would be replaced with a dot, as in e.g. facet_grid(habitat ~ .), then that variable is ignored, and only the other variable is used in creating the subplots.\nThe above plot is not satisfactory. This is because shell size and shell shape are measured on vastly different scales. Therefore it would be better to use different x-axes for the two facet columns. This can be achieved by adding the scales = \"free_x\" argument to facet_grid:\n\nsnails |&gt;\n  pivot_longer(cols = size | shape,\n               names_to = \"trait\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 20) +\n  facet_grid(habitat ~ trait, scales = \"free_x\")\n\n\n\n\n\n\n\n\nIn a similar manner, scales = \"free_y\" is also a valid option (making the y-axes across facet rows vary individually), as is scales = \"free\" (both x- and y-axes vary across facet columns and rows). The default setting, which uses the same axes for all facet rows and columns, is scales = \"fixed\".\n\n8.3.1 Grouped histograms\nRefer back to the population density data analyzed in Chapter 6. When we summarized the mean and standard deviation of the density of each species we concluded that there are some similarities and some differences between the three groups. The distribution of each species can also help in understanding our data, especially if we also add some summary statistics into the figure.\n\npop |&gt; \n  pivot_longer(cols = starts_with(\"species\"),\n               names_to = \"species\",\n               values_to = \"density\") |&gt; \n  group_by(species) |&gt; \n  mutate(mean = mean(density)) |&gt;\n  ungroup() |&gt;\n  # Begin visualization\n  ggplot(aes(x = density)) + \n  geom_histogram(binwidth = 0.5, fill = \"steelblue\") + \n  # Facets the histogram based on species\n  facet_grid(species ~ .) +\n  labs(y = \"count\") +\n  # Adds a custom segment to the visualization depicting the mean\n  geom_segment(\n    # Define start and end values of the segment:\n    aes(x = mean, xend = mean, y = 0, yend = 50),\n    # Define width and color of the segment:\n    linewidth = 1, color = \"firebrick\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 8.1: The density distribution of three different species, with their means marked in red.\n\n\n\n\n\nThe mean gives us a measure of the center of the data—but as we can see in the visualizations, sometimes the mean by itself misrepresents the data as a whole. For example, the mean of Species 2 is 4.56% but the values of the variable are grouped either lower or higher than this value. The mean itself isn’t actually close to an observed value. Compare this to the mean of Species 1 (5.3%) which actually falls close to observed values. The mean is better at describing this variable than the previous.\nThis also shows why the groups have different standard deviations, as the observations of Species 2 are on average further away from its mean than the observations of Species 1. We would not have been able to draw these conclusions without visualizing the variable and this shows the importance of visualizations when describing data.\n\n\n8.3.2 Grouped boxplots\nIf we want to compare the distribution of multiple groups we can also use facet_grid().\n\npop |&gt; \n  pivot_longer(cols = starts_with(\"species\"),\n               names_to = \"species\",\n               values_to = \"density\") |&gt; \n  ggplot(aes(x = density)) + \n  geom_boxplot(color = \"steelblue\", fill = \"steelblue\", alpha = 0.2) +\n  # Facets the histogram based on species\n  facet_grid(species ~ .) +\n  labs(x = \"Population Density\") + \n  # The y-axis is not relevant in this type of visualization,\n  # so it is removed by setting breaks = NULL:\n  scale_y_continuous(breaks = NULL) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe line (whiskers) mark the range of the first and last 25% of the data limited by the smallest and largest value. The box in the middle mark the range of the middle 50% of the data with the bold line inside the box showing the median.\nThe function in R also does something that we might not have expected for Species 1. There are a number of points further away from the line that are not considered to be a part of the box plot data, they are instead considered outliers. R defines any value further than 1.5 times the interquartile range (see Section 6.3.2.2) from the edges of the box as outliers.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Some further plotting options; introducing factors</span>"
    ]
  },
  {
    "objectID": "08_Further_plotting_options.html#sec-CI",
    "href": "08_Further_plotting_options.html#sec-CI",
    "title": "8  Some further plotting options; introducing factors",
    "section": "8.4 Summaries and confidence intervals",
    "text": "8.4 Summaries and confidence intervals\nProviding appropriate information on experimental errors is a hallmark of any credible scientific graph. Choose a type of error based on the conclusion that you want the reader to draw. For example, one can show the largest and smallest values in the dataset apart from the average.\nHere we will display the median shell shape in each habitat (the value such that exactly half the data points have a smaller and half have a larger shape measurement), together with the smallest and largest values. To do so, we first obtain the median, minimum, and maximum of the shell shapes in each habitat. So we first group_by habitat and summarize to obtain the required statistics (medians, minima, and maxima). We then hand these data over for plotting. One can include all these steps in a single logical workflow:\n\nsnails |&gt;\n  group_by(habitat) |&gt; # Perform summary calculations for each habitat\n  summarize(medianShape = median(shape), # Median shape\n            minShape = min(shape), # Minimum shape\n            maxShape = max(shape)) |&gt; # Maximum shape\n  ungroup() |&gt; # Ungroup data\n  # Start plotting:\n  ggplot(aes(x = habitat, y = medianShape,\n             ymin = minShape, ymax = maxShape)) +\n  geom_point() + # This takes the y aesthetic, for plotting the median\n  geom_errorbar(width = 0.2) # Takes the ymin and ymax aesthetics\n\n\n\n\n\n\n\n\n(The geom_errorbar function generates the ranges, going from ymin to ymax. Its width parameter controls how wide the horizontal lines at the ends of the whiskers should be.) Note that the y-axis label is neither as pretty nor as descriptive as it could be. This is because it inherited the name of the corresponding data column, medianShape. But we are not only displaying the median; we are also showing the full range of the data. So let us override this name. This could be done by adding scale_y_continuous(name = \"median with full range\") to the plot. But since we only want to adjust the axis label and not any of the other properties of the axis, there is a simpler alternative using the labs function:\n\nsnails |&gt;\n  group_by(habitat) |&gt; # Perform summary calculations for each habitat\n  summarize(medianShape = median(shape), # Median shape\n            minShape = min(shape), # Minimum shape\n            maxShape = max(shape)) |&gt; # Maximum shape\n  ungroup() |&gt; # Ungroup data\n  # Start plotting:\n  ggplot(aes(x = habitat, y = medianShape,\n             ymin = minShape, ymax = maxShape)) +\n  geom_point() + # This takes the y aesthetic, for plotting the median\n  geom_errorbar(width = 0.2) + # Takes the ymin and ymax aesthetics\n  labs(y = \"median with full range\")\n\n\n\n\n\n\n\n\nAnother way of characterizing uncertainty is to turn away from the spread of values in the actual raw data and instead focus on the uncertainty of an estimated quantity such as the mean. The rest of this subsection shows how to calculate confidence intervals for estimated means, and is intended for those who are already familiar with some basic statistical concepts. Alternatively, this section can be referred back to after reading the second part of the book.\nWe can also make use of statistical inference, for instance 95% confidence intervals of the mean (they will be explained more in detail in Section 12.1 and Section 13.1.1). We must first obtain some necessary summary statistics: the number of observations (sample size) in each group; the standard error of the mean (standard deviation divided by the square root of the sample size); and finally, the confidence interval itself (read off from the quantile function of the t-distribution, with one fewer degrees of freedom than the sample size). We can then include these confidence intervals on top of the mean values:\n\nsnails |&gt;\n  group_by(habitat) |&gt;\n  summarize(mean = mean(shape), # Mean shape in each habitat\n            sd = sd(shape), # Standard deviation per habitat\n            N = n(), # Sample size (number of observations) per habitat\n            SEM = sd / sqrt(N), # Standard error of the mean\n            CI = SEM * qt(1 - 0.025, N - 1)) |&gt; # Confidence interval,\n                 # read off at 1 - 0.025 because +/- 2.5% adds up to 5%\n  ungroup() |&gt;\n  ggplot(aes(x = habitat, y = mean, ymin = mean - CI, ymax = mean + CI)) +\n  geom_point() +\n  geom_errorbar(width = 0.2) +\n  labs(y = \"mean shell shape, +/- 95% confidence interval\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to note that these visualizations do not take into account the variance between the groups in a correct manner. What this means is that we cannot make conclusions about the differences between groups. We must instead calculate a confidence interval of the difference in means between the groups which we will do in Section 14.2.\n\n\nIn the literature, it is common to encounter bar-and-whisker plots to represent the same information as above. These replace the points showing the means with bars that start from zero. One should be aware of how to read and make such graphs. This is almost trivially simple: all one needs to do is replace geom_point with geom_col.\n\nsnails |&gt;\n  group_by(habitat) |&gt;\n  summarize(mean = mean(shape), # Mean shape in each habitat\n            sd = sd(shape), # Standard deviation per habitat\n            N = n(), # Sample size (number of observations) per habitat\n            SEM = sd / sqrt(N), # Standard error of the mean\n            CI = SEM * qt(1 - 0.025, N - 1)) |&gt; # Confidence interval,\n                 # read off at 1 - 0.025 because +/- 2.5% adds up to 5%\n  ungroup() |&gt;\n  ggplot(aes(x = habitat, y = mean, ymin = mean - CI, ymax = mean + CI)) +\n  # Below we change geom_point to geom_col (for \"column\"):\n  geom_col(fill = \"steelblue\") +\n  geom_errorbar(width = 0.2) +\n  labs(y = \"mean shell shape, +/- 95% confidence interval\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhile means and their confidence intervals are often displayed with bar-and-whisker plots, there is an important reason why this is not a good idea. By starting the bars from zero, the plot implies that zero is a natural point of comparison for all the data. Unfortunately, this can visually distort the information we wish to convey. Consider the following graph:\n\ntibble(Species = c(\"species 1\", \"species 2\"),\n       Average = c(148, 152),\n       CI = c(0.8, 0.9)) |&gt;\n  ggplot(aes(x = Species, y = Average,\n             ymin = Average - CI, ymax = Average + CI)) +\n  geom_col(fill = \"steelblue\") +\n  geom_errorbar(width = 0.2) +\n  labs(y = \"Trait value\")\n\n\n\n\n\n\n\n\nIt is impossible to see whether there are any relevant differences between the two species. The following is exactly the same, but with the mean values shown with points instead of bars:\n\ntibble(Species = c(\"species 1\", \"species 2\"),\n       Average = c(148, 152),\n       CI = c(0.8, 0.9)) |&gt;\n  ggplot(aes(x = Species, y = Average,\n             ymin = Average - CI, ymax = Average + CI)) +\n  geom_point() +\n  geom_errorbar(width = 0.2) +\n  labs(y = \"Trait value\")\n\n\n\n\n\n\n\n\nIt is now obvious that the two observations are distinct. Due to this problem, when plotting groups of continuous variables side by side, it is recommended to avoid bar-and-whisker plots and simply use point-and-whisker plots instead.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Some further plotting options; introducing factors</span>"
    ]
  },
  {
    "objectID": "08_Further_plotting_options.html#sec-factors",
    "href": "08_Further_plotting_options.html#sec-factors",
    "title": "8  Some further plotting options; introducing factors",
    "section": "8.5 Reordering labels using factors",
    "text": "8.5 Reordering labels using factors\nIn the previous examples, the order of text labels was always automatically determined. The basic rule in R is that the ordering follows the alphabet: the default is for the arid habitat to precede humid. This default ordering can be inconvenient, however. Consider the data file temp-Lin.csv of average temperatures per month, measured in the town of Linköping, Sweden:\n\nread_delim(\"temp-Lin.csv\", delim = \",\")\n\n# A tibble: 12 × 2\n   month temp_C\n   &lt;chr&gt;  &lt;dbl&gt;\n 1 Jan     -1.9\n 2 Feb     -1.7\n 3 Mar      0.9\n 4 Apr      6.3\n 5 May     11.5\n 6 Jun     15.4\n 7 Jul     17.9\n 8 Aug     16.6\n 9 Sep     12.8\n10 Oct      7.5\n11 Nov      3.4\n12 Dec      0.1\n\n\nThe table has two columns: month and temp_C, giving the mean temperature in each month across years 1991-2021.1 So far so good. However, if we plot this with months along the x-axis and temperature along the y-axis, we run into trouble because R displays items by alphabetical instead of chronological order:\n\nread_delim(\"temp-Lin.csv\", delim = \",\") |&gt;\n  ggplot(aes(x = month, y = temp_C)) +\n  geom_point(color = \"steelblue\") +\n  labs(y = \"average temperature (Celsius)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nTo fix this, one must convert the type of month from a simple vector of character strings to a vector of factors. Factors are categorical variables (i.e., take on well-defined distinct values instead of varying on a continuous scale like double-precision numbers), but with an extra attribute which determines the order of those values. This ordering is often referred to as the levels of the factor. The first of the values has level 1, the next one level 2, and so on.\nOne very convenient way of assigning factor levels is through the tidyverse function as_factor.2 This function takes a vector of values and, if the values are numeric, assigns them levels based on those numerical values. However, if the values are character strings, then the levels are assigned in order of appearance within the vector. This is perfect for us, because the months are in proper order already within the tibble:\n\nread_delim(\"temp-Lin.csv\", delim = \",\") |&gt;\n  mutate(month = as_factor(month)) |&gt;\n  ggplot(aes(x = month, y = temp_C)) +\n  geom_point(color = \"steelblue\") +\n  labs(y = \"average temperature (Celsius)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIt is also possible to take a factor and reassign its levels manually. This can be done with the fct_relevel function:\n\nread_delim(\"temp-Lin.csv\", delim = \",\") |&gt;\n  mutate(month = fct_relevel(month, \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n                             \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")) |&gt;\n  ggplot(aes(x = month, y = temp_C)) +\n  geom_point(color = \"steelblue\") +\n  labs(y = \"average temperature (Celsius)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nAlso, the use of fct_relevel need not be this laborious. If all we want to do is place a few factor levels to be first ones without changing any of the others, it is possible to enter just their names. Often, for example, a factor column holds various experimental treatments, one of which is called \"control\". In that case, all we might want to do is to make the control be the first factor level, without altering any of the others. If treatment is the name of the vector (or column in a data frame) that holds the different experimental treatment names, then this can be done with fct_relevel(treatment, \"control\").",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Some further plotting options; introducing factors</span>"
    ]
  },
  {
    "objectID": "08_Further_plotting_options.html#sec-moreggplot-exercises",
    "href": "08_Further_plotting_options.html#sec-moreggplot-exercises",
    "title": "8  Some further plotting options; introducing factors",
    "section": "8.6 Exercises",
    "text": "8.6 Exercises\nLet us revisit the data of Fauchald et al. (2017) which we used in Section 7.5, tracking the population size of various herds of caribou in North America over time and correlating population cycling with the amount of vegetation and sea-ice cover. Using the file sea_ice.tsv (sea ice cover per year and month for each caribou herd), do the following:\n\nOne exercise from Section 7.5 was to plot Year along the x-axis and Month along the y-axis, with color tile shading indicating the level of ice cover for the herd labeled WAH in each month-year pair (using geom_tile). The resulting graph had an obvious weakness: the months along the y-axis were not in proper chronological order. Fix this problem by converting the Month column from character strings to factors whose levels go from January to December.\nCreate a similar plot, but do not filter for one single herd. Instead, have each herd occupy a different facet (sub-plot). Try doing this both with facet_grid and facet_wrap. Which do you like better, and why?\nFor each herd, compute the mean, the minimum, and the maximum cover over the full time span of the data. Then plot herd along the x-axis, and the mean (using points) plus the range (using error bars) along the y-axis.\n\nThe remaining exercises use the Galápagos land snail data (Section 4.2.2).\n\nCreate a plot with standardized size along the x-axis (you can review what standardization means in Section 5.1.3), standardized shape along the y-axis, each individual represented by a point colored by species, and with two facets corresponding to humid and arid habitat types. The facets should be side by side. How does this figure influence the interpretation you had in Section 7.5, exercise 15? That is: does the splitting of communities based on habitat type increase or decrease the overlap between different species?\nRe-create the previous plot with the two side-by-side facets, but in reverse order: the humid facet should be on the left and arid on the right. (Hint: convert habitat to factors!)\n\n\n\n\n\nFauchald, Per, Taejin Park, Hans Tømmervik, Ranga Myneni, and Vera Helene Hausner. 2017. “Arctic greening from warming promotes declines in caribou populations.” Science Advances 3 (4): e1601365. https://doi.org/10.1126/sciadv.1601365.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Some further plotting options; introducing factors</span>"
    ]
  },
  {
    "objectID": "08_Further_plotting_options.html#footnotes",
    "href": "08_Further_plotting_options.html#footnotes",
    "title": "8  Some further plotting options; introducing factors",
    "section": "",
    "text": "Data from https://climate-data.org.↩︎\nThere also exists a similarly-named function called as.factor, in addition to as_factor. It is the base R version of the same functionality. As usual, the tidyverse version offers improvements over the original, so it is recommended not to use as.factor at all, relying on just as_factor instead.↩︎",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Some further plotting options; introducing factors</span>"
    ]
  },
  {
    "objectID": "09_Joining_data.html",
    "href": "09_Joining_data.html",
    "title": "9  Joining data",
    "section": "",
    "text": "9.1 Merging two related tables into one\nSo far, we have been working with a single table of data at a time. Often however, information about the same observation or experiment is scattered across multiple tables and files. In such cases, we sometimes want to join those separate tables into a single one. To illustrate how this can be done, let us create two simple tables. The first will contain the names of students, along with their chosen subject:\nlibrary(tidyverse)\n\nstudies &lt;- tibble(\n  name    = c(\"Sacha\", \"Gabe\", \"Alex\"),\n  subject = c(\"Physics\", \"Chemistry\", \"Biology\")\n)\n\nprint(studies)\n\n# A tibble: 3 × 2\n  name  subject  \n  &lt;chr&gt; &lt;chr&gt;    \n1 Sacha Physics  \n2 Gabe  Chemistry\n3 Alex  Biology\nThe second table contains slightly different information: it holds which year a given student is currently into their studies.\nstage &lt;- tibble(\n  name = c(\"Sacha\", \"Alex\", \"Jamie\"),\n  year = c(3, 1, 2)\n)\n\nprint(stage)\n\n# A tibble: 3 × 2\n  name   year\n  &lt;chr&gt; &lt;dbl&gt;\n1 Sacha     3\n2 Alex      1\n3 Jamie     2\nNotice that, while Sacha and Alex appear in both tables, Gabe is only included in studies and Jamie only in stage. While in our tiny tables might seem like an avoidable oversight, such non-perfect alignment of data is often the norm when working with data spanning hundreds or more rows. Here, for the purposes of illustration, we use small tables, but the principles we learn here apply in a broader context as well.\nThere are four commonly used ways of joining these tables into one single dataset. All of them follow the same general pattern: the arguments are two tibbles to be joined, plus a by = argument which defines a join specification of the columns by which the tables should be joined. The join specification can be given using a helper function called join_by. It simply receives the names of the columns to use for joining.1 The output is always a single tibble, containing some type of joining of the data. Let us now look at each joining method in detail.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Joining data</span>"
    ]
  },
  {
    "objectID": "09_Joining_data.html#merging-two-related-tables-into-one",
    "href": "09_Joining_data.html#merging-two-related-tables-into-one",
    "title": "9  Joining data",
    "section": "",
    "text": "9.1.1 left_join\nThe left_join function keeps only those rows that appear in the first of the two tables to be joined:\n\nleft_join(studies, stage, by = join_by(name))\n\n# A tibble: 3 × 3\n  name  subject    year\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 Sacha Physics       3\n2 Gabe  Chemistry    NA\n3 Alex  Biology       1\n\n\nThere are two things to notice. First, Jamie is missing from the name column above, even though s/he did appear in the stage tibble. This is exactly the point of left_join: if a row entry in the joining column (specified in by = join_by(...)) does not appear in the first table listed in the arguments, then it is omitted. Second, the year entry for Gabe is NA. This is because Gabe is absent from the stage table, and therefore has no associated year of study. Rather than make up nonsense, R fills out such missing data with NA values.\n\n\n9.1.2 right_join\nThis function works just like left_join, except only those rows are retained which appear in the second of the two tables to be joined:\n\nright_join(studies, stage, by = join_by(name))\n\n# A tibble: 3 × 3\n  name  subject  year\n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Sacha Physics     3\n2 Alex  Biology     1\n3 Jamie &lt;NA&gt;        2\n\n\nIn other words, this is exactly the same as calling left_join with its first two arguments reversed:\n\nleft_join(stage, studies, by = join_by(name))\n\n# A tibble: 3 × 3\n  name   year subject\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;  \n1 Sacha     3 Physics\n2 Alex      1 Biology\n3 Jamie     2 &lt;NA&gt;   \n\n\nThe only difference is in the ordering of the columns, but the data contained in the tables are identical.\nIn this case, the column subject is NA for Jamie. The reason is the same as it was before: since the studies table has no name entry for Jamie, the corresponding subject area is filled in with a missing value NA.\n\n\n9.1.3 inner_join\nThis function retains only those rows which appear in both tables to be joined. For our example, since Gabe only appears in studies and Jamie only in stage, they will be dropped by inner_join and only Sacha and Alex are retained (as they appear in both tables):\n\ninner_join(studies, stage, by = join_by(name))\n\n# A tibble: 2 × 3\n  name  subject  year\n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Sacha Physics     3\n2 Alex  Biology     1\n\n\n\n\n9.1.4 full_join\nThe complement to inner_join, this function retains all rows in all tables, filling in missing values with NAs everywhere:\n\nfull_join(studies, stage, by = join_by(name))\n\n# A tibble: 4 × 3\n  name  subject    year\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 Sacha Physics       3\n2 Gabe  Chemistry    NA\n3 Alex  Biology       1\n4 Jamie &lt;NA&gt;          2\n\n\n\n\n9.1.5 Joining by multiple columns\nIt is also possible to use the above joining functions specifying multiple columns to join data by. To illustrate how to do this and what this means, imagine that we slightly modify the student data. The first table will contain the name, study area, and year of study for each student. The second table will contain the name and study area of each student, plus whether they have passed their most recent exam:\n\nprogram  &lt;- tibble(\n  name    = c(\"Sacha\", \"Gabe\", \"Alex\"),\n  subject = c(\"Physics\", \"Chemistry\", \"Biology\"),\n  year    = c(1, 3, 2)\n)\n\nprint(program)\n\n# A tibble: 3 × 3\n  name  subject    year\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 Sacha Physics       1\n2 Gabe  Chemistry     3\n3 Alex  Biology       2\n\n\n\nprogress &lt;- tibble(\n  name     = c(\"Sacha\", \"Gabe\", \"Jamie\"),\n  subject  = c(\"Physics\", \"Chemistry\", \"Biology\"),\n  examPass = c(TRUE, FALSE, TRUE)\n)\n\nprint(progress)\n\n# A tibble: 3 × 3\n  name  subject   examPass\n  &lt;chr&gt; &lt;chr&gt;     &lt;lgl&gt;   \n1 Sacha Physics   TRUE    \n2 Gabe  Chemistry FALSE   \n3 Jamie Biology   TRUE    \n\n\nAnd now, since the tables share not just one but two columns, it makes sense to join them using both. This can be done by specifying each column inside join_by in the by = argument. For example, left-joining program and progress by both name and subject leads to a joint table in which all unique name-subject combinations found in program are retained, but those found only in progress are discarded:\n\nleft_join(program, progress, by = join_by(name, subject))\n\n# A tibble: 3 × 4\n  name  subject    year examPass\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;   \n1 Sacha Physics       1 TRUE    \n2 Gabe  Chemistry     3 FALSE   \n3 Alex  Biology       2 NA      \n\n\nThe other joining functions also work as expected:\n\nright_join(program, progress, by = join_by(name, subject))\n\n# A tibble: 3 × 4\n  name  subject    year examPass\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;   \n1 Sacha Physics       1 TRUE    \n2 Gabe  Chemistry     3 FALSE   \n3 Jamie Biology      NA TRUE    \n\n\n\ninner_join(program, progress, by = join_by(name, subject))\n\n# A tibble: 2 × 4\n  name  subject    year examPass\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;   \n1 Sacha Physics       1 TRUE    \n2 Gabe  Chemistry     3 FALSE   \n\n\n\nfull_join(program, progress, by = join_by(name, subject))\n\n# A tibble: 4 × 4\n  name  subject    year examPass\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;   \n1 Sacha Physics       1 TRUE    \n2 Gabe  Chemistry     3 FALSE   \n3 Alex  Biology       2 NA      \n4 Jamie Biology      NA TRUE",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Joining data</span>"
    ]
  },
  {
    "objectID": "09_Joining_data.html#binding-rows-and-columns-to-a-table",
    "href": "09_Joining_data.html#binding-rows-and-columns-to-a-table",
    "title": "9  Joining data",
    "section": "9.2 Binding rows and columns to a table",
    "text": "9.2 Binding rows and columns to a table\nOccasionally, a simpler problem presents itself: there is a single dataset, but its rows are contained across separate tables. For example, a table containing student names and subject areas might be spread across two tables, like this:\n\nstudies1 &lt;- tibble(\n  name    = c(\"Sacha\", \"Gabe\", \"Alex\"),\n  subject = c(\"Physics\", \"Chemistry\", \"Biology\")\n)\n\nprint(studies1)\n\n# A tibble: 3 × 2\n  name  subject  \n  &lt;chr&gt; &lt;chr&gt;    \n1 Sacha Physics  \n2 Gabe  Chemistry\n3 Alex  Biology  \n\n\n\nstudies2 &lt;- tibble(\n  name    = c(\"Jamie\", \"Ashley\", \"Dallas\", \"Jordan\"),\n  subject = c(\"Geology\", \"Mathematics\", \"Philosophy\", \"Physics\")\n)\n\nprint(studies2)\n\n# A tibble: 4 × 2\n  name   subject    \n  &lt;chr&gt;  &lt;chr&gt;      \n1 Jamie  Geology    \n2 Ashley Mathematics\n3 Dallas Philosophy \n4 Jordan Physics    \n\n\nThe tables have the exact same structure, in that the column names and types are identical. It’s just that the rows are, for some reason, disparate. To combine them together, we could recourse to full-joining the tables by both their columns:\n\nfull_join(studies1, studies2, by = join_by(name, subject))\n\n# A tibble: 7 × 2\n  name   subject    \n  &lt;chr&gt;  &lt;chr&gt;      \n1 Sacha  Physics    \n2 Gabe   Chemistry  \n3 Alex   Biology    \n4 Jamie  Geology    \n5 Ashley Mathematics\n6 Dallas Philosophy \n7 Jordan Physics    \n\n\nThis, however, is not necessary. Whenever all we need to do is take two tables and stick their rows together, there is the simpler bind_rows:\n\nbind_rows(studies1, studies2)\n\n# A tibble: 7 × 2\n  name   subject    \n  &lt;chr&gt;  &lt;chr&gt;      \n1 Sacha  Physics    \n2 Gabe   Chemistry  \n3 Alex   Biology    \n4 Jamie  Geology    \n5 Ashley Mathematics\n6 Dallas Philosophy \n7 Jordan Physics    \n\n\nSimilarly, in case two tables have the same number of rows but different columns, one can stick their columns together using bind_cols. For example, suppose we have\n\nstudies &lt;- tibble(\n  name    = c(\"Sacha\", \"Gabe\", \"Alex\"),\n  subject = c(\"Physics\", \"Chemistry\", \"Biology\")\n)\n\nprint(studies)\n\n# A tibble: 3 × 2\n  name  subject  \n  &lt;chr&gt; &lt;chr&gt;    \n1 Sacha Physics  \n2 Gabe  Chemistry\n3 Alex  Biology  \n\n\nas well as a table with year of study and result of last exam only:\n\nyearExam &lt;- tibble(\n  year     = c(3, 1, 2),\n  examPass = c(FALSE, TRUE, TRUE)\n)\n\nprint(yearExam)\n\n# A tibble: 3 × 2\n   year examPass\n  &lt;dbl&gt; &lt;lgl&gt;   \n1     3 FALSE   \n2     1 TRUE    \n3     2 TRUE    \n\n\nWe can now join these using bind_cols:\n\nbind_cols(studies, yearExam)\n\n# A tibble: 3 × 4\n  name  subject    year examPass\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;   \n1 Sacha Physics       3 FALSE   \n2 Gabe  Chemistry     1 TRUE    \n3 Alex  Biology       2 TRUE",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Joining data</span>"
    ]
  },
  {
    "objectID": "09_Joining_data.html#exercises",
    "href": "09_Joining_data.html#exercises",
    "title": "9  Joining data",
    "section": "9.3 Exercises",
    "text": "9.3 Exercises\nWe have used the data of Fauchald et al. (2017) before in other exercises, in Section 7.5 and Section 8.6. As a reminder, they tracked the population size of various herds of caribou in North America over time, and correlated population cycling with the amount of vegetation and sea-ice cover. Two files from their data are pop_size.tsv (herd population sizes) and sea_ice.tsv (sea ice cover per year and month).\n\nLoad these two datasets into two variables. They could be called pop and ice, for instance. Look at the data to familiarize yourself with them. How many rows and columns are in each?\nBefore doing anything else: how many rows will there be in the table that is the left join of pop and ice, based on the two columns Herd and Year? Perform the left join to see if you were correct. Where do you see NAs in the table, and why?\nNow do the same with right-joining, inner-joining, and full-joining pop and ice.\n\n\n\n\n\nFauchald, Per, Taejin Park, Hans Tømmervik, Ranga Myneni, and Vera Helene Hausner. 2017. “Arctic greening from warming promotes declines in caribou populations.” Science Advances 3 (4): e1601365. https://doi.org/10.1126/sciadv.1601365.",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Joining data</span>"
    ]
  },
  {
    "objectID": "09_Joining_data.html#footnotes",
    "href": "09_Joining_data.html#footnotes",
    "title": "9  Joining data",
    "section": "",
    "text": "It can also do a lot more. You can check its help page (?join_by) after you finish reading this chapter—by that time, it will make much more sense.↩︎",
    "crumbs": [
      "Part I: Processing and visualizing data with R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Joining data</span>"
    ]
  },
  {
    "objectID": "21_Probabilities_and_distributions.html",
    "href": "21_Probabilities_and_distributions.html",
    "title": "10  Probabilities and probability distributions",
    "section": "",
    "text": "10.1 What is a probability?\nWithin statistics we want to make decisions that are influenced by uncertainty. For example how often do we expect to find a snail larger than the mean size? Depending on the snail we sample, we might find one larger than the mean or not. In order to understand this event we can use probabilities to inform us whether this event is something that occurs often or not. Probabilities are also useful in making decisions, for instance should I bring my umbrella with me when I leave in case it will rain? Probabilities are something us humans weigh and assess semi-consciously all the time when making decisions and statistics is just a formalization of these concepts with proper numbers.\nA probability is a representation (or model) of the chance that an event — for example “a snail is larger than the mean size of all snails” — occurs. It it our way of modelling a natural phenomenon that is impacted by randomness.\nThere are three common ways of measuring a probability:\nIn Section 7.3.2 we presented a way to show which values and how often they appear within a variable. This corresponds to the relative frequency definition, we calculate how often an event has occurred — a specific value of the variable has been observed — out of the total number of samples observed. This way of defining a probability is an approximation as the samples observed might not represent the population from which they are drawn from, but the law of large numbers proves that if the sample is big enough the relative frequencies are close to the theoretical probabilities.\nThese theoretical probabilities are calculated using the second definition. Using reasoning we can define a probability as the ratio between “the number of outcomes that fulfill the desired event” and the “total number of outcomes that are possible to occur”.\nThe third definition is not something that we commonly use for calculations, at least not in the areas of statistics covered by this book, but it is closely tied to the semi-conscious assessments we humans do on a regular basis. The probability of it raining and you deciding to take your umbrella with you is subjective in the sense that one person might look at the weather and think that the probability of rain is high whereas another person looking at the same weather might have other pieces of knowledge that makes them think the probability is low. This definition of probability is difficult to measure, so the remainder of this text will focus on the two earlier definitions.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probabilities and probability distributions</span>"
    ]
  },
  {
    "objectID": "21_Probabilities_and_distributions.html#what-is-a-probability",
    "href": "21_Probabilities_and_distributions.html#what-is-a-probability",
    "title": "10  Probabilities and probability distributions",
    "section": "",
    "text": "Using relative frequencies\nUsing reasoning\nUsing subjectivity\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nA probability must take a value in the range of 0 - 1. If calculations on probabilities result in a value outside this range, something has gone horribly wrong.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probabilities and probability distributions</span>"
    ]
  },
  {
    "objectID": "21_Probabilities_and_distributions.html#probability-distributions",
    "href": "21_Probabilities_and_distributions.html#probability-distributions",
    "title": "10  Probabilities and probability distributions",
    "section": "10.2 Probability distributions",
    "text": "10.2 Probability distributions\nSimilar to how we can show the distribution of an observed variable using relative frequencies (Section 7.3.2), we can use probabilities to show the probability distribution of a random variable. One important restriction of a probability distribution is that the events need to correspond to numerical values, e.g. the event needs to define the number of times a category occurs (discrete random variable) or a continuous random variable.\n\n10.2.1 Discrete distributions\nA common discrete probability distribution is the binomial distribution which counts “the number of successful trials from n independent trials with the same chance of success”.\nThe population of snails on Floreana Island of the Galápago contain seven species (as described in Section 4.2.2). We can define a successful trial as observing a snail of the Naesiotus calvus species and if we assume that the species is approximately 23.5% of the population, the random variable X counting the number of N. calvus snails observed in a sample of 10 snails is binomially distributed.\n\n\n\n\n\n\nWarning\n\n\n\nThe binomial distribution assumes that the trials are independent from one another in the sense that the probability of observing a snail of the specific species does not change regardless of the other trials. Whether or not we can assume independence is also partially connected to the way we conduct the study, or sample the snails in this case.\nIf the sample is drawn only from one specific area of the island — the small clearing outside the research station — there might exist some condition that increases or decreases the prevalence of the species. It can then be argued that the probability of a snail of the species change from the initial 23.5% to a higher or lower number depending on if the first snail observed is a Naesiotus calvus.\n\n\nWe can describe this variable as: \\[\nX \\sim Bin(n = 10, p = 0.235)\n\\] where n is the number of trials (the size of the sample), and p is the probability of a successful trial (observing a N. calvus snail). Calculating probabilities from a distribution is done using a density function. The probability of getting x successful trials is calculated using the function: \\[\nP(x) = \\binom{n}{x} p^x (1-p)^{n-x}\n\\]\nProbability distributions can also be summarized using a measure of center and spread but we call these by different names in this context. The expected value (E[X] or \\(\\mu\\)) describes the “center” of the distribution, e.g. the expected number of snails of the specific species in a random experiment with 10 trials. The variance (\\(\\sigma^2\\)) describes the spread around the expected value. Notice how we are using the population notation because that is what is described, not a sample of the random variable but the underlying population where the trials (samples) are drawn from.\nThe different events that can occur for X are each number of successes, from 0 to 10. The binomial distribution makes calculations of the probability for each event relatively easy using the density function, and with the help of R even more so. R has a range of functions for common distributions all containing four different types of calculations:\n\nd: calculates the density (probability) of a specific value of X, in the discrete case the probability P(x) = ?.\np: calculates the cumulative density (probability) up to or from a given value of X, e.g. P(X \\(\\le\\) x) = ?.\nq: calculates the value of X based on a probability, e.g. P(X \\(\\le\\) x) = 0.25 \\(\\rightarrow\\) x = ?, not covered for the Binomial distribution.\nr: returns a random sample from the distribution, not covered in this text.\n\nIn the binomial case the functions are called dbinom, pbinom, qbinom and rbinom, with other distributions following the same format.\n\n10.2.1.1 dbinom\nThe probability distribution of X can be presented in a table or a visualization. For a visualization, similar to the bar plots we have created earlier the x-axis should contain the different values the variable can be and the height of each bar correspond to the probability of each value. We can calculate the probability of each value with dbinom.\nThe dataset sent to ggplot contain the actual heights of the bars so we assign the y argument in aes to the probabilities and tell geom_bar to actually use the values provided by adding stat = \"identity\".\n\n# Defining the number of trials and probability of success\nn &lt;- 10\np &lt;- 0.235\n\nsnails &lt;- \n  tibble(\n    # Creates a vector with all possible values of x, from 0 to 10 (n)\n    x = 0:n,\n    # The arguments size and prob correspond to n and p\n    px = dbinom(x = x, size = n, prob = p)\n  )\n\nBy default, R handles the variable x as a continuous numeric variable no matter if the dataset only contains whole numbers, so for the sake of clarity we should adjust the x-axis to show each whole number. An easy way to do this is to convert the variable x to a factor inside the visualization as we then force R to show each category on the x-axis.\n\nsnails |&gt; \n  ggplot() + \n  aes(x = factor(x), y = px) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") + \n  labs(x = \"x\", y = \"P(x)\") +\n  theme_bw() \n\n\n\n\n\n\n\nFigure 10.1: Probability distribution of the number of N. calvus snails\n\n\n\n\n\nThe probability of observing three N. calvus snails in the sample is P(3) = dbinom(x = 3, size = 10, p = 0.235) = 0.2387893. The visualization below has highlighted this probability in red.\n\n\n\n\n\n\n\n\n\n\n\n10.2.1.2 pbinom\nSometimes we are interested in more than just a single value of x, for instance what is the probability of observing 3 or more N. calvus snails in our sample. This probability can be expressed as:\n\\[\nP(X \\ge 3) = P(3) + P(4) + P(5) + P(6) + P(7) + P(8) + P(9) + P(10)\n\\] We simply calculate the sum of the probabilities for the individual values that correspond to the inequality, highlighted by the bars in red below.\n\n\n\n\n\n\n\n\nFigure 10.2: Binomial distribution (Bin(n = 10, p = 0.235)) with P(X \\(\\ge\\) 3) highlighted.\n\n\n\n\n\nUsing visualizations we can understand what the probability actually depicts and learn when to use the properties of a probability distribution to our advantage.\n\n\n\n\n\n\nImportant\n\n\n\nAll probabilities of a probability distribution must sum to 1, otherwise it is not a probability distribution. This property allows us to make use of the complement of an event to simplify calculations.\n\n\nInstead of calculating P(X \\(\\ge\\) 3) (all the red bars) we can calculate 1 - P(X &lt; 3) (the blue bars) because all bars in the plot should sum to 1 and removing the bars corresponding to values less than 3 leaves us with the probability we are looking for.\n\\[\nP(X \\ge 3) = 1 - P(X &lt; 3) = 1 - [P(0) + P(1) + P(2)]\n\\] Now with the help of R, we do not necessarily have to use this type of technique to simplify the calculations — most modern computers can calculate relatively large probability calculations quickly — but it is always useful to know how to structure your calculations in case you run into issues. The total area of the red bars can be calculated either using the sum of individual dbinom function calls or simply by the function pbinom.\nInstead of an argument x that defines the value for which to calculate a density (as we saw in dbinom), we now make use of the q argument, specifying which quantile of the distribution we want to calculate from. However qbinom expresses its probability calculation as P(X \\(\\le\\) x) using the argument lower.tail = TRUE or P(X &gt; x) using lower.tail = FALSE.\nSince our inequality is “greater than or equal to 3” we need to adjust the value to conform to dbinom’s “greater than x”. In Figure 10.2 we can see another way to highlight the same red bars, namely P(X &gt; 2).\n\n# A vector of values as x provides the probabilities for each value\ndbinom(x = 3:10, size = 10, prob = 0.235) |&gt; \n  sum()\n\n[1] 0.4289805\n\n# Adding q = 3 would only calculate P(X &gt; 3) and not include P(3) in the sum\npbinom(q = 3, size = 10, prob = 0.235, lower.tail = FALSE)\n\n[1] 0.1901911\n\n# Rewriting our probability to P(X &gt; 2) provides the same answer as\n# the sum of individual probabilities\npbinom(q = 2, size = 10, prob = 0.235, lower.tail = FALSE)\n\n[1] 0.4289805\n\n\nThe probability of finding at least 3 snails of the N. calvus species is approximately 42.9%.\n\n\n\n10.2.2 Continuous distributions\nWhen the random variable is continuous, the number of unique values becomes infinitely many. The probability of getting an exact number down to the last decimal spot is infinitesimally small because the number of total outcomes is infinitely large, so we need to think about calculating probabilities a bit differently.\nInstead of calculating the probability of a specific value of X, we see it solely as a point on a function or curve. The density function describes the shape of the continuous distribution and probabilities are calculated by the area under the curve. Within mathematics this would be done by integrating the function between two set points, but the distributions used in statistics tend to have relatively complex functions hence the need of a computer program.\nA common continuous distribution is the normal distribution1. The name normal comes from the fact that the shape of the distribution is found in many different aspects of life when describing a continuous variable, especially within biological data.\nWe can define a new random variable Y as the size of the snail shell and assume that it is normally distributed as: \\[\nY \\sim N(\\mu_Y = 19.59, \\sigma_Y = 3.79)\n\\] where \\(\\mu_Y\\) is the mean of the variable Y and \\(\\sigma_Y\\) is the standard deviation of the variable Y.\n\n\n\n\n\n\nWarning\n\n\n\nIn this example we assume that the population of size is normally distributed but there is always an inherent risk with making assumptions. Usually assumptions are made after discussing with experts in the field and/or using previous research after which we have an idea whether it is reasonable to assume a specific distribution applies or not.\n\n\nAs we mentioned earlier the shape of the distribution is created using a density function. Instead of calculating P(x) we use the function f(x) to plot the line for every continuous value of x. The function for the normal distribution is: \\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n\\] where \\(\\pi\\) is the constant 3.1415, \\(\\mu\\) is the mean of the distribution and \\(\\sigma^2\\) is the variance of the distribution.\n\n10.2.2.1 dnorm\nWe can use the function dnorm to plot the curve of the distribution for Y. Similar to the binomial function, the arguments of dnorm include the parameters that we use to describe the distribution — the mean and the standard deviation.\n\nsnailDist &lt;- \n  tibble(\n    # Create a vector of the values of x as a sequence with 0.001 step size\n    x = seq(from = 5, to = 35, by = 0.001),\n    y = dnorm(x, mean = 19.59, sd = 3.79)\n  ) \n\n\nsnailDist |&gt; \n  ggplot() + \n  aes(x = x, y = y) +\n  # geom_line will draw a line between every point in the plot\n  geom_line(linewidth = 1) + \n  theme_bw() +\n  # Adds a line for the\n  geom_segment(\n    aes(\n      x = 19.59, xend = 19.59, \n      y = 0, yend = dnorm(19.59, mean = 19.59, sd = 3.79)\n    ),\n    color = \"steelblue\",\n    linewidth = 1,\n    # Make a striped line\n    linetype = 2\n  )\n\nWarning in geom_segment(aes(x = 19.59, xend = 19.59, y = 0, yend = dnorm(19.59, : All aesthetics have length 1, but the data has 30001 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nOne of the main properties of the normal distribution is its symmetry around the mean (striped blue line). If we move a specific number of x-steps to the right, the value of f(x) would be the same if we move the same number of x-steps to the left. Note that just because the plot is limited from 5 to 35 on the x-axis the distribution continues outside of the view window, just with very small densities.\n\n\n10.2.2.2 pnorm\nWe can use pnorm to simplify calculations that involve integration of areas under the curve. For instance, what is the probability of a randomly selected snail to have a shell size less than 17? Because we now have a continuous distribution there is no difference between P(Y &lt; 17) or P(Y \\(\\le\\) 17), it is only the direction of the inequality that is important. In the plot below, the red area highlights the probability we want to calculate.\n\n\n\n\n\n\n\n\n\nIn order to find this probability we need to define the quantile of the inequality as well as the mean and the standard deviation of the distribution. If we were to calculate this “by hand” without the use of R or another computer program, we would instead standardize the value and look for the probability in a table of the standardized normal distribution. We could also do this in R by standardizing the quantile using \\(z = \\frac{y - \\mu_Y}{\\sigma_Y}\\).\n\n# Calculates from the distribution of Y\npnorm(q = 17, mean = 19.59, sd = 3.79, lower.tail = TRUE)\n\n[1] 0.2471842\n\n# Calculates from the standardized distribution Z (default argument values)\npnorm(q = (17 - 19.59) / 3.79)\n\n[1] 0.2471842\n\n\nThe argument lower.tail acts the same as it does for the binomial distribution, except that we do not differentiate between &lt; and \\(\\le\\). If we were interested in the probability of a randomly selected snail to have a shell size of more than 17, it can be calculated as:\n\npnorm(q = 17, mean = 19.59, sd = 3.79, lower.tail = FALSE)\n\n[1] 0.7528158\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.2.3 Finding quantiles\nFor both of these distributions there might come a time where we need to calculate a quantile (value) from a given probability. In Section 6.3.1.3 we defined a quantile as a value of the variable that splits the data into a specific proportion, e.g. the 25th percentile splits the data where 25 percent of the data is lower and 75 percent of the data is higher than the value. When we now talk about quantiles, the value defines the split where a certain percentage of the distribution is lower or higher than the value.\nIn the discrete case, we might encounter some issues when finding specific quantiles as the discrete outcomes tend to contain a big part of the probability distribution. The solution is to find the smallest value of x fulfills the inequality with at least the given probability. We can phrase a question such as: At most how many N. calvus snails do we find in 25 percent of the experiments? This question can be written mathematically as P(X \\(\\le\\) x) = 0.25.\nFrom Figure 10.1 we see that P(0) only sums up to around 0.07 which is less than the probability we are looking for. Adding P(1) to the sum results in the cumulative probability being around 0.28 which is larger than the sought probability. Therefore the value of x = 1 is the smallest value of x that is at least the probability we are looking for.\n\n# The function asks for p, a probability, and returns a quantile\nqbinom(p = 0.25, size = 10, prob = 0.235)\n\n[1] 1\n\n\nFor continuous distributions we do not need to make these approximations as the quantile can take on infinite amount of values. Instead we simply look for the exact quantile (value) that corresponds to the given probability. For instance, what is the shell size for the largest 5 percent of the population? We want to find y from P(Y &gt; y) = 0.05 or using R:\n\nqnorm(p = 0.05, mean = 19.59, sd = 3.79, lower.tail = FALSE)\n\n[1] 25.824\n\n\n\n\n\n\n\n\n\n\n\nThis is conceptually similar to finding critical values that we will return to when introducing statistical inference.\nIn the following application (in Swedish), you can try to visualize different distributions and either ways to calculate probabilities or quantiles. On top of the normal and binomial distribution, the tool has additional distributions that we will discuss more in later chapters.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probabilities and probability distributions</span>"
    ]
  },
  {
    "objectID": "21_Probabilities_and_distributions.html#sec-probabilities-exercises",
    "href": "21_Probabilities_and_distributions.html#sec-probabilities-exercises",
    "title": "10  Probabilities and probability distributions",
    "section": "10.3 Summary and exercises",
    "text": "10.3 Summary and exercises\nWe can measure uncertainty with the help of probabilities and probability distributions allow us to asses how likely an event is to occur. In discrete distributions there is a distinct difference between inequalities using &lt; and \\(\\le\\), while in the continuous case the probability of a specific outcome is essentially 0 that there is no practical difference. We can use functions in R to both calculate probabilities of a specific outcome (e.g. dbinom), probabilities of inequalities (e.g. pbinom), or quantiles from a given probability (e.g. qbinom).\nFor the exercises we will return to the iris dataset seen in earlier chapters (e.g. Section 5.3). For these questions we assume that the prevalence of the species in a specific area are:\n\n\n\n \n  \n    species \n    prevalence \n  \n \n\n  \n    setosa \n    0.24 \n  \n  \n    versicolor \n    0.47 \n  \n  \n    virginica \n    0.29 \n  \n\n\n\n\nAssume that we pick 20 flowers at random in the specific area.\n\nWhat is the probability of finding exactly 10 flowers of the species setosa?\nWhat is the probability of finding more than 10 flowers of the species versicolor?\n\nFor the next questions assume that the petal length of the setosa species is normally distributed with a mean length of 1.46 cm and a standard deviation of 0.174 cm.\n\nWhat is the probability of finding a plant with a petal length of less than 1.1 cm?\nWhat is the probability of finding a plant with a petal length between 1.1 and 1.46 cm?\nWhich length must a plant be larger than to be considered among the largest 10 percent?",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probabilities and probability distributions</span>"
    ]
  },
  {
    "objectID": "21_Probabilities_and_distributions.html#footnotes",
    "href": "21_Probabilities_and_distributions.html#footnotes",
    "title": "10  Probabilities and probability distributions",
    "section": "",
    "text": "Also called the Gaussian distribution or a bell curve.↩︎",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probabilities and probability distributions</span>"
    ]
  },
  {
    "objectID": "22_Sampling_distributions.html",
    "href": "22_Sampling_distributions.html",
    "title": "11  Sampling distributions",
    "section": "",
    "text": "11.1 Simulation of the sampling distribution\nIn the previous chapter we have focused on probability distributions for a random variable that describes a characteristic of the observed unit. We can also consider a sample statistic — e.g. the sample mean — a random variable because the units the statistic is calculated from are drawn randomly from a population, thereby being affected by randomness. The probability distribution of all statistics calculated from all possible samples of the same size is called a sampling distribution.\nSampling distributions are a vital part of statistical inference — the process of interpreting the unknown population parameter based on the sample statistic — which we will begin covering in the next chapter. However we must first get an understanding of the properties of a sampling distribution and a couple of laws that can be used to determine the shape of them.\nThe sample mean \\(\\bar{X}\\) is an estimator1 of the population mean \\(\\mu\\) because it is used to estimate the value of the unknown parameter. Depending on the sample drawn, the sample mean will result in different values.\nSay we want to measure the weight of a population of 60 wolves located in Southern Sweden (adapted from Svensson et al. (2023)) by randomly sampling 5 wolves. For the purposes of this example, assume the population weights of all wolves are found in wolves2023.csv.\nwolves &lt;- read_csv(\"wolves2023.csv\")\n\nwolves |&gt; \n  ggplot() + \n  aes(x = weight) + \n  geom_histogram(\n    bins = 20,\n    fill = \"steelblue\", \n    color = \"black\"\n  ) + \n  theme_bw() +\n  scale_x_continuous(breaks = seq(5, 60, by = 10)) +\n  labs(x = \"Weight (kg)\", y = \"Count\")\nThe distribution of X — the weight — is skewed towards the higher values. We can also calculate the population mean from the given data to 32.407 kg.\nSo let us draw a sample from the population using the slice_sample function. The argument n controls how many units to sample and without any other additional arguments, we will draw the 5 units completely at random.\n# Draws a sample of size 5 \nwolvesSample &lt;- \n  wolves |&gt; \n  slice_sample(n = 5)\n\n# Visualizes the population distribution, its mean and the sample mean\nwolves |&gt; \n  ggplot() + \n  aes(x = weight) + \n  geom_histogram(\n    bins = 20,\n    fill = \"steelblue\", \n    color = \"black\"\n  ) + \n  theme_bw() +\n  scale_x_continuous(breaks = seq(5, 60, by = 10)) +\n  labs(x = \"Weight (kg)\", y = \"Count\") +\n  # Adds the population mean\n  geom_vline(\n    xintercept = mean(wolves$weight), \n    color = \"firebrick\", \n    linewidth = 1\n  ) + \n  # Adds the sample mean\n  geom_vline(\n    xintercept = mean(wolvesSample$weight), \n    color = \"black\", \n    linewidth = 1\n  )\nWe can see that the sample mean (black vertical line) overestimates the population mean (red vertical line) but if we were to draw another sample of the same size the sample mean would be a different value.\nLet us simulate — make the computer do a lot of work — a large number of samples of the same size. We can code this in various different ways but the pseudo code2 should look something like this:\nFirst we create a function of our own that draws a sample from the given data and returns the sample mean. This is step 1 and 2 of the pseudo code.\ndrawSample &lt;- function(wolves, n){\n  # Draw a sample of size n\n  sample &lt;- \n    wolves |&gt; \n    slice_sample(n = n)\n  \n  # Calculate sample mean\n  mean &lt;- \n    sample$weight |&gt; \n    mean()\n  \n  # Return the value\n  return(mean)\n}\nNext we need some way to replicate the function as many times as we want. This is step 3 of the pseudo code. The function replicate does just that, with n defining how many times to run the expression in the expr argument.\n# Runs the function drawSample 10 000 times and returns a vector of all sample means.\nsampleMeans &lt;- \n  replicate(\n    n = 10000,\n    expr = drawSample(wolves, n = 5)\n  )\n  \nsampleMeans |&gt; \n  tibble() |&gt; \n  ggplot() +\n  # Changes the y-axis to density instead of absolute frequencies\n  aes(x = sampleMeans) + \n  geom_histogram(\n    aes(y = after_stat(density)),\n    bins = 20, \n    fill = \"steelblue\", \n    color = \"black\"\n  ) + \n  theme_bw() +\n  # Adds the population mean\n  geom_vline(\n    xintercept = mean(wolves$weight), \n    color = \"firebrick\", \n    linewidth = 1\n  ) + \n  labs(x = \"Sample Mean\", y = \"Density\")\nThe shape of the distribution seems to be centered around the population mean (red line) with a relative symmetrical decline in frequency to the extreme weights. This looks very similar to a normal distribution, so let us draw the curve of the distribution on top of the histogram using the mean and standard deviation of the 10 000 samples as its properties.\nsampleMeans |&gt; \n  tibble() |&gt; \n  ggplot() +\n  aes(x = sampleMeans) + \n  geom_histogram(\n    aes(y = after_stat(density)),\n    bins = 20, \n    fill = \"steelblue\", \n    color = \"black\"\n  ) + \n  theme_bw() +\n  labs(x = \"Sample Mean\", y = \"Count\") +\n  # Adds the normal distribution curve\n  geom_function(\n    fun = dnorm, \n    color = \"firebrick\",\n    linewidth = 1,\n    args = list(mean = mean(sampleMeans), sd = sd(sampleMeans))\n  )\nThe normal distribution seems to mimic the shape of the histogram relatively well. There are some discrepancies in the symmetry as well as the position of the center. We can try and increase the sample size to 15.\nsampleMeans &lt;- \n  replicate(\n    n = 10000,\n    expr = drawSample(wolves, n = 15)\n  )\n\nsampleMeans |&gt; \n  tibble() |&gt; \n  ggplot() +\n  aes(x = sampleMeans) + \n  geom_histogram(\n    aes(y = after_stat(density)),\n    bins = 20, \n    fill = \"steelblue\", \n    color = \"black\"\n  ) + \n  theme_bw() +\n  labs(x = \"Sample Mean\", y = \"Count\") +\n  # Adds the normal distribution curve\n  geom_function(\n    fun = dnorm, \n    color = \"firebrick\",\n    linewidth = 1,\n    args = list(mean = mean(sampleMeans), sd = sd(sampleMeans))\n  )\nThe curve seems to follow the histogram a bit better with some minor discrepancies. We can increase the sample size once more to 30.\nAnd then again to 45.\nThis phenomenon we see — that the sampling distribution follows the normal distribution better with larger sample sizes — is due to the central limit theorem. The theorem states that:\nWhat constitutes as a large number is can be different in different scenarios, but in the case above we saw that when n &gt; 30, the histogram looked approximately normal. Most literature use n &gt; 30 as the rule of thumb when the central limit theorem can be applied.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Sampling distributions</span>"
    ]
  },
  {
    "objectID": "22_Sampling_distributions.html#simulation-of-the-sampling-distribution",
    "href": "22_Sampling_distributions.html#simulation-of-the-sampling-distribution",
    "title": "11  Sampling distributions",
    "section": "",
    "text": "Important\n\n\n\nNormally we do not have the possibility to calculate the population mean unless we conduct a census, that is gather information from all units in the population. We only do this for pedagogical purposes in this chapter to visualize the properties of the sampling distribution.\nIn practice we use the theories shown here as the basis for our next chapters where we only have information from a sample of the population.\n\n\n\n\n\n\n\n\nDraw a sample of size 5,\nCalculate the mean of the sample,\nRepeat 1-2 m times and save all sample means.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe sampling distribution is the distribution of all sample means from all possible samples of the same size. With 30 units in the population and sampling 5 units in a sample there exist choose(n = 30, k = 5) = 142506 possible samples that can be created. This is a lot even for the computer to run, so by repeating the sampling method a large number of times we can approximate how the sampling distribution should look like.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA sum (or mean) of a large number of independent and equally distributed variables will be approximately normal distributed.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Sampling distributions</span>"
    ]
  },
  {
    "objectID": "22_Sampling_distributions.html#sec-sampling-distribution-mean",
    "href": "22_Sampling_distributions.html#sec-sampling-distribution-mean",
    "title": "11  Sampling distributions",
    "section": "11.2 Sampling distribution of the mean",
    "text": "11.2 Sampling distribution of the mean\nIf the random variable X is not normally distributed we need to rely on the central limit theorem to use the normal distribution. If n &gt; 30 we can then state that:\n\\[\n\\bar{X} \\approx N\\left(\\mu_{\\bar{X}} = \\mu_X; \\sigma_{\\bar{X}} = \\frac{\\sigma_X}{\\sqrt{n}}\\right)\n\\tag{11.1}\\] where the mean of the sample mean (\\(\\mu_{\\bar{X}}\\)) is the same as the mean of X, and the standard deviation of the sample mean (\\(\\sigma_{\\bar{X}}\\)) is the standard deviation of X divided by the root of the sample size n.\nWithin biological data the sample sizes tend to be small, most of the times n &lt; 30. This means that the central limit theorem cannot be applied and we cannot assume that the sampling distribution of \\(\\bar{X}\\) is approximately normal distributed. Instead we would need to rely on properties of linear combinations.\nA sum or mean of independent random variables, \\(X_1\\)-\\(X_n\\), are examples of linear combinations that keep many of the properties from the original variables. For instance if X follows the normal distribution, the mean of X will also follow the normal distribution regardless of how big the sample is. The same mean and standard deviation as in Equation 11.1 apply and \\(\\bar{X}\\) now follows an exact normal distribution.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Sampling distributions</span>"
    ]
  },
  {
    "objectID": "22_Sampling_distributions.html#exercises",
    "href": "22_Sampling_distributions.html#exercises",
    "title": "11  Sampling distributions",
    "section": "11.3 Exercises",
    "text": "11.3 Exercises\nFor the exercises we will return to the iris dataset seen in earlier chapters (e.g. Section 5.3). For the next questions assume that the petal length of the setosa species is normally distributed with a mean length of 1.46 cm and a standard deviation of 0.174 cm.\n\nWhat is the probability of finding the mean petal length of 10 setosa plants less than 1.1 cm?\nWhat is the probability of finding the mean petal length of 10 setosa plants between 1.1 and 1.46 cm?\nHow does the answers to these questions differ from question 3 and 4 in Section 10.3? What law is this evidence of?\n\n\n\n\n\nSvensson, Linn, Petter Wabakken, Erling Maartmann, Kristoffer Nordli, Øystein Flagstad, Anna Danielsson, Henrikke Hensel, Katarina Pöchhacker, and Mikael Åkesson. 2023. Inventering av varg vintern 2022-2023. Bestandsstatus for Store Rovdyr i Skandinavia;1-2023. Rovdata (NINA) og SLU Viltskadecenter. https://hdl.handle.net/11250/3068933.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Sampling distributions</span>"
    ]
  },
  {
    "objectID": "22_Sampling_distributions.html#footnotes",
    "href": "22_Sampling_distributions.html#footnotes",
    "title": "11  Sampling distributions",
    "section": "",
    "text": "The term statistic refers to the summarized measurement of a sample which is calculated by an estimator.↩︎\nA skeleton for what we want to do with code.↩︎",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Sampling distributions</span>"
    ]
  },
  {
    "objectID": "23_Statistical_inference.html",
    "href": "23_Statistical_inference.html",
    "title": "12  Introduction to statistical inference",
    "section": "",
    "text": "12.1 Confidence intervals\nIn the previous chapters we have extensively covered methods that describe a dataset in one way or another, either by summarizing a data using its measures of center and spread or visualizing its distribution using an appropriate figure. All of these methods focus only on describing the dataset collected which might not always what we want to use as the basis for making decisions. It is commonplace to use a random sample as a representation of the underlying population, but describing the sample does not account for the uncertainty that the random sampling creates.\nStatistical inference are methods that we can use to infer something about the population based on a smaller sample. These methods take into account the inherent uncertainty within a sample to either provide a — hopefully informative — interval of values where we expect a parameter to be within or a conclusion about specific values of a parameter. We will in this chapter focus on investigating one population or group with the help of two types of inference, parametric and non-parametric methods. The main difference between the two is whether or not we assume that the sample statistic has a known distribution or not. Non-parametric methods are also sometimes called distribution free methods to specifically describe that difference.\nWithin statistical inference we can focus on methods relating to the measure of center, such as the mean (\\(\\mu\\)) or median, for a quantitative variable or methods relating to a proportion (\\(p\\)) for a qualitative variable. Whether we want to calculate an interval or test hypotheses, using a parametric method needs to fulfill some criteria in order for the results to be a trustworthy.\nThe goal is to ascertain more information about the unknown parameter either by using a confidence interval or hypothesis test using the information from the sample. Confidence intervals produces a range of values which we assume with a degree of confidence covers the parameter value while hypothesis tests can either falsify or not falsify a hypothesis we consider being true about the parameter.\nIn Chapter 11 we saw how the values of a statistic were impacted by the sample they were calculated from, producing a distribution of all the statistics that is centered on the population parameter. In practice we only draw one sample but we can make use of the shape of the sampling distribution to account for the uncertainty.\nThere exist many different confidence intervals for many different parameters but the main structure is always the same. The center point of the interval is the point estimate from the sample while the width of the interval is calculated using the margin of error.\n\\[\n\\begin{aligned}\n\\text{Point Estimate } \\pm \\text{ Margin of Error}\n\\end{aligned}\n\\tag{12.1}\\] The margin of error takes two things into account; first the uncertainty of the sampling distribution, and second the level of confidence of the interval. The confidence level is a proportion that determines how confident we are that the true parameter is covered by the interval and if we were to draw all the possible samples from the population, the level describes the proportion of intervals that actually covers the true parameter.1\nWhile the confidence level tries to explain a concept of convergence, in practice we usually only draw one sample which creates some difficulty in interpreting a confidence interval. We can say that an interval can either cover or not cover the true parameter but that’s not informative so we instead phrase our interpretation as “the interval covers the true parameter with the given confidence level”. If we want to be absolute certain that the interval covers the true parameter the level of confidence would be 100%, which results in an interval that also covers the entirety of the sample space, \\(-\\infty \\le \\text{parameter} \\le \\infty\\). This does not give us any information at all about the parameter so we tend to use confidence levels of 90%, 95% or 99%.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to statistical inference</span>"
    ]
  },
  {
    "objectID": "23_Statistical_inference.html#sec-hypothesis-testing",
    "href": "23_Statistical_inference.html#sec-hypothesis-testing",
    "title": "12  Introduction to statistical inference",
    "section": "12.2 Hypothesis testing",
    "text": "12.2 Hypothesis testing\nStatistical hypothesis testing takes its inspiration from philosophy, namely the hypothetico-deductive model. The method describes a structure of creating hypotheses with connected evidence that would follow if the hypothesis is true and evidence that would follow if the hypothesis is false. Given this premise we can then use observations to conclude whether or not the hypothesis is falsified or not.\nThe process of testing statistical hypotheses uses this theory to shape different steps we can use for any and all types of hypothesis testing. If we know which parameter or type of conclusion we want to make, we can decide to use a specific hypothesis test and follow the following steps:\n\nCheck if the test’s assumptions are fulfilled\nFormulate a null (\\(H_0\\)) and alternative (\\(H_1\\)) hypothesis\nDefine the acceptable risk of making a wrong decision.\nObserve evidence in collected data\nDetermine if the evidence falsifies the null hypothesis or not\nDraw conclusions and interpret the results\n\n\n\n\n\n\n\nNote\n\n\n\nThese steps assume that data concerning the problem has been collected but in some studies we might actually start with formulating hypotheses in order to guide us to what type of data we need to collect.\n\n\nMost of these steps are more philosophical than mathematical in nature, in fact it is only within step 4 and 5 we actually make use of mathematical formulas. This means that we will be limited in the use of tools — such as a calculator, R or any other computer software — for only these specific steps and we need to make sure that we are using correct data and the correct formulas for the given problem as well as interpret them in the correct manner.\n\n12.2.1 Step by step process\nThis sub-chapter will describe each of the steps in the procedure a bit more in detail while still focusing on the general way of thinking. Explicit descriptions of the procedure in relation to specific parameters will be given later in this chapter.\n\n12.2.1.1 Checking assumptions\nFor every method of inference there are usually some assumptions that must be fulfilled in order for the method to actually produce reliable results. These assumptions can be anything from the variable type or scale to specific distributions the statistic must follow. It is not only important to state a given method’s assumption but actually check that they are fulfilled so the results are presented in its proper context and gives some weight that they are reliable to make decisions from.\n\n\n12.2.1.2 The hypotheses\nThe null hypothesis is the currently considered truth — or status quo — which we want to falsify while the alternative hypothesis is, as the name implies, the alternative to it describing something else. One thing to keep in mind when formulating these hypotheses is that the null hypothesis can never be proven directly, it can only be falsified or not. “Not falsifying” a null hypothesis is philosophically not the same as “proving” it, even though looking at the language of the two words one can assume they are equivalent.\nIn contrast to the philosophical theory, statistical hypotheses require a quite strict formulation, most often directly related to specific mathematical values. We also restrict the two hypotheses to cover all possible outcomes, which means we cannot use one set of hypotheses to test multiple different alternatives.\n\n\n12.2.1.3 The risk of making a wrong decision\nSimilar to the confidence level from Section 12.1, we will never be able to conclude the correct hypothesis with absolute certainty so we need to take into account some risk.\nThe significance level (\\(\\alpha\\)) — calculated as 1 - confidence level — describes the risk of rejecting a true null hypothesis. It is commonly set to 0.01, 0.05 or 0.10, which means we are willing to accept a 1%, 5% or 10% risk of rejecting the null hypothesis when it is actually true.\nThis is only one of the types of errors we can make, called the Type I error, the other being the failure to reject a null hypothesis that is in fact false, the Type II error. We can visualize the four possible scenarios in a table:\n\n\n\nDecision  Truth\n\\(H_0\\) True\n\\(H_0\\) False\n\n\n\n\nReject \\(H_0\\)\nType I error\nCorrect\n\n\nFail to Reject \\(H_0\\)\nCorrect\nType II error\n\n\n\nThere exists a trade-off between the two types of errors, when the risk for Type I error is small the risk for Type II error is large and vice-versa. When we decide the significance level for a test we need to decide which of these risks are more important to minimize, using \\(\\alpha = 0.01\\) results in a higher risk of Type II error, while using \\(\\alpha = 0.10\\) results in a comparatively lower risk of Type II error. In many fields the use of \\(\\alpha = 0.05\\) gives an overall minimal risk for both errors.\n\n\n12.2.1.4 The collected evidence\nThe data we collect should be viewed as objective as possible. Within statistics this step includes a measurement of how far away from the null hypothesis the data is located, with larger values indicating we have observed evidence that falsifies that hypothesis.\n\n\n12.2.1.5 The decision boundary\nUsing the significance level we can determine a boundary for which we conclude that the collected data is far enough away to determine we have strong evidence to falsify the null hypothesis. We can do this in two ways; through a quantile value from the same scale as the measurement of the data’s distance from the null hypothesis, or a probability of collecting a sample with data even further away — more extreme — from the null hypothesis.\nThe second alternative is more commonly known as p-values which we compare directly to the significance level. If the probability of collecting a sample that is more extreme is lower than the risk of Type I error, we conclude that the null hypothesis has been falsified. If the probability of collecting a sample that is more extreme is higher than the risk of Type I error, we conclude that we do not have strong enough evidence to falsify the null hypothesis.\n\n\n12.2.1.6 Conclusions and interpretations\nIf we falsify the null hypothesis we have found evidence for the alternative, while if we do not falsify the null hypothesis it does not mean we have found evidence for the null. This distinction is very important and must be used when interpreting the results.\nIn order to “prove” a null hypothesis we would need to conduct multiple hypothesis tests — each with a new random sample — that all fail to reject the same null hypothesis, thereby giving corroborating evidence for the same hypothesis.\n\n\n\n\n\n\nTipExample of hypothesis testing\n\n\n\n\nAssume we are Aristotle in Ancient Greece and want to disprove the current thinking that the Earth is flat. This could be one way to set up a hypothesis test to investigate that statement.\n\nAssumptions\n\nThis specific hypothesis test does not have any clear assumptions but we could argue that a simple assumption is unbiased data collection.\n\nFormulate a null and alternative hypothesis:\n\nNull Hypothesis (\\(H_0\\)): The Earth is flat.\nAlternative Hypothesis (\\(H_1\\)): The Earth is not flat.\nTo each of these hypotheses there are experiments that can be set up which has expected evidence to support them. For example we could hold a strong light 10 meters over sea level on a big field and measure at which height above sea level we locate the light in our binoculars at different distances from the light source.\nIf the earth was flat we would expect the light to be seen at the same height regardless of the distance from the source, while if the earth was not, we would expect it to be visible only at different heights when the distance changes.\n\nDefine the acceptable risk of making a wrong decision:\n\nFor this experiment we can define some form of height margin that we assume comes from measurement errors leading to us not being able to falsify the null hypothesis.\n\nObserve evidence in collected data:\n\nSet up the experiment with the light source and measure the height above sea level where we locate the source in our binoculars at different distances.\n\nDetermine if the evidence falsifies the null hypothesis or not:\n\nUsing the margin described in step 3, we can calculate the actual height differences that we require to be observed for us to determine if we have falsified the null hypothesis or not.\n\nDraw conclusions and interpret the results:\n\nIf the evidence strongly supports the alternative hypothesis and the null hypothesis is rejected, we conclude that the Earth is not flat.\nIf the evidence does not strongly support the alternative hypothesis, we fail to reject the null hypothesis, meaning we do not have enough evidence to conclude that the Earth is not flat.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to statistical inference</span>"
    ]
  },
  {
    "objectID": "23_Statistical_inference.html#exercises",
    "href": "23_Statistical_inference.html#exercises",
    "title": "12  Introduction to statistical inference",
    "section": "12.3 Exercises",
    "text": "12.3 Exercises\nAssume a small population where the true parameter value is 25. We measure all four (4) possible samples of a set size from this population and measure the following confidence intervals:\n\\[\n\\begin{aligned}\n5 \\le \\text{parameter} \\le 20\\\\\n5 \\le \\text{parameter} \\le 65\\\\\n5 \\le \\text{parameter} \\le 65\\\\\n10 \\le \\text{parameter} \\le 65\n\\end{aligned}\n\\]\n\nWhat is the confidence level of the interval?\nWhat is the level of type 1 error of the interval and how is it shown in practice?\n\nAssume some new information has come to light, for example time has passed or other external forces has caused a shift in the population, that casts doubt on the value of the true parameter. Let us take a sample from this new population and do some investigating.\n\nFormulate with words a null and alternative hypothesis that can be used to investigate the value of the parameter.\nWhat evidence would we expect to see if the null and alternative hypothesis respectively would be true?\nExplain what the type I and type II errors would mean in practice, given the hypotheses formulated in 3.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to statistical inference</span>"
    ]
  },
  {
    "objectID": "23_Statistical_inference.html#footnotes",
    "href": "23_Statistical_inference.html#footnotes",
    "title": "12  Introduction to statistical inference",
    "section": "",
    "text": "You can look through this interactive visualization of confidence intervals that continuously draws samples and calculates intervals from the same population where we know the true mean.↩︎",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to statistical inference</span>"
    ]
  },
  {
    "objectID": "24_Statistical_inference_in_R.html",
    "href": "24_Statistical_inference_in_R.html",
    "title": "13  Statistical inference in R",
    "section": "",
    "text": "13.1 Inference of a mean\nTaking the theoretical framework defined in Chapter 12 we will in this chapter look at how we apply it in practice. Beginning with the parametric methods we are usually interested in making inference about the measure of center, either the mean of a continuous variable or a proportion of a categorical variable.\nFor inference about a mean, the following assumptions must be met:",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Statistical inference in R</span>"
    ]
  },
  {
    "objectID": "24_Statistical_inference_in_R.html#inference-of-a-mean",
    "href": "24_Statistical_inference_in_R.html#inference-of-a-mean",
    "title": "13  Statistical inference in R",
    "section": "",
    "text": "independence in the sample,\n\ncan be fulfilled by using a Simple Random Sample (SRS).\n\nthe sample mean, \\(\\bar{X}\\), must be considered normally distributed,\n\ncan be fulfilled using the Central Limit Theorem (CLT) if the sample is sufficiently large, where the rule of thumb is \\(n &gt; 30\\). CLT states that a mean (or sum) of identically distributed variables will be approximately normally distributed in this case.\nif the sample is small, then the variable of interest, \\(X\\), must be considered normally distributed. This means that the transformation applied to the mean will also be normally distributed.\n\n\n\n13.1.1 Confidence interval of a mean\nUsing the sample distribution of the mean from Section 11.2 and the structure from Equation 12.1, the confidence interval of a mean is calculated as:\n\\[\n\\begin{aligned}\n  \\bar{x} \\pm t_{\\alpha/2; n-1} \\cdot \\frac{s}{\\sqrt{n}}\n\\end{aligned}\n\\] where \\(\\frac{s}{\\sqrt{n}}\\) is the standard error of the mean — the uncertainty of the sampling distribution — and \\(t_{\\alpha/2; n - 1}\\) is a quantile from the t-distribution which corresponds to the level of confidence (\\(1 - \\alpha\\)). \\(\\alpha / 2\\) indicates that we want a two-sided interval where the significance is split equally in both ends of the distribution. Together the two parts form the margin of error.\nFor one-sided intervals we denote \\(\\alpha\\) or \\(1 - \\alpha\\) for lower limit and upper limit intervals respectively to either give a negative or positive quantile.\n\n\n\n\n\n\nImportant\n\n\n\nThe t-distribution is used to solve similar problems to the normal distribution when the sample size is small and the population standard deviation is unknown.\nIn these cases we need to estimate both the mean and standard deviation to make inference about the mean, resulting in an estimate being used within another estimate. This added uncertainty is handled by a property within the t-distribution called the degrees of freedom.\nThe degrees of freedom describes the number of independent bits of information — in this case is n - 1 — which simplified can be described as the number of observations (n) subtracted by the number of additional estimates (1, the standard deviation) needed for the method.\nOne of the main properties of the t-distribution is that it converges to the normal distribution with large enough degrees of freedom.\n\n\n\n\n\nThe t-distribution with different degrees of freedom\n\n\n\n\n\n\nWithin R we can calculate the interval of a mean using the function t.test(). This function does much more than only the interval which we will return to in the next section. From the resulting object we can extract the confidence interval using $conf.int.\n\nobject &lt;- t.test(snailDat$size)\n\nobject$conf.int\n\n[1] 19.08906 20.08912\nattr(,\"conf.level\")\n[1] 0.95\n\n\nWe would interpret this result as; the interval 19.09 to 20.09 covers the true mean size of the shell sizes for all snails with 95 percent confidence.\nIf we want to calculate a confidence interval with a different confidence level than the default 0.95 (95%) we can adjust the argument conf.level.\n\nt.test(snailDat$size, conf.level = 0.99)$conf.int\n\n[1] 18.92986 20.24832\nattr(,\"conf.level\")\n[1] 0.99\n\n\n\n\n13.1.2 Hypothesis testing of a mean\nWhen formulating our hypotheses for a mean, we need to transform what we want to investigate into mathematical expressions. We also need to ensure that we use the correct mathematical symbol in the alternative hypothesis for the test to be conducted correctly depending on if we are interested in a two- or one-sided test.\n\n\n\n\n\n\nImportant\n\n\n\nOne of the most important things to remember when formulating statistical hypotheses is that the equal sign (\\(=\\)) always should be placed in the null hypothesis. What this means in practice is that the null hypothesis is describing a value that is assumed to be true in the population and the evidence found in the sample is calculated using the null hypothesis as the “truth”.\nIf we want the two hypotheses to cover all possible outcomes in a one-sided test, we can formulate the null hypothesis to “less than or equal” (\\(\\le\\)) or “greater than or equal” (\\(\\ge\\)), which both include the \\(=\\) in its denotation.\n\n\nFor example, if we were interested in investigating whether the mean size of snail shells is larger than 19, we would set up hypotheses as follows:\n\\[\\begin{align*}\n  &H_0: \\mu \\le 19 \\\\\n  &H_1: \\mu &gt; 19\n\\end{align*}\\]\nWe now make use of the full functionality of the t.test() function but need to take some time to adjust some of the arguments. Looking at the documentation we can focus on the following arguments of interest:\n\nx: specifies the variable we want to investigate,\nalternative: specifies the type of alternative hypothesis to be tested, with values “two.sided”, “less”, or “greater”,\nmu: specifies the value we assume the mean has in the null hypothesis\n\nIn accordance with our one-sided alternative hypothesis and the value 19, the function we write in R would be as follows:\n\nt.test(x = snailDat$size, \n       alternative = \"greater\",\n       mu = 19)\n\n\n    One Sample t-test\n\ndata:  snailDat$size\nt = 2.3217, df = 222, p-value = 0.01058\nalternative hypothesis: true mean is greater than 19\n95 percent confidence interval:\n 19.16999      Inf\nsample estimates:\nmean of x \n 19.58909 \n\n\nThe output contains a lot of information in a seemingly unstructured manner which is one of the disadvantages of R, the outputs are not always as clearly structured like other software.\nLet us look at the output row by row below the title of the output:\n\nThe first row provides information about which variable the test is performed on.\nThe second row provides information about the actual test where:\n\nt is the computed test statistic,\ndf is the degrees of freedom in the t-distribution,\np-value is the p-value of the test.\n\nThe third row provides information about the type of alternative hypothesis that is being tested.\nThe fourth and fifth rows provide a calculated confidence interval defined by the alternative argument\nThe last rows in the output provide descriptive statistics from the sample.\n\nTo make a decision from this test, we can directly compare the calculated p-value from the output with the specified significance level. If the p-value is lower, we can reject \\(H_0\\), otherwise, we cannot reject it.\n\n\n13.1.3 Visualizing the hypothesis testing\nWe can also make use of visualizations to determine the decision to be made. Assuming the null hypothesis to be “true”, the standardized sampling distribution of the sample mean would look as follows:\n\n# Sample information\nn &lt;- nrow(snailDat)\nmean &lt;- mean(snailDat$size)\nsd &lt;- sd(snailDat$size)\n\n# Create data frame for plotting\ndata &lt;- data.frame(\n  x = seq(-3.5, 3.5, length.out = 1000),\n  y = dt(x, df = n - 1)\n)\n\n# Plot the distributions\nggplot(data, aes(x = x, y = y)) +\n  geom_line(linewidth = 1, color = \"steelblue\") +\n  labs(x = \"x\", y = \"Density\") +\n  theme_bw() \n\n\n\n\nThe t-distribution under the null hypothesis\n\n\n\n\nThe distribution is centered on the standardized mean, 0, and the distance from 0 indicates how many standard deviations away from the hypothesized mean the evidence in the sample is located. The test statistic from the output, 2.3217, shows that the sample drawn had a larger mean shell size than the null hypothesis — a positive value indicates the sample is larger than the null mean, a negative larger value indicates the sample is smaller than the null mean.\nWe can add the test statistic to the visualization with:\n\nggplot(data, aes(x = x, y = y)) +\n  geom_line(linewidth = 1, color = \"steelblue\") +\n  labs(x = \"x\", y = \"Density\") +\n  theme_bw() + \n  geom_vline(xintercept = 2.3217, color = \"firebrick\")\n\n\n\n\nThe null distribution with the test statistic marked in red\n\n\n\n\nWe can see that the test statistic is placed quite a bit away from the expected outcome that should arise if the null hypothesis was true and the next step is to determine whether or not this is far enough away from the null mean to say we have falsified the null hypothesis or not.\nThis is where we make use of the p-value. As defined in Section 12.2.1.5, the p-value is a probability that we would find a sample with a more extreme sample mean than the current. The direction of “extreme” is determined by inequality sign in the alternative hypothesis, in this case \\(&gt; 19\\), leading to the probability calculated as \\(P(t &gt; 2.3217) = 0.01058\\).\nWe then compare the p-value to the level of significance to determine if the evidence is far enough away to reject the null hypothesis. The level of significance is placed on the same side of the distribution that the alternative hypothesis defines as “more extreme”, in this case the positive end.\n\n# Defines the significance level\nalpha &lt;- 0.05\n\n# Calculates the quantile value that corresponds to the level of significance\ncritValue &lt;- qt(p = alpha, df = n - 1, lower.tail = FALSE)\n\nggplot(data, aes(x = x, y = y)) +\n  geom_line(linewidth = 1, color = \"steelblue\") +\n  labs(x = \"x\", y = \"Density\") +\n  theme_bw() +\n  geom_area(data = subset(data, x &gt;= critValue), fill = \"black\", alpha = 0.5) +\n  geom_area(data = subset(data, x &gt;= 2.3217), fill = \"firebrick\", alpha = 0.8)\n\n\n\n\nHighlighting the p-value with the null distribution\n\n\n\n\nIn this case the p-value (red) is smaller than the significance level (black) resulting in the conclusion that we can reject \\(H_0\\).",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Statistical inference in R</span>"
    ]
  },
  {
    "objectID": "24_Statistical_inference_in_R.html#inference-of-a-proportion",
    "href": "24_Statistical_inference_in_R.html#inference-of-a-proportion",
    "title": "13  Statistical inference in R",
    "section": "13.2 Inference of a proportion",
    "text": "13.2 Inference of a proportion\nA proportion is a special case of a mean if we assume that the variable of interest only contain 1s and 0s where 1 denotes a unit with a specific characteristic. Unfortunately we cannot use the assumptions for a mean as the proportion can only take on values between 0 and 1, whereas a mean can be any real value. We must instead look at inference of a proportion as its own type of method with its own assumptions.\nThere are three main ways to make inferences about a proportion; using the binomial distribution of the count of values with the characteristic, approximating the distribution with the normal distribution using the Wald method, or calculating the absolute minimum and maximum proportions that could have been measured in the sample.\nFor inference about proportions using the Wald approximation, the following requirements must be met:\n\nindependence in the sample,\n\ncan be fulfilled by using conducting a Simple Random Sample (SRS).\n\nthe variable must be binomial distributed,\n\nthe variable must calculate the number of successes from n independent trials.\n\nthe sample proportion, \\(\\hat{p}\\), must be considered normally distributed,\n\nspecifically \\(n\\cdot \\hat{p} &gt; 5\\) and \\(n\\cdot \\hat{q} &gt; 5\\) where \\(\\hat{p}\\) is the probability of a success and \\(\\hat{q}\\) is \\(1 - \\hat{p}\\).\n\n\n\n13.2.1 Confidence interval of a proportion\nSimilar to t.test() we can create confidence intervals in R using prop.test().\n\n\n\n\n\n\nNote\n\n\n\nNote that prop.test() uses another variant of the Wald interval with additional continuity corrections, called a Wilson interval (Wilson (1927)), so the comparison to the Wald formula is not entirely identical.\n\n\nFor example, we are interested in making inference about the proportion of snails that live in an arid habitat. The two different intervals would be calculated as:\n\n## Wald Interval\nn &lt;- nrow(snailDat)\np &lt;- (snailDat$habitat == \"arid\") |&gt; sum() / n\n\nse &lt;- sqrt(p * (1-p) / (n-1))\n\nz &lt;- qnorm(p = .975)\n\np - z * se \n\n[1] 0.4409603\n\np + z * se\n\n[1] 0.5724926\n\n## Wilson Interval\nobject &lt;- \n  snailDat$habitat |&gt; \n  table() |&gt; \n  prop.test(correct = FALSE)\n\nobject$conf.int\n\n[1] 0.4415519 0.5716732\nattr(,\"conf.level\")\n[1] 0.95\n\n\nprop.test() makes use of table() to create a frequency table of the two categories where the first is used as the category of interest to create the interval for. If there are more than two categories in the variable of interest, we first must create a new variable that returns one value if the observation is the one sought and another if it is not.\n\n## Using table()\nsnailDat$habitat |&gt; \n  table()\n\n\n arid humid \n  113   110 \n\n## Creating a new variable and then table()\nsnailDat |&gt; \n  mutate(arid = \n           if_else(\n             habitat == \"arid\", \n             true = \"yes\", \n             false = \"no\") |&gt; \n           # Converts to a factor with manual order of levels\n           factor(levels = c(\"yes\", \"no\"))\n         ) |&gt; \n  select(arid) |&gt; \n  table()\n\narid\nyes  no \n113 110 \n\n\n\n13.2.1.1 Using the binomial distribution\nWe can also make use of the binomial distribution to create the confidence interval if we cannot fulfill the assumption that the statistic is normally distributed.\n\nobject &lt;- \n  snailDat$habitat |&gt; \n  table() |&gt; \n  binom.test()\n\nobject$conf.int\n\n[1] 0.4391590 0.5741128\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\n\n\n\n13.2.2 Hypothesis test of a proportion\nSimilar to the hypothesis testing of a mean, we can formulate hypothesis that investigate specific values of a proportion.\nFor example we could be interested in whether or not the majority of snails live in an arid environment. The term “majority” means more than 50% and because this statement does not include an equal sign, it needs to be formulated in the alternative hypothesis.\n\\[\n\\begin{aligned}\n  &H_0: p \\le 0.50 \\\\\n  &H_1: p &gt; 0.50\n\\end{aligned}\n\\]\nWe now make use of the full functionality of the prop.test() function but need to take some time to adjust some of the arguments. Looking at the documentation we can focus on the following arguments of interest:\n-x: specifies the number of successes (e.g., defective items), -n: specifies the number of trials (e.g., total items), -alternative: specifies the type of alternative hypothesis to be tested, with values “two.sided”, “less”, or “greater”, -p: specifies the value we assume the proportion has in the null hypothesis.\nIn accordance with our one-sided alternative hypothesis and the value 0.50, the function we write in R would be as follows:\n\nsnailDat$habitat |&gt; \n  table() |&gt; \n  prop.test(alternative = \"greater\", p = 0.50, correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  table(snailDat$habitat), null probability 0.5\nX-squared = 0.040359, df = 1, p-value = 0.4204\nalternative hypothesis: true p is greater than 0.5\n95 percent confidence interval:\n 0.451908 1.000000\nsample estimates:\n        p \n0.5067265 \n\n\nThe output contains a lot of information in a seemingly unstructured manner which is one of the disadvantages of R, the outputs are not always as clearly structured like other software.\nLet us look at the output row by row below the title of the output:\n\nThe first row provides information about the data and the null proportion being investigated.\nThe second row provides information about the actual test where:\n\nX-squared is the computed test statistic,\ndf is the degrees of freedom,\np-value is the p-value of the test.\n\nThe third row provides information about the type of alternative hypothesis that is being tested.\nThe fourth and fifth rows provide a calculated confidence interval defined by the alternative argument.\nThe last row provides the sample proportion.\n\nTo make a decision from this test, we can directly compare the calculated p-value from the output with the specified significance level. If the p-value is lower, we can reject \\(H_0\\), otherwise, we cannot reject it.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Statistical inference in R</span>"
    ]
  },
  {
    "objectID": "24_Statistical_inference_in_R.html#exercises",
    "href": "24_Statistical_inference_in_R.html#exercises",
    "title": "13  Statistical inference in R",
    "section": "13.3 Exercises",
    "text": "13.3 Exercises\nDavid Lack (Lack et al. 1945) conducted analyses of information about finches on the Galápagos Islands during the first half of the 20th century, using data collected by the California Academy of Sciences during their expeditions from 1905-06. The dataset darwin-finches.csv contains a subset of the collected information from four species within the genus Geospiza. The six variables are:\n\nisland: Abbreviation of the island where the specimen was examined;\nspecies: Genus and species of the specimen;\nsex: F or M;\nbeak_height: height of the bird’s beak (mm);\nupper_beak_length: length of the upper part of the bird’s beak from the base to the tip (mm);\nnostril_upper_beak_length: similar to upper_beak_length but measures from the nostril to the tip of the beak (mm).\n\nEnsure that the dataset appears as expected in the R environment before proceeding with the exercises!\n\nConstruct a 95% confidence interval for the proportion of Geospiza magnirostris in the population using the normal approximation. Is the method suitable for this data? Interpret the interval.\nInvestigate whether the proportion of Geospiza magnirostris is significantly greater than 10%. Calculate the p-value for the test using the normal and binomial distribution to draw your conclusions. Compare the two p-values and discuss the practical difference between the two methods and their conclusion.\nInvestigate whether the average beak height among Geospiza magnirostris is significantly different from 19.5 mm.\nInvestigate whether the average beak height among Geospiza magnirostris is significantly less than 19.5 mm.\nCalculate two confidence intervals that examines the same hypotheses as in c) and d) respectively. What is the practical difference between the two interpretations of the intervals?\n\n\n\n\n\nLack, D., E. C. Van Dyke, R. T. Orr, R. H. Alden, J. D. Ifft, and Berkeley. Museum of Vertebrate Zoology University of California. 1945. The Galapagos Finches (Geospizinae) a Study in Variation, by David Lack. Early Naturalists in the Far West, nos. 19-22. California academy of sciences. https://books.google.se/books?id=gISquwEACAAJ.\n\n\nWilson, Edwin B. 1927. “Probable Inference, the Law of Succession, and Statistical Inference.” Journal of the American Statistical Association 22 (158): 209–12. https://doi.org/10.1080/01621459.1927.10502953.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Statistical inference in R</span>"
    ]
  },
  {
    "objectID": "25_Statistical_inference_two.html",
    "href": "25_Statistical_inference_two.html",
    "title": "14  Statistical inference in two groups",
    "section": "",
    "text": "14.1 Introductory example\nWe do not have to limit ourselves to only making inference on one population or group. In fact most analyses that are conducted focus on comparing groups with one another in some capacity. In this chapter we will focus on the simplest of those comparisons, when we only have two groups.\nTo illustrate why statistical inference is needed in such comparisons, let us take a look at a fictive dataset (fictive_bird_example.csv) which contains equally fictive weight measurements of different bird individuals from the same species. The birds are assumed to come from two islands, a larger and a smaller one. The question is: do the data provide evidence of insular dwarfism—the phenomenon that the body sizes of species tend to decline on small islands over evolutionary time?\nWe can load the data:\nlibrary(tidyverse)\n\nbird &lt;- read_csv(\"fictive_bird_example.csv\")\nprint(bird, n = Inf)\n\n# A tibble: 40 × 2\n   island  weight\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 smaller   22.9\n 2 smaller   23.6\n 3 smaller   19.4\n 4 smaller   17.7\n 5 smaller   23.0\n 6 smaller   10.9\n 7 smaller   23.2\n 8 smaller   18.6\n 9 smaller   18.6\n10 smaller   15.4\n11 smaller   19.4\n12 smaller   29.1\n13 smaller   21.8\n14 smaller   22.6\n15 smaller   16.2\n16 smaller   24.1\n17 smaller   15.6\n18 smaller   18.3\n19 smaller   25.6\n20 smaller   21.5\n21 larger    23.9\n22 larger    27.3\n23 larger    16.8\n24 larger    12.2\n25 larger    12.0\n26 larger    29.0\n27 larger    17.6\n28 larger    23.1\n29 larger    23.1\n30 larger    19.2\n31 larger    24.1\n32 larger    31.0\n33 larger    30.2\n34 larger    28.2\n35 larger    21.3\n36 larger    22.5\n37 larger    18.4\n38 larger    11.7\n39 larger    28.8\n40 larger    20.1\nA quick visualization below looks promising, with individuals on the smaller island indeed appearing to be smaller:\nbird |&gt;\n  ggplot(aes(x = island, y = weight)) +\n  geom_boxplot(color = \"steelblue\", fill = \"steelblue\",\n               alpha = 0.2, outlier.shape = NA) +\n  geom_jitter(alpha = 0.4, width = 0.05, color = \"steelblue\") +\n  theme_bw()\nFurthermore, the computed difference between the means and medians of the two samples are also clearly different:\nbird |&gt;\n  group_by(island) |&gt;\n  summarize(mean = mean(weight), median = median(weight)) |&gt;\n  ungroup()\n\n# A tibble: 2 × 3\n  island   mean median\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 larger   22.0   22.8\n2 smaller  20.4   20.5\nCan we conclude that the two samples are indeed different, and birds on the smaller island tend to be smaller, supporting the insular dwarfism hypothesis? As mentioned above, the data are fictive—they are not based on actual measurements. In fact, these “observations” were created by sampling each data point from the same distribution, regardless of island: a normal distribution with mean 20 and standard deviation 5. This means that any supposedly observed difference between the samples must be accidental.\nIn Section 8.4 we looked at ways of visualizing two individual confidence intervals but determined that we did not take into account the variance between the groups properly to draw any statistical conclusions.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical inference in two groups</span>"
    ]
  },
  {
    "objectID": "25_Statistical_inference_two.html#sec-example-comparison",
    "href": "25_Statistical_inference_two.html#sec-example-comparison",
    "title": "14  Statistical inference in two groups",
    "section": "",
    "text": "Note\n\n\n\nIn the above plot, geom_jitter was used to display the actual data points that are summarized by the boxplots. The function geom_jitter is just like geom_point, except it adds a random sideways displacement to the data points, to reduce visual overlap between them. The width = 0.05 option restricts this convulsion of the points to a relatively narrow band. Since all data points are now displayed, it makes no sense to rely on the feature of box plots which explicitly draws points that are classified as outliers—their plotting is turned off by the outlier.shape = NA argument to geom_boxplot. It is a useful exercise to play around with these settings, to see what the effects of changing them are.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical inference in two groups</span>"
    ]
  },
  {
    "objectID": "25_Statistical_inference_two.html#sec-parametric-comparison",
    "href": "25_Statistical_inference_two.html#sec-parametric-comparison",
    "title": "14  Statistical inference in two groups",
    "section": "14.2 Parametric test",
    "text": "14.2 Parametric test\nLet us wait no longer, and perform a statistical test. Instead of looking at two single groups of means we now look at the difference between them. Similar to the parametric test for a single mean there are several assumptions that must be fulfilled in order for us to find the results reliable.\n\nindependence within both samples,\n\ncan be fulfilled by using a Simple Random Sample (SRS).\n\nindependence between samples,\n\ncan as well be fulfilled by using one SRS for the entire data or one SRS per group and there is no inherent relationship between the data points in each group.\n\nthe sample mean, \\(\\bar{X}\\), of each sample must be considered normally distributed,\n\ncan be fulfilled using the Central Limit Theorem (CLT) if the sample is sufficiently large, where the rule of thumb is \\(n_1 &gt; 30\\) and \\(n_2 &gt; 30\\). CLT states that a mean (or sum) of identically distributed variables will be approximately normally distributed in this case.\nif the sample is small, then the variable of interest, \\(X\\), must be considered normally distributed. This means that the transformation applied to the mean will also be normally distributed.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nSuppose you want to study the effect of a specific fertilizer on the growth of a particular plant species. You could set up an experiment where you measure the height of the same group of plants before and after applying the fertilizer. In this case, the samples are dependent because the measurements are taken from the same plants at two different times, the height of the plant before the treatment is going to affect the height of the plant after the treatment.\nThis example does not fulfill the second assumption, even if the plants were to be selected using an SRS. We would have to use a statistical test for dependent samples which we will cover later in this chapter.\n\n\nIf all assumptions are fulfilled we could describe the parameter of interest as the difference between the two means: \\[\n\\begin{aligned}\n\\bar{\\mu}_1 - \\bar{\\mu}_2\n\\end{aligned}\n\\] where 1 and 2 denote the larger and smaller island respectively. To simplify things we could just use \\(\\bar{\\mu}_L - \\bar{\\mu}_S\\) to make it clear which mean corresponds to which group.\nIt does not matter which group we denote to which side of the difference, just that we remember the order and what evidence we expect to see that can falsify our null hypothesis. For example if we want to find evidence for the insular dwarfism theory we need to place the evidence of that theory in \\(H_1\\). This might feel a bit counter-intuitive to the process defined in Section 12.2 but there are two aspects to take into account;\n\nwe can only find evidence in support of an alternative hypothesis, never a null hypothesis, and\nwithin statistics, the null hypothesis must always include the equal (\\(=\\)) sign.\n\nWe can find evidence of the insular dwarfism theory no matter if the difference is a really small or really big positive number1 — as long as there is a difference — so the evidence to support the theory would be defined in the alternative hypothesis as: \\[\n\\begin{aligned}\n  &H_0: \\bar{\\mu}_L - \\bar{\\mu}_S \\le 0\\\\\n  &H_1: \\bar{\\mu}_L - \\bar{\\mu}_S &gt; 0\n\\end{aligned}\n\\]\nThe dataset is structured as a tidy data which means we can make use of formula notation in R to conduct the test of the difference. Using the tidy format, the function t.test() takes two arguments: a formula, and the data to be analyzed, in the form of a data frame or tibble. The formula in the first argument establishes a relationship between two (or more) columns of the data. We will discuss formulas and their syntax in more detail later.\nFor now: the way to write them is to first type the variable we wish to predict, then a tilde (~), and then the explanatory variable (predictor) by which the data are subdivided into the two groups. In our case, we are trying to explain the difference in weight between the islands, so weight comes before the tilde and the predictor island comes after. The default value to test for is 0, so we do not need to change that argument to conform to our hypotheses, but we need to set the one-sided \\(H_1\\) in alternative.\n\nt.test(weight ~ island, data = bird, alternative = \"greater\")\n\n\n    Welch Two Sample t-test\n\ndata:  weight by island\nt = 0.99716, df = 33.742, p-value = 0.1629\nalternative hypothesis: true difference in means between group larger and group smaller is greater than 0\n95 percent confidence interval:\n -1.139879       Inf\nsample estimates:\n mean in group larger mean in group smaller \n              22.0195               20.3820 \n\n\n\n\n\n\n\n\nNote\n\n\n\nMake sure to note the order of the groups stated in the alternative hypothesis of the output. In this case it says “true difference in means between group larger and group smaller is greater than 0” which is what we defined in \\(H_1\\).\nIf the text is different from what we defined in our hypotheses, we must change the factor levels of the grouping variable in the data to the same order as we have set. We can do this by manually setting the order in the levels argument of factor().\n\nbird &lt;- bird |&gt; \n  mutate(island = factor(island, levels = c(\"larger\", \"smaller\")))\n\n\n\nSimilar to the output from the one-sample t-test we can compare the p-value to the level of significance in order to determine if the null hypothesis can be rejected or not. In this example the p-value of 0.1629 is higher than the significance level of 0.05 which means we cannot reject \\(H_0\\). We then conclude that with 5% significance we have not found evidence in support of the insular dwarfism theory.2\n\n14.2.1 Assumption of equal variance\nWhen performing a two-sample t-test, we need to consider whether the variances of the two groups are equal or unequal. This decision impacts the test we use and the reliability of our results.\nIf we assume that the variances of the two groups are equal, we use a pooled variance. This assumption can simplify calculations and increase the power of the test if it holds true, however, if the variances are not equal, this assumption can lead to incorrect conclusions.\nTo perform a t-test with equal variances in R, we use the var.equal = TRUE argument in the t.test() function:\n\nt.test(weight ~ island, data = bird, alternative = \"greater\", var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  weight by island\nt = 0.99716, df = 38, p-value = 0.1625\nalternative hypothesis: true difference in means between group larger and group smaller is greater than 0\n95 percent confidence interval:\n -1.131123       Inf\nsample estimates:\n mean in group larger mean in group smaller \n              22.0195               20.3820 \n\n\nIf we do not assume equal variances, we use Welch’s t-test, which adjusts the degrees of freedom to account for the difference in variances. This approach is more robust when the variances are indeed different between the two groups. To perform a t-test without assuming equal variances, we simply omit the var.equal argument or set it to FALSE which is the default.\nWe can use a visualization of the distribution for the two to approximate whether or not the variances are equal.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical inference in two groups</span>"
    ]
  },
  {
    "objectID": "25_Statistical_inference_two.html#sec-nonparametric-comparison",
    "href": "25_Statistical_inference_two.html#sec-nonparametric-comparison",
    "title": "14  Statistical inference in two groups",
    "section": "14.3 Non-parametric test",
    "text": "14.3 Non-parametric test\nIf the assumptions of the parametric tests — specifically the assumption of normality — cannot be fulfilled, we can instead choose to use a non-parametric alternative. One widely used test to check if two samples differ from one another is the Wilcoxon rank sum test (also known as the Mann-Whitney test). This test does not look at a specific parameter but rather the general idea of the measure of center, which is called the location.\nIts implementation is as follows:\n\nwilcox.test(weight ~ island, data = bird, alternative = \"greater\")\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 237, p-value = 0.1636\nalternative hypothesis: true location shift is greater than 0\n\n\nMost of the output is not particularly relevant for us: we are first informed that a Wilcoxon rank sum exact test is being performed; then we see that we are explaining weight differences by island; then we see the test statistic W itself (we need not concern ourselves with its precise meaning); then the p-value; and finally, a reminder of what the alternative hypothesis is (the null hypothesis is that the shift in location is in fact zero).\nWe find that the p-value, given our data, is 0.1636. What this number means is that we have a roughly one-in-six chance that the observed difference is nothing but a fluke. Since science leans towards erring on the side of caution (i.e., we would rather miss out on making a discovery than falsely claim having seen an effect), this value is in general a bit too high for comfort. And indeed: since in this case we know that the data were generated by sampling from the same distribution, any distinctiveness between them is incidental.\n\n\n\n\n\n\nWarning\n\n\n\nIn many subfields of science, it is standard practice to consider p-values falling below 0.05 as “significant” and those falling above as “non-significant”. Besides the fact that such a one-size-fits-all approach ought to be suspect even under the best of circumstances, a significance threshold of 0.05 is awfully permissive to errors. In fact, we should expect about one out of twenty (\\(1 / 0.05\\)) of all papers ever published which have reported \\(p = 0.05\\) to be wrong! Digging deeper into this issue reveals that the figure is possibly much worse—see, e.g., Colquhoun (2014).\nOne way to ameliorate the problem is to adopt a less exclusive and parochial view of p-values. Instead of having rigid significance thresholds, p-values can simply be reported as-is, and interpreted for what they are: the probability that the outcome is at least as extreme as observed, assuming that the null model holds. Treating this information as just another piece of the larger data puzzle is a first step towards avoiding the erroneous classification of random patterns as results.\nWe must keep in mind that we cannot use this when comparing p-values between different tests and determine which is “more significant”. This will become important in Chapter 17.\n\n\nApart from the p-value which measures whether the observed effect is likely to have been due to chance alone, another important piece of information is some measure of the effect size: how different are the two samples? We have already computed the means and medians; their differences across the islands provide one way of measuring this effect size. It is possible to add a calculation of the effect size, as well as the confidence intervals, to the Wilcoxon rank sum test. All one needs to do is pass conf.int = TRUE as an argument:\n\nwilcox.test(weight ~ island, data = bird, \n            conf.int = TRUE, alternative = \"greater\")\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 237, p-value = 0.1636\nalternative hypothesis: true location shift is greater than 0\n95 percent confidence interval:\n -1.02   Inf\nsample estimates:\ndifference in location \n                  1.53 \n\n\nAs additional output, we now receive the 95% confidence interval, as well as the explicit difference between the “locations” of the two samples. (A small word of caution: this “difference in location” is neither the difference of the means nor the difference of the medians, but the median of the difference between samples from the two groups of data—feel free to check the help pages by typing ?wilcox.test for more details.) If he interval were to cover 0, the practical interpretation would be that it is difficult to rule out the possibility that the observed difference in location is just due to chance.\nThe default confidence level of 95% can be changed via the conf.level argument. For example, to use a 99% confidence interval instead:\n\nwilcox.test(weight ~ island, data = bird, \n            conf.int = TRUE, conf.level = 0.99, alternative = \"greater\")\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 237, p-value = 0.1636\nalternative hypothesis: true location shift is greater than 0\n99 percent confidence interval:\n -2.54   Inf\nsample estimates:\ndifference in location \n                  1.53",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical inference in two groups</span>"
    ]
  },
  {
    "objectID": "25_Statistical_inference_two.html#some-general-conclusions",
    "href": "25_Statistical_inference_two.html#some-general-conclusions",
    "title": "14  Statistical inference in two groups",
    "section": "14.4 Some general conclusions",
    "text": "14.4 Some general conclusions\nThe example of Section 14.1 illustrates two important general points.\nFirst, instead of jumping into statistical tests, we started the analysis with qualitative and descriptive data exploration: we plotted the data, computed its means and medians, etc. This is almost always the correct way to go.3 To perform tests blindly, without visually exploring the data first, is rarely a good idea.\nSecond, we only performed the statistical test after we have made an effort to understand the data, and after setting clear expectations about what we might find. We knew, going into the test, that finding a difference between the two island samples was questionable. And indeed, the test revealed that such a distinction cannot be made in good conscience. Following a similar strategy for all statistical inference can prevent a lot of frustration. To make the point more sharply: do not perform a statistical test without an expectation of what its result will be! A more nuanced way of saying the same thing is that if you think you see a relationship in your data, then you should also make sure that your observation is not just a mirage, by using a statistical test. But if a relationship is not visually evident, you should first ask the question whether it makes much sense to try to explore it statistically.\nIn summary, the tests failed to yield serious evidence in favor of rejecting the null hypothesis. Therefore, we cannot claim with any confidence that our fictive birds have smaller sizes on the smaller island. While failure to reject the null is not the same as confirming that the null is true (absence of evidence is not evidence of absence!), the notion that the two samples are different could not be supported. In this particular case, since we ourselves have created the original data using the null hypothesis, we have the privilege of knowing that this is the truth. When working with real data, such knowledge is generally not available.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical inference in two groups</span>"
    ]
  },
  {
    "objectID": "25_Statistical_inference_two.html#parametric-versus-non-parametric-tests",
    "href": "25_Statistical_inference_two.html#parametric-versus-non-parametric-tests",
    "title": "14  Statistical inference in two groups",
    "section": "14.5 Parametric versus non-parametric tests",
    "text": "14.5 Parametric versus non-parametric tests\nThe Wilcoxon rank sum test we employed does not assume either the data or the residuals to come from any particular distribution. This is both the test’s advantage and disadvantage. It is an advantage because fewer assumptions must be satisfied for the test to be applicable. Non-parametric techniques also tend to fare better when only limited data are available. Since in many areas of biology (such as ecology) data are hard to come by, datasets are correspondingly small, making non-parametric techniques a natural candidate for interrogating the data with. On the downside, non-parametric tests tend to be less powerful than parametric ones: the additional assumptions of parametric tests may allow for sharper inference—provided that those assumptions are actually met by the real data, of course.\nThe conclusions drawn from both of these tests in the sections above were the same, but in other situations, the choice of test may matter more. The data in t_wilcox_example.csv have also been artificially created as an example. They contain two groups of “measurements”. In the first group (group x), the data have been sampled from a lognormal distribution with mean 0 and standard deviation 1. In the second group (group y), the data have been sampled from a lognormal distribution with mean 1 and standard deviation 1. Thus, we know a priori that the two distributions from which the samples were created have different means, and a statistical test may be able to reveal this difference. However, since the data are heavily non-normal, a t-test will struggle to do so. Let us load and visualize the data first:\n\nexample &lt;- read_csv(\"t_wilcox_example.csv\")\n\nexample |&gt;\n  ggplot(aes(x = measurement)) +\n  geom_histogram(bins = 25, alpha = 0.3,\n                 color = \"steelblue\", fill = \"steelblue\") +\n  facet_wrap(~ group, labeller = label_both) +\n  theme_bw()\n\n\n\n\n\n\n\n\nApplying a t-test first:\n\nt.test(measurement ~ group, data = example)\n\n\n    Welch Two Sample t-test\n\ndata:  measurement by group\nt = -1.4796, df = 19.622, p-value = 0.1549\nalternative hypothesis: true difference in means between group x and group y is not equal to 0\n95 percent confidence interval:\n -19.853561   3.388384\nsample estimates:\nmean in group x mean in group y \n        2.33571        10.56830 \n\n\nThe parametric t-test cannot detect a difference between the samples. The fact is, since its assumption of normality is violated, such a test should not have been attempted in the first place. By contrast, the non-parametric Wilcoxon rank sum test correctly suggests that there might be a difference:\n\nwilcox.test(measurement ~ group, data = example, conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  measurement by group\nW = 106, p-value = 0.01031\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -3.039534 -0.378063\nsample estimates:\ndifference in location \n             -1.798516 \n\n\nThe p-value of 0.01 means that there is a one-in-a-hundred probability that the observed difference is due only to chance.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical inference in two groups</span>"
    ]
  },
  {
    "objectID": "25_Statistical_inference_two.html#dependent-samples",
    "href": "25_Statistical_inference_two.html#dependent-samples",
    "title": "14  Statistical inference in two groups",
    "section": "14.6 Dependent samples",
    "text": "14.6 Dependent samples\nWhen dealing with dependent samples, the measurements are related in some way, such as repeated measurements on the same subjects. This dependency must be accounted for in the analysis.\nThe most common parametric test for dependent samples is the paired t-test. This test compares the means of two related groups to determine if there is a statistically significant difference between them. One additional assumption is that observations in the two groups can be paired together to create a paired difference that is analyzed similar to a one-sample test for the mean of the difference, \\(\\mu_d\\).\nIn R we can no longer make use of the formula notation but must instead add the two groups of values if the argument paired = TRUE is used.\n\nt.test(x = before, y = after, data = dataset, paired = TRUE)\n\n\nWhen the assumptions of the paired t-test are not met, we can use a non-parametric test such as the Wilcoxon signed-rank test. This test does not assume normality and is used to compare the location differences between paired observations.\n\n# Conducting a Wilcoxon signed-rank test\nwilcox.test(x = before, y = after, data = dataset, paired = TRUE)",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical inference in two groups</span>"
    ]
  },
  {
    "objectID": "25_Statistical_inference_two.html#sec-dot",
    "href": "25_Statistical_inference_two.html#sec-dot",
    "title": "14  Statistical inference in two groups",
    "section": "14.7 Statistical tests in a data analysis pipeline: The underscore notation",
    "text": "14.7 Statistical tests in a data analysis pipeline: The underscore notation\nOne natural question is whether statistical tests can be included in a data analysis pipeline. The answer is yes, but not without having to consider some complications that are due to historical accidents. Attempting the following results in an error:\n\nbird |&gt;\n  wilcox.test(weight ~ island, conf.int = TRUE)\n\nError in wilcox.test.default(bird, weight ~ island, conf.int = TRUE): 'x' must be numeric\n\n\nThe reason for the error is in how the pipe operator |&gt; works: it substitutes the expression on the left of the pipe into the first argument of the expression on the right. This works splendidly with functions of the tidyverse, since these are designed to always take the data as their first argument. However, a quick glance at the documentation of both wilcox.test and t.test (neither of which are tidyverse functions) reveals that they take the data as their second argument, with the formula coming first. A naive application of the pipe operator, like above, will therefore fail.\nFortunately, there is a way to work around this. One may use the underscore (_) in an expression to stand for whatever was being piped into it. The following will therefore work:\n\nbird |&gt;\n  wilcox.test(weight ~ island, data = _, conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by island\nW = 237, p-value = 0.3273\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -1.70  5.43\nsample estimates:\ndifference in location \n                  1.53 \n\n\nThe underscore stands for the object being piped in—which, in our case, is simply the tibble bird. The data = _ argument therefore evaluates to data = bird.\nThis almost concludes all there is to using the underscore notation, except there are three subtleties involved. The first is a simple rule: the underscore must always be used in conjunction with a named argument. So in the above piece of code, writing wilcox.test(weight ~ island, _, conf.int = TRUE) will fail—the argument name must be written out, as data = _. Second, the underscore can only appear at most once in a function call. And third, using the underscore overrides the default behavior of silently substituting the data as the first argument to the function on the right of the pipe. That is, if one uses the underscore, then the data on the left of the pipe will be substituted only where the underscore is.\n\n\n\n\n\n\nNote\n\n\n\nIn case you are using the magrittr pipe %&gt;% instead of the built-in pipe |&gt;, you can observe all of the rules above, except you should replace the underscore by a dot (.). So\n\nbird |&gt;\n  wilcox.test(weight ~ island, data = _, conf.int = TRUE)\n\ncan be equivalently rewritten as\n\nbird %&gt;%\n  wilcox.test(weight ~ island, data = ., conf.int = TRUE)\n\nThere are other subtle differences in how %&gt;% works compared with |&gt;; you can look at the help pages of both (?`%&gt;%`, resp. ?`|&gt;`) for a more thorough explanation.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical inference in two groups</span>"
    ]
  },
  {
    "objectID": "25_Statistical_inference_two.html#exercises",
    "href": "25_Statistical_inference_two.html#exercises",
    "title": "14  Statistical inference in two groups",
    "section": "14.8 Exercises",
    "text": "14.8 Exercises\n\nAtmospheric ozone levels are important for urban gardens, because ozone concentrations above 80 parts per hundred million can damage lettuce plants. The file ozone.csv contains measured ozone concentrations in gardens distributed around a city. The gardens were either east or west of the city center. Is there a difference in average ozone concentration between eastern vs. western gardens? Create a plot of the concentrations first and try to guess the answer, before performing any statistical analyses. Then do the statistics, using both nonparametric and parametric methods. What result do you find? Do the outcomes of the different methods agree with one another?\nIn the built-in iris dataset, the sepal and petal characteristics of the various species differ. Let us focus on just petal length here. Is there a detectable difference between the petal lengths of Iris versicolor and I. virginica? (Since we are not working with I. setosa in this exercise, make sure to remove them from the data first—e.g., by using filter.) Like before, start with a graph which you can base a hypothesis on. Then perform both nonparametric and parametric tests to see if your hypothesis can be supported.\nRepeat the above exercise for any and all of the possible trait-species pair combinations in the iris dataset. E.g., one can compare the sepal widths of I. setosa and I. virginica, or the petal widths of I. versicolor and I. setosa, and so on.\n\n\n\n\n\nColquhoun, David. 2014. “An investigation of the false discovery rate and the misinterpretation of p-values.” Royal Society Open Science 1 (3): 140216. https://doi.org/10.1098/rsos.140216.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical inference in two groups</span>"
    ]
  },
  {
    "objectID": "25_Statistical_inference_two.html#footnotes",
    "href": "25_Statistical_inference_two.html#footnotes",
    "title": "14  Statistical inference in two groups",
    "section": "",
    "text": "Remember that the theory says the sizes are smaller on a smaller island!↩︎\nIn this case we know that it is true given we created the sample ourselves.↩︎\nThe exception is when one analyzes data from a pre-registered experimental design. In that case, one must follow whatever statistical techniques were agreed upon before data collection even started.↩︎",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical inference in two groups</span>"
    ]
  },
  {
    "objectID": "26_Statistical_inference_three.html",
    "href": "26_Statistical_inference_three.html",
    "title": "15  Statistical inference in qualitative variables",
    "section": "",
    "text": "15.1 \\(\\chi^2\\)-test for independence\nThe previous chapters have focused on quantitative variables, except for the proportion or binomial tests of a specific category in a qualitative variable. If we want to compare groups of a qualitative variables we are in this part of the book limited in the methods available.\nThe \\(\\chi^2\\)-test for independence (pronounced chi-square) is used to determine if there is a significant association between two categorical variables. It compares the observed frequencies in each category to the frequencies we would expect if the variables were independent. For example, you might want to test whether the distribution of plant species is different across habitats or whether treatment groups in a study differ in their responses.\nFormally we state the hypotheses as: \\[\n\\begin{aligned}\n&H_0: \\text{The variables are independent}\\\\\n&H_1: \\text{The variables are dependent}\n\\end{aligned}\n\\]\nThe process calculates the difference in observed frequencies with the expected frequencies under the null hypothesis.\n\\[\n\\begin{aligned}\n\\chi^2 = \\sum_{\\text{cells}} \\frac{(O_i - E_i)^2}{E_i}\n\\end{aligned}\n\\] where:\nThe expected frequency is calculated as: \\[\n\\begin{aligned}\nE_i = \\frac{(\\text{row total}) \\times (\\text{column total})}{\\text{grand total}}\n\\end{aligned}\n\\]\nThe assumptions for the test is that no more than 20% of the expected frequencies are lower than 5, and none are lower than 1. If this assumption does not hold, we cannot in earnest believe the test results.\nSuppose you are studying whether different plant species are distributed differently across two habitat types (“Forest” and “Grassland”). The observed counts are as follows:\nYou can create a contingency table and run the chi-square test:\n# Create a contingency table\nobserved &lt;- matrix(c(30, 20, \n                     25, 15, \n                     10, 5), \n                   nrow = 3, byrow = TRUE)\nrownames(observed) &lt;- c(\"Species A\", \"Species B\", \"Species C\")\ncolnames(observed) &lt;- c(\"Forest\", \"Grassland\")\n\n# Run the chi-square test using the custom chi.sq() function\nchi_result &lt;- chisq.test(observed)\n\n# Display the test results\nprint(chi_result)\n\n\n    Pearson's Chi-squared test\n\ndata:  observed\nX-squared = 0.22716, df = 2, p-value = 0.8926\nThe output shows a p-value which can be used to determine whether we can reject the \\(H_0\\) or not.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Statistical inference in qualitative variables</span>"
    ]
  },
  {
    "objectID": "26_Statistical_inference_three.html#chi2-test-for-independence",
    "href": "26_Statistical_inference_three.html#chi2-test-for-independence",
    "title": "15  Statistical inference in qualitative variables",
    "section": "",
    "text": "\\(O_i\\) is the observed frequency in cell \\(i\\),\n\\(E_i\\) is the expected frequency in cell \\(i\\) under the assumption of independence.\n\n\n\n\n\nSpecies A: 30 in Forest, 20 in Grassland\nSpecies B: 25 in Forest, 15 in Grassland\nSpecies C: 10 in Forest, 5 in Grassland",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Statistical inference in qualitative variables</span>"
    ]
  },
  {
    "objectID": "26_Statistical_inference_three.html#chi2-test-for-goodness-of-fit",
    "href": "26_Statistical_inference_three.html#chi2-test-for-goodness-of-fit",
    "title": "15  Statistical inference in qualitative variables",
    "section": "15.2 \\(\\chi^2\\)-test for goodness-of-fit",
    "text": "15.2 \\(\\chi^2\\)-test for goodness-of-fit\nThe \\(\\chi^2\\) goodness-of-fit test is used to determine if a sample data matches a population with a specific distribution. It compares the observed frequencies in each category to the expected frequencies based on the specified distribution.\n\\[\n\\begin{aligned}\n&H_0: \\text{X follows a binomial distribution with probability p}\\\\\n&H_1: \\text{X does not follow a binomial distribution with probability p}\n\\end{aligned}\n\\]\nFor example when using the binomial distribution as the null hypothesis, we are checking if a dichotomous outcome (such as “defective” vs. “non‐defective” or “success” vs. “failure”) occurs with a specified probability, \\(p\\). We assume that in \\(n\\) independent trials, the probability of success is given by \\(p\\). Under this hypothesis, the expected number of successes and failures is computed as follows:\n\\[ \\begin{aligned}\nE_{\\text{success}} = n \\cdot p \\quad \\text{and} \\quad E_{\\text{failure}} = n \\cdot (1-p).\n\\end{aligned} \\]\nSuppose we have observed counts \\(O_{\\text{success}}\\) and \\(O_{\\text{failure}}\\). The \\(\\chi^2\\) test statistic is then calculated by comparing these observed counts with the expected counts:\n\\[ \\begin{aligned}\n\\chi^2 = \\frac{(O_{\\text{success}} - E_{\\text{success}})^2}{E_{\\text{success}}} + \\frac{(O_{\\text{failure}} - E_{\\text{failure}})^2}{E_{\\text{failure}}}.\n\\end{aligned} \\]\nSince we have two categories (success and failure), the degrees of freedom for this test is given by:\n\\[ \\begin{aligned}\n\\text{df} = k - 1,\n\\end{aligned} \\]\nwhere ( k = 2 ). Therefore, in our scenario,\n\\[ \\begin{aligned}\n\\text{df} = 2 - 1 = 1.\n\\end{aligned} \\]\nIf the p-value associated with the computed \\(\\chi^2\\) statistic (evaluated using a chi-square distribution with 1 degree of freedom) is less than the chosen significance level (typically \\(\\alpha = 0.05\\)), the null hypothesis is rejected. This result indicates that the observed frequencies significantly deviate from those expected under the assumed binomial distribution.\nThis use of the \\(\\chi^2\\)-test is in practice very similar to the test for independence, but the main difference is what \\(H_0\\) represents — a specific distribution instead of independence — and how the expected values are calculated.",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Statistical inference in qualitative variables</span>"
    ]
  },
  {
    "objectID": "26_Statistical_inference_three.html#exercises",
    "href": "26_Statistical_inference_three.html#exercises",
    "title": "15  Statistical inference in qualitative variables",
    "section": "15.3 Exercises",
    "text": "15.3 Exercises\nThe material to be analyzed in these exercises is a collection of phosphorus levels in 4189 lakes from different parts of Sweden. The phosphorus levels have been divided into five different condition classes. The dataset can be found via lake-phosphorus.csv.\nThis task aims to analyze the collected data from the Swedish lakes. What we want to investigate is whether there is a dependency between condition class and region.\n\nFormulate hypotheses that describe what you want to investigate and set the level of significance.\nShow the expected values and check that the assumptions for the test has been fulfilled.\nConduct the full test and interpret the results of the output. Does the data provide evidence that there is a dependency between the region of Sweden and the amount of phosphorus in the lakes?",
    "crumbs": [
      "Part II: Introductory statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Statistical inference in qualitative variables</span>"
    ]
  },
  {
    "objectID": "40_Intro_adv_statistics.html",
    "href": "40_Intro_adv_statistics.html",
    "title": "16  Relationships between variables",
    "section": "",
    "text": "16.1 Linear regression models\nInvestigating relationships between variables is a common step in understanding connections or phenomena occurring in the real world. For example, how the age of a tree affects its volume, how different doses of a medicine affect an individual’s blood pressure, or how a person’s age and education level affect their starting salary.\nRegression analysis is a group of methods that fit mathematical models which best provide a simplified picture of reality. Generally, the term model refer to artificial constructions or representations created to increase understanding of something real. A model airplane is not a real airplane but can be used to understand how an airplane is built and how it can handle air flows and other phenomena. Another classic example is the painting “Ceci n’est pas une pipe” by René Magritte (Figure 16.1), which shows a representation — a model — of a pipe, not a functioning pipe.\nThere are many different types of regression models, but in the simplest case, a linear model is applied when the relationship between variables, for example X and Y, is assumed to be unidirectional and constant. We can write this linear model as: \\[\nY_i = \\beta_0 + \\beta_1 \\cdot X_i + \\varepsilon_i\n\\tag{16.1}\\]\nwhere:",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Relationships between variables</span>"
    ]
  },
  {
    "objectID": "40_Intro_adv_statistics.html#linear-regression-models",
    "href": "40_Intro_adv_statistics.html#linear-regression-models",
    "title": "16  Relationships between variables",
    "section": "",
    "text": "\\(Y\\) is the dependent/response variable assumed to be affected by \\(X\\),\n\\(X\\) is the independent/explanatory variable assumed to affect \\(Y\\),\n\\(i\\) is the i:th observation of the data,\n\\(\\beta_0\\) is the model’s intercept, where the regression line crosses the y-axis at \\(X = 0\\).\n\\(\\beta_1\\) is the slope that describes the unidirectional relationship between \\(X\\) and \\(Y\\). More specifically, it describes the change in \\(Y\\) when \\(X\\) increases by one unit.\n\\(\\varepsilon\\) is the model’s error term, the distance between the observed value of \\(Y\\) and the model’s estimated value \\(\\hat{Y}\\).\n\n\n\n\n\n\n\nImportant\n\n\n\nIf multiple explanatory variables are assumed to affect the response variable, the linear model is extended with several \\(\\beta_j\\), one for each explanatory variable \\(X_j\\). More on that in Chapter 18.\n\n\n\n16.1.1 Model assumptions\nThe purpose of a model is to provide a suitable simplification of reality but in order to do that it needs to fulfill some criteria or be constructed using some assumptions. A linear regression model can be a suitable simplification of a relationship between two variables if the following assumptions are met:\n\nfor each value of \\(X\\), there exists a random variable \\(Y\\) with a finite mean and variance,\nall observations are independent of each other,\nthe mean of \\(Y\\) for every value of \\(X\\), \\(\\mu_{Y|X}\\), can be modeled linearly,\nthe variance of \\(Y\\) is the same for all values of \\(X\\), \\(\\sigma^2_{Y|X} \\equiv \\sigma^2\\),\nthe random variable \\(Y\\) is normally distributed for all values of \\(X\\).\n\nTogether these assumptions can be summarized as:\n\\[\n   Y|X \\overset{\\mathrm{iid}}{\\sim} N(\\mu_{Y|X}, \\sigma^2)\n\\] where \\(\\mathrm{iid}\\) means “independent and identically distributed”, corresponding to assumption 2.\n\n\n\n\n\n\nImportant\n\n\n\nThere is no assumption that \\(Y \\sim N(\\mu_Y, \\sigma^2_Y)\\)! All assumptions for a linear regression model focus on the fact that with the help of \\(X\\), we have a normally distributed random variable \\(Y\\).\n\n\nIf the assumption of linearity is met, we can model the expected value of \\(Y|X\\) with the linear model: \\[\n  E[Y|X] = \\beta_0 + \\beta_1 \\cdot X\n\\tag{16.2}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe expected value of a random variable is the mean of its distribution, which is why we can use the notation \\(E[Y|X]\\) to denote the mean of the distribution of \\(Y\\) given \\(X\\).\nUnlike Equation 16.1, Equation 16.2 lacks the model’s error term because we are now modeling only the mean of the random variable’s distribution, \\(\\mu_{Y|X}\\). The error term describes the uncertainty around the mean, which is the variance of the distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.2: Unsuitable airplane model, (“Create a Model of a Commercial Airplane Where Some Parts Are Taken from a Car or a Boat” 2024)\n\n\n\nIf a model airplane is constructed without following the same assumptions as a real airplane, the model will not be suitable for the understanding or simplification of reality. The same applies to regression models; if the model does not meet its assumptions, conclusions drawn may not align with reality. We will cover more of how this is done in practice in Section 17.3.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Relationships between variables</span>"
    ]
  },
  {
    "objectID": "40_Intro_adv_statistics.html#study-design-and-additional-variables",
    "href": "40_Intro_adv_statistics.html#study-design-and-additional-variables",
    "title": "16  Relationships between variables",
    "section": "16.2 Study design and additional variables",
    "text": "16.2 Study design and additional variables\nApart from the assumptions, a regression model does not necessarily describe a “cause-and-effect” relationship, or as we usually call it, a causal relationship from X to Y. Relationships can sometimes arise purely by chance, where there is no logical connection between the variables.\nThis type of relationship is referred to as a correlation and although correlation mathematically describes a relationship, it is most often not appropriate or irrelevant to use or interpret that model in reality.\n\n\n\n\n\n\n\n\nFigure 16.3: The relationship between the number of movies Nicolas Cage appeared in and the number of drowning deaths between 1999 and 2009 in the USA (“CDC - NCHS - National Center for Health Statistics — Cdc.gov”; “Nicolas Cage | Actor, Producer, Director — Imdb.com”)\n\n\n\n\n\nFigure 16.3 shows an example of a relationship where the two variables have no logical connection to each other but have been observed to have a positive correlation, when one’s value increases the other does as well. Describing this relationship would not provide any information about reality, so an important part of regression analysis is assessing the suitability and relevance of selected variables. A regression model cannot distinguish between causal and correlation relationships, which means we must consider what kind and how data has been collected in order to use and interpret the models correctly.\nThe example in the figure was collected as an observational study, where measurements (the number of deaths and movies) for each unit (year) were observed from various registry data. With an observational study, we are not able to control the relationship between these variables, and the study itself has not considered any specific cause and effect direction between the two. Therefore, we can only draw conclusions about correlations from an observational study — we can say that years that have more movies with Nicolas Cage also have more deaths, which is not actually relevant — but we cannot say anything about the causal effect.\nTo be able to draw conclusions about causal relationships, we need to conduct an experimental study where we control the values of the explanatory variable and the direction of the relationship; the response variable is assumed to be directly affected by the explanatory variable(s). Medical studies, such as studies on the effectiveness of the COVID-19 vaccine in preventing infection, are typical examples of experimental studies where an explanatory variable (dose) is given to certain groups of measurement units while other influencing effects are controlled to isolate the true impact of the explanatory variable.\n\n16.2.1 Control variables\nIn an observational study, we can sometimes observe control variables that can adjust the actual impact of the explanatory variable, but it is primarily in experimental studies that these types of variables are used. Unobserved variables are called confounding effects (or confounders) and are assumed to influence both the explanatory and response variable.\nIn the following two figures called relational graphs, the main explanatory variable and the chosen response variable are shown as ovals. The observed control (solid) and unobserved confounder (dashed) variables are shown as rectangles.\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nConfounder\n\nConfounder\n\n\n\nMovies\n\nMovies\n\n\n\nConfounder-&gt;Movies\n\n\n\n\n\nDeaths\n\nDeaths\n\n\n\nConfounder-&gt;Deaths\n\n\n\n\n\nMovies-&gt;Deaths\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.4: Example of the relationship between variables in an observational study.\n\n\n\nSince the relationship in Figure 16.4 between movies and deaths likely arose by chance, there are most certainly unobserved confounder variables influencing both.\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nAge\n\nAge\n\n\n\nBloodPressure\n\nBloodPressure\n\n\n\nAge-&gt;BloodPressure\n\n\n\n\n\nGender\n\nGender\n\n\n\nGender-&gt;BloodPressure\n\n\n\n\n\nMedicine\n\nMedicine\n\n\n\nMedicine-&gt;BloodPressure\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.5: Example of the relationship between variables in an experimental study.\n\n\n\nThe effect of a medicine on blood pressure can also be influenced by a person’s age and gender (Gu et al. 2008), which are two observed control variables included in Figure 16.5 to isolate the effect of the explanatory variable.\n\n\n\n\n“CDC - NCHS - National Center for Health Statistics — Cdc.gov.” https://www.cdc.gov/nchs/.\n\n\n“Create a Model of a Commercial Airplane Where Some Parts Are Taken from a Car or a Boat.” 2024. OpenAI. https://chat.openai.com/chat.\n\n\nGalton, Francis. 1886. “Regression Towards Mediocrity in Hereditary Stature.” Journal of the Anthropological Institute of Great Britain and Ireland 15: 246–63.\n\n\nGu, Qiuping, Vicki L. Burt, Ryne Paulose-Ram, and Charles F. Dillon. 2008. “Gender Differences in Hypertension Treatment, Drug Utilization Patterns, and Blood Pressure Control Among US Adults With Hypertension: Data From the National Health and Nutrition Examination Survey 1999–2004.” American Journal of Hypertension 21 (7): 789–98. https://doi.org/10.1038/ajh.2008.185.\n\n\n“Nicolas Cage | Actor, Producer, Director — Imdb.com.” https://www.imdb.com/name/nm0000115/.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Relationships between variables</span>"
    ]
  },
  {
    "objectID": "41_Linear_regression.html",
    "href": "41_Linear_regression.html",
    "title": "17  Simple linear regression",
    "section": "",
    "text": "17.1 Example data\nThe first step in a regression analysis is to explore the dataset. Regardless of whether we conducted an observational or experimental study, we have likely selected a response variable beforehand that we want to describe or predict using other variables. The goal of exploring the data is to gain insights that can help us in later steps; which variables are relevant to include in a model, what each relationship looks like, and whether there are any issues with the collected data that need to be corrected.\nSome key questions we need to answer are:\nOnce we’ve answered these questions, it becomes much easier to build a correct model structure and evaluate any estimated models.\nWe can investigate these points using simple functions (e.g., mean(), min(), summary(variable)), but we can also gain relevant information through individual and pairwise visualizations of the variables.\nAs an introductory example, let us load the Galápagos land snail data from Floreana Island (Section 4.2.2). We look only at individuals belonging to the species Naesiotus nux, and plot the shell shape measurement against the shell size measurement for each of them:\nsnails &lt;- read_csv(\"island-FL.csv\")\n\nnux &lt;- snails |&gt;\n  filter(species == \"nux\")\n\nnux |&gt; \n  ggplot(aes(x = size, y = shape)) +\n  geom_point(color = \"steelblue\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 17.1: Scatter plot showing the relationship between shell shape and size\nBoth of these variables are continuous quantitative, which means we can create a scatter plot where each observation is represented by a point. The explanatory variable is placed on the x-axis and the response variable on the y-axis. The point cloud in the scatter plot can give us information about the relationship between the two variables. We focus on four main points:\nFigure 17.1 shows a very sparse relationship that is a bit difficult to interpret. One could say that the relationship appears mostly linear, as an increase in shell size leads to a unidirectional change (increase/positive relationship) in shell shape. However, the relationship is not very strong, as there is a lot of variation in the shape values for each size value. There are also some outliers, which we can see as points that are far from the main point cloud.\nWe can calculate Pearson’s correlation coefficient (\\(r\\)) so we don’t have to rely on a subjective interpretation of the strength.1 This coefficient measures the strength of the linear relationship between two quantitative variables and is appropriate in this case. A value near 0 indicates no or weak relationship, while values near -1 or +1 indicate a strong negative or positive relationship, respectively.\n\\[\nr = 0.358\n\\]\nThis is an additional indication that the correlation between the two variables is relatively weak, mostly due to the large variation in shape values for each size value. The correlation coefficient is not very high, but it is positive, indicating that larger shells tend to be more elongated.\nAfter gathering information from visualizations and descriptive statistics, the next step in the process is to build the structure of the model.\n\\[\n\\text{shape}_i = \\beta_0 + \\beta_1 \\cdot \\text{size}_i + \\varepsilon_i\n\\tag{17.1}\\]\nRegardless of whether the model includes one or several explanatory variables, we must always keep in mind the five assumptions presented in Section 16.1.1, especially the assumption of linearity. If we have discovered non-linear relationships in the exploratory step, we need to model a more complex relationship than what is shown in Equation 17.1.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "41_Linear_regression.html#sec-example-data",
    "href": "41_Linear_regression.html#sec-example-data",
    "title": "17  Simple linear regression",
    "section": "",
    "text": "Is the relationship linear?\nIs the relationship positive or negative?\nIs the relationship strong or weak?\nAre there any outliers?\n\n\n\n\n\n\n\nImportant\n\n\n\nDefining the proper variable type and scale is very important in this exploratory step. Using the wrong type of visualization can easily misconstrue the relationship and send us on a wild goose chase.\nWe cannot use scatter plots to visualize the relationship between qualitative explanatory variables and a continuous response variable. Instead, we need visualizations that account for the qualitative scale, usually ordinal or nominal.\nThere are several ways to visualize the distribution of the response variable across levels of the explanatory variable, such as grouped histograms or box plots, but one type of visualization that shows distribution details is a violin plot. A violin plot is a mirrored density plot, where areas with many observations have a larger area under the curve. Using ggplot2, we can create such a plot with geom_violin().\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf the scatter plot shows a non-linear and non-monotonic (not unidirectional) relationship, the Pearson correlation coefficient will not accurately describe the strength of the relationship. It is easy to rely solely on the correlation coefficient since it is simple to calculate for many variable pairs, but it can often miss relevant information. A visualization enables the identification of complex relationships that often require different modeling approaches.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "41_Linear_regression.html#sec-model-estimation",
    "href": "41_Linear_regression.html#sec-model-estimation",
    "title": "17  Simple linear regression",
    "section": "17.2 Model estimation",
    "text": "17.2 Model estimation\nEquation 17.1 shows the “true” model based on all observed values in the population, but (nearly) all studies are based on some form of sample. Even a population census during a specific period can be considered a sample in time if the model is intended to be used after the study period ends.\nWe can denote the estimated model with its estimated parameters as:\n\\[\n\\hat{Y}_i = b_0 + b_1 \\cdot X_{1i}\n\\tag{17.2}\\]\nwhere: \\[\\begin{align*}\n  \\hat{Y}_i &= \\text{estimated value of the response variable for observation } i\\\\\n  b_0 &= \\text{estimate of the intercept}\\\\\n  b_1 &= \\text{estimate of the slope parameter}\n\\end{align*}\\]\n\n\n\n\n\n\nNote\n\n\n\nSome literature uses \\(\\hat{\\beta}\\) to denote estimated parameters.\n\n\nWe want this model to best describe the relationship so we first need to define what we mean by “best”. If we project two different estimated simple linear models onto a scatter plot of the two variables (Figure 17.2), each line will not hit all points exactly, each observation will lie at a certain distance from the regression line. This distance is the observation’s residual, denoted by \\(e_i\\), an estimate of \\(\\varepsilon_i\\).\n\n\n\n\n\n\n\n\nFigure 17.2: Visualization of regression model residuals\n\n\n\n\n\nThe vertical lines are the estimated residuals, measuring the deviation from the line and the actual data points. It is intuitively obvious that the first line (with intercept \\(b_0 = -0.126\\) and slope \\(b_1 = 0.0038\\)) is better than the second one (intercept \\(b_0 = 0.148\\) and slope \\(b_1 = -0.0051\\)). This is because the size of the residuals tend to be larger for the right panel. The model on the left is estimated using Ordinary Least Squares (OLS), where the goal is to minimize the model’s total error, whereas the one on the right is just a random line drawn on the graph.\nMathematically, we calculate \\(e_i = Y_i - \\hat{Y}_i\\), where \\(Y_i\\) is the observed value (the point) and \\(\\hat{Y}_i\\) is the model’s estimated value (the line). OLS estimates all model parameters so that the total error (Sum of Squares of Error, SSE)2 for all observations is minimized:\n\\[\nSSE = \\sum_{i = 1}^n e_i^2 = \\sum_{i = 1}^n (Y_i - \\hat{Y}_i)^2\n\\tag{17.3}\\]\nIn simple linear regression, we can use simple formulas for the two parameter estimates, \\(b_0\\) and \\(b_1\\), that minimize SSE. However, as soon as we include multiple variables, this becomes significantly more complex. For these complex calculations we instead make use of matrix algebra, which happen behind the scenes in R when using the the function lm().\n\n\n\n\n\n\nNote\n\n\n\nFormulas for parameter estimates in simple linear regression are: \\[\\begin{align*}\n  b_1 &= \\frac{\\sum_{i=1}^n(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n(X_i - \\bar{X})^2}\\\\\n  b_0 &= \\bar{Y} - b_1 \\cdot \\bar{X}\n\\end{align*}\\]\n\\(b_1\\) can also be reformulated as: \\[\n\\begin{aligned}\n\\frac{\\sum_{i=1}^n(X_i \\cdot Y_i) - \\frac{\\sum_{i=1}^nX_i \\cdot \\sum_{i=1}^nY_i}{n}}{\\sum_{i=1}^nX_i^2 - \\frac{(\\sum_{i=1}^nX_i)^2}{n}}\n\\end{aligned}\n\\]\n\n\n\n17.2.1 Model estimation in R\nTo fit a linear regression model in R, we use the lm() function with the following arguments:\n\nformula: the model structure as a formula object\ndata: the dataset containing the variables\n\nA formula object is a special format R uses to describe the relationship between variables. Generally, the format is y ~ x, where x consists of the explanatory variables, for example shape ~ size.\n\n\n\n\n\n\nImportant\n\n\n\nIf you have not experienced problems already, using lm() requires that we have the correct variable type for all variables as we expect. Make sure that any continuous variable is read as numeric!\n\n\n\nsimpleModel &lt;- lm(formula = shape ~ size, data = nux)\n\nWith summary(), we get a detailed output of the model that includes the estimated parameters which are also called regression coefficients. When presenting such output, we can use kable() or xtable() for a cleaner display.\n\n\n\n\n\n\nTip 17.1\n\n\n\nTo create this clean output of the coefficients, we need to extract a specific part of summary() using coef(). The documentation for lm() provides more information about what can be retrieved from the resulting regression object.\nR is an object-oriented programming language, and the lm() function returns an object of class “lm”, which is a list. It is easy to extract desired parts from that list when needed. There are many functions associated with objects of class “lm”:\n\ncoef(): Returns regression coefficients\n\nresiduals(): Returns residuals\n\nfitted(): Returns estimated values (\\(\\hat{Y}\\))\n\nsummary(): Returns a summary analysis of the regression model. This function returns an object of class “summary.lm”. See ?summary.lm in the documentation. coef() also works on these objects as shown above.\n\nanova(): Returns the ANOVA table for the model\n\npredict(): Makes predictions for (new) x-values, i.e., calculates \\(\\hat{Y}\\) for given x-values. Can also calculate confidence intervals and prediction intervals for \\(\\hat{Y}\\). See ?predict.lm() for details.\n\nconfint(): Calculates confidence intervals for the regression coefficients\n\nIt is also useful to use str() on lm objects. Check ?lm() under the “Value” section to see the different components of the object. We will make use of a couple of these objects in later chapters.\n\n\n\n\n\n\nsummary(simpleModel)\n\n\nCall:\nlm(formula = shape ~ size, data = nux)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.048880 -0.016772  0.001423  0.013181  0.048515 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -0.129240   0.040706  -3.175  0.00307 **\nsize         0.004083   0.001777   2.297  0.02752 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02275 on 36 degrees of freedom\nMultiple R-squared:  0.1279,    Adjusted R-squared:  0.1036 \nF-statistic: 5.278 on 1 and 36 DF,  p-value: 0.02752\n\n\n\n\nFigure 17.3: Not a very clean output\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nkable() is a function from the knitr package which produces or HTML style formatted tables within R. Alongside knitr, we can also customize the design of the table with the help of functions from the kableExtra package, for example kable_styling() that is used in the below example.\nxtable() comes from the xtable package and is used to create style tables, most often used for non-web resources.\n\n\n\nrequire(knitr)\nrequire(kableExtra)\n\nsummary(simpleModel) |&gt; \n  coef() |&gt; \n  as_tibble(rownames = NA) |&gt; \n  rownames_to_column() |&gt; \n  rename(\n    ` ` = rowname,\n    Estimate = Estimate,\n    StdError = `Std. Error`,\n    `t-value` = `t value`,\n    `p-value` = `Pr(&gt;|t|)`\n  ) |&gt; \n  kable(\n    digits = 4\n  ) |&gt; \n  kable_styling(\"striped\")\n\n\n\nTable 17.1: A clean table of the model’s estimated parameters\n\n\n\n\n \n  \n     \n    Estimate \n    StdError \n    t-value \n    p-value \n  \n \n\n  \n    (Intercept) \n    -0.1292 \n    0.0407 \n    -3.1749 \n    0.0031 \n  \n  \n    size \n    0.0041 \n    0.0018 \n    2.2974 \n    0.0275 \n  \n\n\n\n\n\n\n\nTable 18.2 shows the estimated parameters (coefficients). For example, we can see that for each additional size of shell, the shape increases by approximately 0.0041 units on average.\nThe intercept is only relevant to interpret if the value range includes all zeros; if the data covers the region where all explanatory variables take the value 0. In this example, there is no data in that region, which means the intercept value has no meaningful interpretation.\n\n\n\n\n\n\nImportant\n\n\n\nEven though the interpretation of the intercept may not be meaningful, it must be included in the model in order for the OLS estimation to minimize SSE. If the intercept were removed, it would correspond to a line forced to cross the y-axis at \\(y = 0\\), which would result in a model that does not meet our criteria of the “best” model.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "41_Linear_regression.html#sec-residual-analysis",
    "href": "41_Linear_regression.html#sec-residual-analysis",
    "title": "17  Simple linear regression",
    "section": "17.3 Model diagnostics",
    "text": "17.3 Model diagnostics\nAfter fitting a model based on observations from visualizations and descriptive statistics, we can interpret the estimated relationship between the explanatory variables and the response variable, as we did in Section 17.2.1. However, there are two aspects we have yet to consider:\n\nwe cannot assume these interpretations describe the true relationship since we do not yet know if the model is an appropriate representation,\nthese interpretations only describe the collected sample, not the population we want to draw conclusions for.\n\nTo assess the appropriateness of the model, we must examine whether it meets the assumptions presented in Section 16.1.1 through residual analysis, and conclusions about the population can be made using statistical inference. We always begin by evaluating the model’s appropriateness because the inference methods also rely on these assumptions being met.\nRemember Equation 16.1? When modeling each individual observation, \\(\\varepsilon_i\\) measures the difference between the regression line and the actual observation, which means we can shift the model’s assumptions presented in Section 16.1.1 from \\(Y|X\\) to \\(\\varepsilon\\). \\[\n  \\varepsilon \\overset{\\mathrm{iid}}{\\sim} N(0, \\sigma^2)\n\\tag{17.4}\\]\nThis reformulation gives us a good starting point to evaluate the suitability of a estimated model.\n\n17.3.1 Residual analysis\nResidual analysis involves calculating and visually exploring the residuals from a model against the model assumptions \\(\\varepsilon\\overset{iid}{\\sim}N(0, \\sigma^2)\\). Since we estimate error term with residuals, in practice we check if the residuals are independent, normally distributed with mean 0 and constant variance. Residuals can also be used to assess whether the estimated linear model is appropriately structured, i.e. whether the relationship between the response variable and the explanatory variables is linear.\nFor simplicity, we can extract the residuals as well as the observed and estimated values of the response variable from the estimated model (see Tip 17.1).\n\n# Create a dataset for visualizations\n\nresidualData &lt;- \n  tibble(\n    # Extracts the residuals from the model\n    residuals = residuals(simpleModel),\n    # The observed response values from the data\n    y = nux$shape,\n    # Extracts the estimated response values from the model\n    yHat = fitted(simpleModel)\n  )\n\nWe will visualize these variables in various forms using ggplot2, which requires a data.frame or tibble with data.\n\n17.3.1.1 Normal distribution\nWe can examine the assumption of normally distributed residuals using a histogram and/or a QQ plot (quantile-quantile plot).\n\nggplot(residualData) + \n  aes(x = residuals, y = after_stat(density)) +\n  geom_histogram(bins = 10, fill = \"steelblue\", color = \"black\") + \n  theme_bw() + \n  labs(x = \"Residuals\", y = \"Density\")\n\n\n\n\nDistribution of residuals\n\n\n\n\n\nggplot(residualData) + \n  # Use standardized residuals\n  aes(sample = scale(residuals)) + \n  geom_qq_line() +\n  geom_qq(color = \"steelblue\") +\n  theme_bw() + \n  labs(x = \"Theoretical Quantiles\", y = \"Observed Quantiles\")\n\n\n\n\nObserved residual quantiles compared to theoretical normal quantiles\n\n\n\n\nIn the histogram, we want to see the symmetric, bell-shaped form of the normal distribution centered around 0, which can sometimes be difficult to detect especially with small datasets. The QQ plot shows the observed and theoretical quantiles, where we want the points to follow the drawn line for a “perfect” normal distribution.\n\n\n\n\n\n\nNote\n\n\n\nRemember that quantiles is a generalization of a quartile that divides the observed values into equal parts.\nThe observed quantiles takes the information from the variable and defines each value as a specific quantile in order from smallest to largest.\nThe theoretical quantile assumes that the data is normally distributed with its given mean and standard deviation and for each observation calculates its quantile position within that distribution. We did that “manually” in Section 10.2.2 using pnorm().\nThe QQ plot then maps for each observation the observed quantile against the theoretical and draws a line where we would expect the points to lie if the data were normally distributed. If the points follow close to the line, we can assume that the data is approximately normally distributed.\n\n\nFor this model, we do not see any clear deviations from the expected pattern, which suggests that the residuals are normally distributed. However, we can never be 100% certain that the assumption is met, especially with small datasets.\n\n\n\n\n\n\nImportant\n\n\n\nWe can consider the assumption of normality to be violated if these plots show strong deviations from what we expect. Even when we know a sample is drawn from a normal distribution, the histogram may not always show the shape we are looking for.\n\n\n\n\n\nDistribution of a sample from the actual normal distribution\n\n\n\n\nStrong deviations from normality may include, for example, multiple areas of high density:\n\n\n\n\n\nDistribution of a sample from another distribution than the normal\n\n\n\n\nor a highly skewed distribution:\n\n\n\n\n\nDistribution of a sample from another distribution than the normal\n\n\n\n\nThese plots suggest that the model is missing an explanatory variable or the data needs to be transformed in some way to meet the assumption.\nOn top of non-normality, the QQ plot can also indicate if the model deviates from the assumption of linearity. If the QQ plot shows clear symmetrical patterns — for example if the points curve around the line as in the example below — it means the model does not meet the assumption of a linear relationship. In this example we are trying to model a sinus function with a linear model.\n\n\n\n\n\nExample of patterns in a QQ plot\n\n\n\n\n\n\n\n\n17.3.1.2 Constant variance\nWe can check the assumption of constant variance in the residuals using a scatter plot with residuals on the y-axis and either the estimated values or observed values of the explanatory or response variable on the x-axis. Typically, the estimated values of the response are used so that the x-axis reflects the entire model, but other variables may be useful to visualize to identify potential causes of a violated assumption.\n\nggplot(residualData) + \n  aes(x = yHat, y = residuals) + \n  geom_point(color = \"steelblue\") + \n  theme_bw() +\n  labs(x = \"Estimated Values\", y = \"Residuals\") + \n  geom_hline(\n    aes(yintercept = 0)\n  ) + \n  # Imaginary boundaries\n  geom_hline(\n    aes(yintercept = -0.05),\n    color = \"#d9230f\",\n    linetype = 2\n  ) + \n  geom_hline(\n    aes(yintercept = 0.05),\n    color = \"#d9230f\",\n    linetype = 2\n  )\n\n\n\n\n\n\n\nFigure 17.4: Spread of residuals against estimated values\n\n\n\n\n\nTo meet the assumption of constant variance, the points at each cross-section of x-values should be evenly spread around the same limits. Think of it as placing an imaginary boundary two parallel lines along the maximum and minimum values of the residuals (the two red dashed lines in Figure 17.4), and the majority of points should be scattered between them with no big gaps from the point to the boundary. In Figure 17.4, we see that the variation seem to diminish with larger estimated values as observations fall further from the boundary. Since the dataset is relatively small and only three of the observations (\\(\\hat{Y} &gt; -0.025\\)) cause the issue we could consider the residuals to meet the assumption of constant variance.\n\n\n\n\n\n\nImportant\n\n\n\nIf the lines covering the maximum and minimum values of the residuals are not parallel, the model does not meet the requirement of constant variance.\n\n\n\n\n\nExample of non-constant variance in residuals\n\n\n\n\n\n\n\nExample of non-constant variance in residuals\n\n\n\n\nThese phenomena usually mean that the entire model or parts of it need to be transformed to meet the assumption of constant variance.\nWe can also identify problems with linearity in this scatter plot. The figure below shows roughly constant variance in terms of variation across each cross-section of the x-axis, but there is a clear pattern in the residuals. This means the model has not successfully captured the relationship. In this case, it would be appropriate to visualize the residuals against each explanatory variable to identify which ones contribute to the non-linear relationship.\n\n\n\n\n\nPattern in residuals indicating a non-linear relationship\n\n\n\n\n\n\n\n\n17.3.1.3 Independence\nMost often we assess this assumption based on the data collection process. Only when we know the data has a time aspect — such as in time series data or when the same unit has been measured multiple times — we check that the model has accounted for this dependence properly.\nA line plot of residuals in observation order can be used to examine independence, but as mentioned, this visualization is only used in special cases. The line plot should show “randomness,” meaning no clear patterns in the residuals.\n\nggplot(residualData) + \n  aes(x = 1:nrow(residualData), y = residuals) + \n  geom_line(color = \"steelblue\") + \n  theme_bw() +\n  labs(x = \"Obs. index\", y = \"Residuals\") + \n  geom_hline(\n    aes(yintercept = 0),\n    color = \"black\"\n  )\n\n\n\n\nResiduals in observation order\n\n\n\n\nOther examples of data with dependence include:\n\nWe collect data from individuals, but some individuals come from the same family, which may create dependence between them.\nWe collect spatial data, such as temperature or rainfall at different locations in Östergötland. It is common for there to be positive correlation (a dependence) between geographically close observations.\n\n\n\n\n17.3.2 Creating a custom function (diagnosticPlots)\nYou will encounter these residual plots many times when conducting regression analyses, so to simplify their generation we can create a custom function that does all of this for us at once. The cowplot package provides a function (plot_grid) that can combine multiple plots into one.\n\n#' Custom function diagnosticPlots that generate visualizations of residuals\n#' @param model A fitted model object of class \"lm\"\n#' @param alpha Defines the opacity of the points (0-1)\n#' @param bins  Defines the number of bins in the histogram\n#' @param scaleLocation Boolean if a scale location graph should be added\n\ndiagnosticPlots &lt;- \n  function(\n    model, \n    alpha = 1, \n    bins = 10, \n    scaleLocation = FALSE\n  ) {\n    if (model |&gt; class() != \"lm\") {\n      stop(\"model must be an lm object\")\n    }\n    if (alpha &lt; 0 | alpha &gt; 1) {\n      stop(\"alpha must be between 0 and 1\")\n    }\n    if (bins &lt;= 0) {\n      stop(\"bins must be a positive number\")\n    }\n    \n    # Summarizes the residuals, observed and fitted values in a tibble\n    residualData &lt;- \n      dplyr::tibble(\n        residuals = residuals(model),\n        # The response variable is the first column in the model's model object\n        y = model$model[,1],\n        yHat = fitted(model)\n      )\n    \n    # Generates the histogram to assess normality\n    p1 &lt;- \n      ggplot2::ggplot(residualData) + \n      ggplot2::aes(x = residuals, y = after_stat(density)) +\n      ggplot2::geom_histogram(bins = bins, fill = \"steelblue\", color = \"black\") + \n      ggplot2::theme_bw() + \n      ggplot2::labs(x = \"Residuals\", y = \"Density\")\n    \n    # Generates the scatter plot to assess constant variance\n    p2 &lt;- \n      ggplot2::ggplot(residualData) + \n      ggplot2::aes(x = yHat, y = residuals) + \n      ggplot2::geom_hline(aes(yintercept = 0)) + \n      ggplot2::geom_point(color = \"steelblue\", alpha = alpha) + \n      ggplot2::theme_bw() +\n      ggplot2::labs(x = \"Estimated Values\", y = \"Residuals\")\n    \n    # Generates the QQ plot to assess normality\n    p3 &lt;- \n      ggplot2::ggplot(residualData) + \n      # Use standardized residuals\n      ggplot2::aes(sample = scale(residuals)) + \n      ggplot2::geom_qq_line() + \n      ggplot2::geom_qq(color = \"steelblue\", alpha = alpha) +\n      ggplot2::theme_bw() + \n      ggplot2::labs(x= \"Theoretical Quantiles\", y = \"Observed Quantiles\")\n    \n    # If scaleLocation is TRUE, add a scale location plot\n    if (scaleLocation) {\n      p4 &lt;- \n        ggplot2::ggplot(residualData) + \n        ggplot2::aes(x = yHat, y = sqrt(abs(residuals))) + \n        ggplot2::geom_point(color = \"steelblue\", alpha = alpha) + \n        ggplot2::theme_bw() +\n        ggplot2::labs(x = \"Estimated Values\", y = expression(sqrt(\"|Residuals|\")))\n      \n      cowplot::plot_grid(p1, p2, p3, p4, nrow = 2)\n      \n    } else {\n      cowplot::plot_grid(p1, p2, p3, nrow = 2)  \n    }\n    \n}\n\ndiagnosticPlots(simpleModel)\n\n\n\n\n\n\n\nFigure 17.5: Residual plots in a single image\n\n\n\n\n\nIn summary, Figure 17.5 shows that the residuals seem to meet the assumptions of normal distribution with mean 0 and constant variance. There are no clear patterns in any plot that suggest otherwise nor that the model fails to capture part of the relationship. A few outliers have been identified, specifically three observations with large estimated values. The conclusion is that the model is a suitable simplification of reality.\n\n\n17.3.3 More on diagnostics: Anscombe’s quartet\nTo further illustrate how diagnostics can be used to visually judge whether the assumptions of linear regression are met, let us take a look at a famous dataset that was designed for precisely this purpose (Anscombe 1973). The data are built into R (with the name anscombe), but are not in the most convenient format:\n\nprint(anscombe)\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\n\nThese are actually four datasets merged into one: x1 and y1 are \\(x\\)- and \\(y\\)-coordinates of the points from the first set, x2 and y2 from the second set, and so on. We can use pivot_longer to put these data in tidy format:\n\nansLong &lt;- \n  anscombe |&gt;\n  pivot_longer(cols = everything(), names_to = c(\".value\", \"set\"),\n               names_pattern = \"(.)(.)\")\nprint(ansLong)\n\n# A tibble: 44 × 3\n   set       x     y\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1        10  8.04\n 2 2        10  9.14\n 3 3        10  7.46\n 4 4         8  6.58\n 5 1         8  6.95\n 6 2         8  8.14\n 7 3         8  6.77\n 8 4         8  5.76\n 9 1        13  7.58\n10 2        13  8.74\n# ℹ 34 more rows\n\n\nWe can now visualize each set, along with linear fits:\n\nansLong |&gt;\n  ggplot(aes(x = x, y = y, color = set)) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  facet_wrap(~ set, nrow = 2, labeller = label_both) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe data have been carefully crafted so that the least-squares regression line has an intercept of 3 and a slope of 0.5 for each of the four sets. Furthermore, the p-values are also identical to many decimal places. But this visual representation reveals what would have been much harder to intuit otherwise: that only the first set has a real chance of conforming to the assumptions of linear regression. Performing the regression on just this set and creating diagnostic plots:\n\nlm(y ~ x, data = filter(ansLong, set == \"1\")) |&gt; summary()\n\n\nCall:\nlm(formula = y ~ x, data = filter(ansLong, set == \"1\"))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx             0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm(y ~ x, data = filter(ansLong, set == \"1\")) |&gt;\n  diagnosticPlots(bins = 5)\n\n\n\n\n\n\n\n\nWhile the number of data points is small, there is otherwise nothing to suggest in these diagnostic plots that there is anything wrong with the regression.\nThe situation changes for the other three sets. Let us look at set 2:\n\nlm(y ~ x, data = filter(ansLong, set == \"2\")) |&gt; summary()\n\n\nCall:\nlm(formula = y ~ x, data = filter(ansLong, set == \"2\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx              0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm(y ~ x, data = filter(ansLong, set == \"2\")) |&gt;\n  diagnosticPlots(bins = 5)\n\n\n\n\n\n\n\n\nBlindly reading off the p-values without considering the diagnostic plots might lead one to take them seriously. This would be wrong however, as the assumptions of the linear regression are clearly not fulfilled. The right diagnostics show that the residuals are not independent, and certainly not homoscedastic (equal variance). The QQ plot and histogram additionally shows that they are not even normally distributed.\nIn set 3, the trends are driven too much by a single outlier:\n\nlm(y ~ x, data = filter(ansLong, set == \"3\")) |&gt; summary()\n\n\nCall:\nlm(formula = y ~ x, data = filter(ansLong, set == \"3\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx             0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm(y ~ x, data = filter(ansLong, set == \"3\")) |&gt;\n  diagnosticPlots(bins = 5)\n\n\n\n\n\n\n\n\nAs before, the right diagnostic plots show that the independence of the residuals is violated. Finally, in set 4, the whole regression is based on a single point whose predictor x is different from that of the rest:\n\nlm(y ~ x, data = filter(ansLong, set == \"4\")) |&gt; summary()\n\n\nCall:\nlm(formula = y ~ x, data = filter(ansLong, set == \"4\"))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx             0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nlm(y ~ x, data = filter(ansLong, set == \"4\")) |&gt;\n  diagnosticPlots(bins = 5)\n\n\n\n\n\n\n\n\nClearly, the assumption that the residual variances are independent of the predictor is heavily violated.\n\n\n\n\n\n\nImportant\n\n\n\nThese examples are there to urge caution when interpreting regression statistics. This problem becomes much more acute when relying on multiple regression, where there is more than one predictor variable. Since high-dimensional data cannot be visualized as easily as the datasets above, often the diagnostic plots are the only way to tell whether the assumptions of regression hold or not.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "41_Linear_regression.html#sec-inference",
    "href": "41_Linear_regression.html#sec-inference",
    "title": "17  Simple linear regression",
    "section": "17.4 Statistical inference",
    "text": "17.4 Statistical inference\nOnce we have confirmed that the model is suitable, i.e. that it fulfills its assumptions, we can focus on interpreting the model’s results in relation to the population. In regression analysis, we can perform several types of statistical inference: on the entire model, on groups of parameters, or on individual parameters.\nWe can start with an F-test for the entire model to determine whether at least one slope parameter is significant — whether the model is worth investigating further — and then perform individual t-tests for each parameter to assess which explanatory variables have a significant effect on the response variable. Since qualitative variables often consist of multiple parameters, these need to be combined to examine the variable’s overall relationship, which we can do with a partial F-test.\n\n\n\n\n\n\nNote\n\n\n\nIn the case of simple linear regression the entire model, groups of parameters and individual parameters are all one and the same test because we only include one explanatory variable (one slope parameter) in the model. Once we start adding more variables to the model these three types of inference differ and we choose the one suitable for the analysis we want to perform.\n\n\nBefore diving into the details of the different tests, we need to present the ANOVA table, which is used to break down the variation in the response variable into the model’s components: the explanatory variables and the error term.\n\n17.4.1 ANOVA\nAnalysis of Variance is a collection of methods that calculate the variation of different model components. The goal of a model is to explain the total variation in the response variable as effectively as possible. Everything the explanatory variables help to describe is called the explained variation, and what the model fails to explain (the remaining error) is the unexplained variation.\n\\[\n\\underbrace{Y}_\\text{total variation} = \\underbrace{\\beta_0 + \\beta_1 \\cdot X}_\\text{explained variation} + \\underbrace{\\mathbf{\\varepsilon}}_\\text{unexplained variation}\n\\tag{17.5}\\]\nEquation 17.5 shows that the total variation is the sum of the explained and unexplained variation, which is also reflected in the formulas for each component. Each component is calculated as follows:\n\\[\n  \\text{total variation} = SST = \\sum_{i=1}^n(Y_i - \\bar{Y})^2\n\\] This corresponds to the numerator for the variance of \\(Y\\). The total variation describes how much variation exist if we were to use the mean of \\(Y\\) as the model.\n\\[\n  \\text{unexplained variation} = SSE = \\sum_{i=1}^n(Y_i - \\hat{Y_i})^2\n\\]\nWe have previously used SSE as a measure of model error (see Equation 17.3), describing the variation of the observed and estimated value.\n\\[\n  \\text{explained variation} = SSR = \\sum_{i=1}^n(\\hat{Y_i} - \\bar{Y})^2\n\\]\nSSR describes the variation between the model’s estimated values and the mean of \\(Y\\). This can be interpreted as how much more variation the model explains compared to the mean, or simply, how much better the model is at explaining variation in \\(Y\\) than only its mean.\n\n\n\n\n\n\nNote\n\n\n\nAdding these all together we can simplify its sum as: \\[\n  \\sum_{i=1}^n(Y_i - \\bar{Y})^2 = \\sum_{i=1}^n(\\underbrace{\\hat{Y_i}}_\\text{positive estimates} - \\bar{Y})^2 + \\sum_{i=1}^n(Y_i - \\underbrace{\\hat{Y_i}}_\\text{negative estimates})^2\n\\] where the positive and negative estimates cancel each other out to produce the left hand side.\n\n\nWe can also visualize this relationship in a stacked bar chart. The total height of the bar is SST, while the different segments show how much of the total variation is explained or unexplained in a given model.\n\n\n\n\n\nVisualization of the different sources of variation\n\n\n\n\n\n17.4.1.1 ANOVA Table\nAn ANOVA table is a way to efficiently summarize these components and show additional information, such as degrees of freedom (\\(df\\)) for each component and mean squares.\nDegrees of freedom describe how many slope parameters are estimated for each part3 and mean squares show the average variation per degree of freedom, \\(\\frac{SS}{df}\\).\n\n\n\n\nTable 17.2: Simple ANOVA table\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nDF\nSum of Squares\nMean Square\n\n\n\n\nModel (Regression)\n\\(df_R = k\\)\n\\(SSR\\)\n\\(MSR = \\frac{SSR}{df_R}\\)\n\n\nError\n\\(df_E = n - (k + 1)\\)\n\\(SSE\\)\n\\(MSE = \\frac{SSE}{df_E}\\)\n\n\nTotal\n\\(df_T = n - 1\\)\n\\(SST\\)\n\n\n\n\n\n\n\n\n\nA simple ANOVA table like Table 17.2 shows only the three main components, but different software may display other breakdowns by default. In a multiple linear regression model, it is common to further divide the explained variation, for example into sequential sums of squares (Section 18.1.7.1).\nUsing the different sources of variation, we can calculate tests for the whole model or parts of it using various F-tests, while the individual parameter estimates and their associated standard errors can be used in tests for individual slope parameters.\n\n\n\n17.4.2 F-test for the Model\nIn linear regression, an F-test for the entire model is a good starting point to determine whether at least one slope parameter is significant. We examine the hypotheses:\n\\[\\begin{align*}\nH_0&: \\beta_1 = \\beta_2 = \\beta_3 = \\cdots = \\beta_k = 0\\\\\nH_a&: \\text{At least one of } \\beta_j \\text{ in } H_0 \\text{ is different from } 0\n\\end{align*}\\]\nIf at least one slope parameter is significant, it means there is at least one variable that contributes some explained variation, hence the model is better than using just \\(\\bar{Y}\\). The test statistic examines the relationship between explained and unexplained variation via their mean squares:\n\\[\nF_{test} = \\frac{SSR / k}{SSE / (n - (k+1))} = \\frac{MSR}{MSE}\n\\]\nThe test statistic follows an F-distribution governed by two degrees of freedom: \\(df_1\\) from the numerator and \\(df_2\\) from the denominator in the calculation, i.e., the model and error degrees of freedom respectively. If \\(H_0\\) is true, the test statistic will be 0, while if \\(H_a\\) is true, the test statistic will be a large positive number. Since both mean squares are positive, the ratio will always be positive, and we can reject \\(H_0\\) if the test statistic is sufficiently far from 0.\n\n\n\n\n\nDifferent F-distributions and their degrees of freedom\n\n\n\n\nUsing the output from summary(), we can find the result of the F-test at the bottom row.\n\nsummary(simpleModel)\n\n\nCall:\nlm(formula = shape ~ size, data = nux)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.048880 -0.016772  0.001423  0.013181  0.048515 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -0.129240   0.040706  -3.175  0.00307 **\nsize         0.004083   0.001777   2.297  0.02752 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02275 on 36 degrees of freedom\nMultiple R-squared:  0.1279,    Adjusted R-squared:  0.1036 \nF-statistic: 5.278 on 1 and 36 DF,  p-value: 0.02752\n\n\nF-statistic: 5.278 on 1 and 36 DF,  p-value: 0.02752 the p-value can be interpreted as we would any hypothesis test, comparing it against the chosen significance level (chosen before estimating the model!). Since the p-value is less than 5 percent, we can reject \\(H_0\\) and conclude that at least one of the variables is associated with the response variable.4\n\n\n17.4.3 t-test for individual parameters\nEven though the F-test for the model and the individual t-test for the slope is the same test in a simple linear regression, this is not the case for a multiple regression model. As such it is not appropriate to look at the ANOVA table when we want to examine individual parameters. Instead, we should use the coefficient table from the summary() output.\nFormally, the hypotheses are:\n\\[\n\\begin{aligned}\nH_0 &: \\beta_j = 0\\\\\nH_a &: \\beta_j \\ne 0\n\\end{aligned}\n\\]\nwhere \\(j\\) is one of the slope parameters in the model.\nThe test statistic is calculated from the estimated slope parameter and its standard error:\n\\[\nt_{test} = \\frac{b_j - 0}{s_{b_j}}\n\\]\nThe test statistic follows a t-distribution under \\(H_0\\) with \\(n - (k + 1)\\) degrees of freedom.\nIn R, t-tests are included in the coefficient table, which we can extract from the summary() object using coef().\n\nsummary(simpleModel) |&gt; \n  coef() |&gt; \n  round(4) |&gt; \n  kable(format = \"markdown\",\n        col.names = \n          c(\n            \"Variable\", \n            \"Estimate\", \n            \"Std. Error\", \n            \"t-value\", \n            \"p-value\"\n          ), \n        parse = TRUE) |&gt; \n  kable_styling(\"striped\")\n\n\n\nTable 17.3: Coefficient table for a model with associated t-tests for individual parameters\n\n\n\n\n \n  \n    Variable \n    Estimate \n    Std. Error \n    t-value \n    p-value \n  \n \n\n  \n    (Intercept) \n    -0.1292 \n    0.0407 \n    -3.1749 \n    0.0031 \n  \n  \n    size \n    0.0041 \n    0.0018 \n    2.2974 \n    0.0275 \n  \n\n\n\n\n\n\n\nIn Table 17.3, we see that the p-value for the slope is less than 5 percent which means we can reject \\(H_0\\) at the 5% significance level, meaning the variable has a significant effect on the response variable.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "41_Linear_regression.html#sec-r-square",
    "href": "41_Linear_regression.html#sec-r-square",
    "title": "17  Simple linear regression",
    "section": "17.5 Simple evaluation metrics",
    "text": "17.5 Simple evaluation metrics\nJust because a model is appropriate, meets model assumptions, and contains significant parameters does not mean it is the best model that can explain the response or even that it is a good one. With the help of various evaluation metrics, we can get an overview of how good the model is.\nThe coefficient of determination (\\(R^2\\)) describes what proportion of the total variation is explained by the model’s explanatory variables. With this description, we can calculate \\(R^2\\) as:\n\\[\nR^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\n\\] Given that this is a proportion of the total variation its values range from 0 (model is useless at explaining the response) to 1 (model perfectly explains the response) and is interpreted in percent.\nThe coefficient of determination is called multiple R-squared in R and can be found at the second to last line in the summary() output. Multiple R-squared:  0.1279 is interpreted as approximately 13 percent of the total variation in snail shape is explained by the snail size, which is a very low value.\n\n\n\n\n\n\nImportant\n\n\n\nGood models typically have an \\(R^2\\) value above 0.5, meaning that at least half of the total variation is explained by the model. However, this is not a strict rule and depends on the context of the analysis. In some fields, such as social sciences, an \\(R^2\\) value of 0.3 or even lower may be considered acceptable.\nWhenever we get a low coefficient, we should consider the model structure and whether it is appropriate for the data. We can also consider adding more explanatory variables to the model, transforming existing variables, or using a different type of model altogether.\nSometimes there is just too much variation in the data that cannot be explained by any measured variable, and the best model we can produce is still a poor one. In these cases, we should be careful not to add too many variables or transformations that do not significantly improve the model’s predictive power.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "41_Linear_regression.html#sec-predictions",
    "href": "41_Linear_regression.html#sec-predictions",
    "title": "17  Simple linear regression",
    "section": "17.6 Predictions",
    "text": "17.6 Predictions\nWe can also use the estimated model to predict new values of the response variable for new observations. From the estimated regression line, we substitute each variable with the observation’s actual value and get a simple sum that describes the response variable’s value on the line.\nPredictions involve estimating the value of \\(Y\\) given observed values of \\(X\\) using the estimated regression line. These predictions will fall along the line, which further motivates that the model needs to be appropriate and good. We don’t want a prediction in one area to be more accurate than a prediction in another, or for the regression line to generally be a poor representation of the response variable. The model is based on a specific domain, the observed values of \\(X\\), and predictions should be made within this domain.\nThere are certain cases, such as in time series analysis, where predictions are made outside the domain, but there is a time dependency that enables these extrapolations. In “normal” regression, we should avoid extrapolating the regression line outside the domain.\n\n17.6.1 Mean of Y for given \\(X\\)\nIf we are interested in the average value of the response variable for all new observations with given values of \\(X\\), we can estimate \\(\\mu_{Y|X_0}\\) where \\(X_0\\) contains values for the new observation. We use the estimated regression model and calculate a point prediction of the response variable with the help of predict() which substitutes the values of \\(X_0\\) into the regression equation:\n\nx0 &lt;- \n  tibble(\n    size = 23\n  )\n\npredict(\n  simpleModel, \n  newdata = x0,\n  interval = \"confidence\"\n  )\n\n         fit         lwr         upr\n1 -0.0353207 -0.04283735 -0.02780405\n\n\nThe interval estimate for a mean becomes a confidence interval because we are making inference of all possible new observations with the shell size of 23 units.\n\n\n17.6.2 Individual prediction of Y for given \\(X\\)\nIf we are instead interested in a single value of \\(Y\\) with given values of \\(X\\), we can estimate \\(Y_{X_0}\\). Since the prediction uses the same estimated model the point estimate will be the same as the mean prediction, but the interval estimate will be wider because it accounts for the additional uncertainty of predicting a single value rather than the mean of all possible values. We can use predict() with the interval argument set to \"prediction\":\n\npredict(\n  simpleModel, \n  newdata = x0,\n  interval = \"prediction\"\n  )\n\n         fit         lwr        upr\n1 -0.0353207 -0.08206763 0.01142622\n\n\nThe interval estimate of an individual observation becomes a prediction interval because we only account for one new observation with the shell size of 23 units.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "41_Linear_regression.html#sec-exercises-regression",
    "href": "41_Linear_regression.html#sec-exercises-regression",
    "title": "17  Simple linear regression",
    "section": "17.7 Exercises",
    "text": "17.7 Exercises\nThe file plant_growth_rate.csv contains individual plant growth data (mm/week), as a function of soil moisture content.\n\nInvestigate the type and scale of the variables.\nSummarize descriptive statistics for each variable.\nVisualize the distribution of each variable.\nCreate a scatter plot showing the relationship between the two variables in the sample. Interpret it based on the four key pieces of information a scatter plot can provide.\nSummarize your observations and justify whether or not you think a simple linear regression model is suitable.\nFit a linear regression model using lm() and interpret the regression coefficients.\nCreate diagnostic plots to check whether the assumptions of the model are satisfied.\nPerform an F-test for the model and interpret the result. Do the same for a t-test for the slope parameter.\nSummarize your findings along with the coefficient of determination and determine whether or not the model is suitable for predicting plant growth based on soil moisture content.\n\nIt is difficult to measure the height of a tree. By contrast, the diameter at breast height (DBH) is easy to measure. Can one infer the height of a tree by measuring its DBH? The built-in dataset trees contains DBH data (labeled Girth), as well as measured height and timber volume of 31 felled black cherry trees. You can ignore timber volume, and focus instead on how well DBH predicts tree height.\n\nRepeat steps 1-9 from the previous exercise using the trees dataset.\nPredict the height of a tree with a DBH of 20 cm, and interpret a confidence interval for the mean height of all trees with that DBH.\n\n\n\n\n\nAnscombe, Francis J. 1973. “Graphs in Statistical Analysis.” American Statistician 27 (1): 17–21. https://doi.org/10.1080/00031305.1973.10478966.\n\n\nKendall, Maurice G. 1955. Rank Correlation Methods, 2nd Ed. Oxford, England: Hafner Publishing Co.\n\n\nSpearman, C. 1904. “The Proof and Measurement of Association Between Two Things.” The American Journal of Psychology 15 (1): 72–101. http://www.jstor.org/stable/1412159.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "41_Linear_regression.html#footnotes",
    "href": "41_Linear_regression.html#footnotes",
    "title": "17  Simple linear regression",
    "section": "",
    "text": "Or other measures of association strength, e.g., Kendall (Kendall 1955) or Spearman (Spearman 1904).↩︎\nWhy the sum of squared residuals, and not just their simple sum? This is to prevent very large positive and very large negative deviations canceling each other out in the sum, making it appear as if the total deviation was very small. Squared deviations are always non-negative and therefore do not suffer from this cancellation problem.↩︎\nDegrees of freedom actually describe how many pieces of independent information are available for a calculation. Think back to the calculation of sample standard deviation, where the degrees of freedom are \\(n - 1\\), the number of observations minus 1, because we estimate the mean when calculating the standard deviation.↩︎\nIf we had made a different decision (not rejecting the null hypothesis), it would not be relevant to continue with the analysis, or at least the remaining analysis should focus on investigating why a linear regression model we expect to show a relationship based on pairwise scatter plots does not show it collectively.↩︎",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "42_Multiple_and_Nonlinear_regression.html",
    "href": "42_Multiple_and_Nonlinear_regression.html",
    "title": "18  Advanced regression topics",
    "section": "",
    "text": "18.1 Multiple linear regression\nIn the vast majority of cases, a simple linear regression model will not be sufficient to model a phenomena of a response variable. Instead the more general case involves multiple explanatory variables of different types that together allow us to describe the response. We can extend the theory presented in Chapter 17 to multiple linear regression, expand to non-linear regression and non-parametric regression methods that does not require the same assumptions of the data or model.\nFollowing the steps in simple linear regression we begin with visualizing the data. In the case of multiple linear regression we now have a set of possible explanatory variables at our disposal to visualize and assess their viability in the model.\nAs an example we will be using the penguins dataset collected by a research team in Antarctica between 2007 and 2009. The team gathered information on 333 penguins across three islands near the Palmer Research Station. The dataset is available via the palmerpenguins package (Horst, Hill, and Gorman 2020) and can be loaded into R using the following code:\n# Don’t forget to install the package if you haven’t already\n# install.packages(\"palmerpenguins\")\n\n# Load the package with the dataset\nrequire(palmerpenguins)\n\n# Filter out observations with missing values\npenguins &lt;- \n  penguins |&gt; \n  filter(!is.na(sex))\nWe can take a closer look at a sample of the dataset in Table 18.1.\n# Generate a formatted table using kable() and style it with kable_styling()\npenguins |&gt; \n  slice_head(n = 5) |&gt; \n  kable() |&gt; \n  kable_styling(\"striped\")\n\n\n\nTable 18.1: Sample of observations from the dataset.\n\n\n\n\n \n  \n    species \n    island \n    bill_length_mm \n    bill_depth_mm \n    flipper_length_mm \n    body_mass_g \n    sex \n    year \n  \n \n\n  \n    Adelie \n    Torgersen \n    39.1 \n    18.7 \n    181 \n    3750 \n    male \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    39.5 \n    17.4 \n    186 \n    3800 \n    female \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    40.3 \n    18.0 \n    195 \n    3250 \n    female \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    36.7 \n    19.3 \n    193 \n    3450 \n    female \n    2007 \n  \n  \n    Adelie \n    Torgersen \n    39.3 \n    20.6 \n    190 \n    3650 \n    male \n    2007\nFrom the table, we can identify the following variables:\nWe will focus on bill length as our response variable in the following examples. Since the dataset is from an observational study, we cannot draw conclusions about causal relationships, but we can investigate correlations between the penguins’ various characteristics.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Advanced regression topics</span>"
    ]
  },
  {
    "objectID": "42_Multiple_and_Nonlinear_regression.html#sec-multiple-regression",
    "href": "42_Multiple_and_Nonlinear_regression.html#sec-multiple-regression",
    "title": "18  Advanced regression topics",
    "section": "",
    "text": "Note\n\n\n\nAs part of the data exploration, we can identify missing values in some variables and we choose to filter them out in this example. Handling missing values is a large field in statistics, and there are several methods that can impute (estimate the missing value) so we don’t have to remove entire observations from the analysis.\nA simple imputation method is mean imputation, where we replace the missing value with the mean of the other measurements, or the mode if a qualitative variable is being imputed. In practice, more advanced methods are used that can be applied in many different cases and also take other information about the observations into account.\n\n\n\n\n\n\nspecies: The penguin’s species is a qualitative variable. We cannot say one species is “better” or “larger” than another. The categories cannot be ranked, so this variable follows a nominal scale.\nisland: The island where the penguin was located at the time of measurement is also a qualitative variable that cannot be ranked. This variable also follows a nominal scale.\nbill_length_mm: A quantitative variable measuring the bill length in millimeters (mm). Length is a typical variable that follows a ratio scale because it has a clear zero point.\nbill_depth_mm: Measures the bill depth in millimeters and follows the same reasoning as bill length.\nflipper_length_mm: Another variable measuring length, this time the flipper length. Same reasoning applies.\nbody_mass_g: The penguin’s weight in grams. Weight also has a clear zero point and is considered quantitative, following a ratio scale.\nsex: The penguin’s biological sex is a qualitative variable that cannot be ranked—nominal scale.\nyear: This variable is a bit harder to assess. It measures the year the penguin was measured as a numeric variable (integers, so R stores it as int), but in this context, it may not be considered quantitative. For simplicity, we can say that since it’s possible to calculate differences between years (e.g., 1 year between 2007 and 2008), but there’s no clear zero point, this variable can be considered quantitative and follows an interval scale.\n\n\n\n18.1.1 Visualizing the response variable\nAs a first step in the exploratory analysis, we can visualize the distribution of the response variable using a histogram.\n\nggplot(penguins) + aes(x = bill_length_mm) + \n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"black\") +\n  theme_bw() +\n  labs(x = \"Bill Length (mm)\", y = \"Count\")\n\n\n\n\n\n\n\nFigure 18.1: Histogram of bill length distribution\n\n\n\n\n\nFigure 18.1 gives us a picture of the variable’s characteristics and whether the data contains any outliers that might be difficult to capture with a model. Bill length appears to have a bimodal structure with two centers of mass around 38–40 and 50 mm. Most observations fall between approximately 35–52 mm, but there are a few around 58–60 mm that seem unusually large.\nThe fact that the distribution doesn’t look normal doesn’t matter, we need to examine the distribution of the response variable with respect to the explanatory variables to check a regression model’s assumptions.\n\n\n18.1.2 Pairwise Relationships\nThe dataset contains several potential explanatory variables that could be included in a model. Depending on how a study was conducted, variables may be excluded if they don’t have a logical relationship with the response variable, for example, ID variables are not relevant to investigate. In our example data, there is a variable describing the year the observation was made, which we can initially assume doesn’t have a logical relationship with bill length. That leaves six other variables that could be included in the model fitting.\n\n18.1.2.1 Quantitative explanatory variables\nSimilar to Figure 17.1 we create scatter plots between each pair of continuous explanatory variable and the continuous response and investigate the four questions about the relationship.\n\nIs the relationship linear?\nIs the relationship positive or negative?\nIs the relationship strong or weak?\nAre there any outliers?\n\n\nggplot(penguins) + aes(x = body_mass_g, y = bill_length_mm) +\n  geom_point(color = \"steelblue\") + \n  theme_bw() + \n  labs(x = \"Body Weight (g)\", y = \"Bill Length (mm)\")\n\n\n\n\n\n\n\nFigure 18.2: Scatterplot showing the relationship between body weight and bill length\n\n\n\n\n\nFigure 18.2 shows that the relationship appears mostly linear, as a constant change (increase) in body weight leads to a constant change (increase) in bill length. Most points seem to follow this trend, indicating a relatively strong relationship. However, there are several observations (highlighted in Figure 18.3) that deviate from this. These observations have lower body weight but the same bill length as penguins with higher body weight, which affects the strength of the relationship.\n\n\n\n\n\n\n\n\nFigure 18.3: Scatterplot with highlighted area in the circle\n\n\n\n\n\n\\[\nr = 0.589\n\\]\nSince the correlation coefficient is close to 0.6, it suggests the relationship is moderately strong.\nFigure 18.2 also shows some observations that could be considered outliers. For example, \\(\\{x = ~2700, y = ~47\\}\\) and \\(\\{x = ~3700, y = ~58\\}\\) are observations that deviate significantly from the expected relationship and other observations. A detailed analysis of outliers will be covered in later chapters, but for exploratory purposes, we note that some observations may affect model fitting.\nIn summary, the relationship between body weight and bill length is:\n\nlinear,\n\npositive,\n\nmoderately strong,\n\nwith possible outliers.\n\nWhen we build our first model, it will likely be sufficient to include a simple \\(\\beta_1 \\cdot \\text{body weight}\\) term.\n\n\n18.1.2.2 Other quantitative variables\nThe same exploration should be done for all quantitative variable pairs, in this case also bill depth and flipper length:\n\n\n\n\n\n\n\n\nFigure 18.4: Scatterplot showing the relationship between bill depth and bill length\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 18.5: Scatterplot showing the relationship between flipper length and bill length\n\n\n\n\n\nBoth variables appear to have a linear relationship with the response variable. Figure 18.4 suggests that the relationship between bill depth and bill length is weakly negative (\\(r = -0.229\\)) as the points are widely scattered, while Figure 18.5 suggests a somewhat stronger positive relationship (\\(r = 0.653\\)), similar to Figure 18.2.\nA new phenomenon visible in Figure 18.4 is that we seem to have several clusters of points, each with a positive relationship, even though the overall relationship was interpreted as weakly negative. If we had only calculated the correlation coefficient, this phenomenon would have escaped our analysis. Figure 18.6 is an example of Simpson’s Paradox, which we will explore further later in this material.\n\n\n\n\n\n\n\n\nFigure 18.6: Groupings of observations\n\n\n\n\n\n\n\n18.1.2.3 Qualitative explanatory variables\nWe cannot use scatter plots to visualize the relationship between qualitative explanatory variables and a continuous response variable. Instead, we need visualizations that account for the qualitative scale, usually ordinal or nominal. There are several ways to visualize the distribution of the response variable across levels of the explanatory variable, such as grouped histograms or box plots, but one type of visualization that shows distribution details is a violin plot. A violin plot is a mirrored density plot, where areas with many observations have a larger area under the curve.\nUsing ggplot2, we can create such a plot with geom_violin():\n\nggplot(penguins) + \n  aes(x = species, y = bill_length_mm) +\n  geom_violin(fill = \"steelblue\") + \n  theme_bw() + \n  labs(x = \"Species\", y = \"Bill Length (mm)\")\n\n\n\n\n\n\n\nFigure 18.7: Distribution of bill length by species\n\n\n\n\n\nFigure 18.7 shows that Adelie penguins generally have shorter bill lengths compared to Chinstrap and Gentoo, as the center of the distribution is around 38–40 mm. Chinstrap penguins have a slightly larger proportion with bill lengths over 50 mm, while Gentoo have a larger proportion under 50 mm.\n\n\n\n\n\n\n\n\nFigure 18.8: Distribution of bill length by sex\n\n\n\n\n\nFigure 18.8 has a slightly different shape, with two large masses for each category. This likely indicates that sex within species has an effect, and that males generally have longer bills than females of the same species.\n\n\n\n\n\n\n\n\nFigure 18.9: Distribution of bill length by island\n\n\n\n\n\nFigure 18.9 suggests that penguins on Torgersen Island have shorter bill lengths than those on other islands. However, we need to consider whether this variable actually describes the relationship or if another phenomenon explains it, for example if one species is more prevalent on one specific island, like a confounder/control.\nThe conclusion from these visualizations is that there appears to be a relationship between species and sex with bill length, and these two variables should be included in the model. We now need to consider how to appropriately include a qualitative variable containing text in a mathematical model that requires numerical input.\n\n\n\n18.1.3 Multiple linear regression model\nAfter summarizing observations from visualizations and descriptive statistics, the next step in the process is to build the structure of the model. The simplest approach is to start with the quantitative variables, which (usually) only require one term each in the model. \\[\n\\begin{aligned}\n  \\text{Bill Length} = &\\beta_0 + \\beta_1 \\cdot \\text{Body Weight} + \\beta_2 \\cdot \\text{Flipper Length} + \\\\\n&\\beta_3 \\cdot \\text{Bill Depth} + \\cdots + \\varepsilon\n\\end{aligned}\n\\tag{18.1}\\]\n\n\n\n\n\n\nImportant\n\n\n\nRegardless of whether the model includes one or several explanatory variables, we must always keep in mind the five assumptions presented in Section 16.1.1, especially the assumption of linearity. If we’ve discovered non-linear relationships in the pairwise visualizations, it’s often not enough to include just a single term in the model, but we might need to look at transformations.\n\n\n\n\n18.1.4 Indicator variables\nA regression model cannot directly handle qualitative variables, such as \\(\\beta_4 \\cdot \\text{Species}\\), because the variable’s values represent categories, not values from a numerical scale. This applies even if the qualitative variable is numerically coded, e.g. 1, 2, 3 for each species. A slope parameter describes the constant change in the response variable when the corresponding explanatory variable increases by one unit, but a qualitative variable usually has no unit and no constant changes between adjacent values. Instead, we must transform the qualitative variable into numeric form using indicator variables (also called dummy variables).\nAs the name suggests, indicator variables are used to indicate which category an observation has for the qualitative variable. We need to create a limited number of indicator variables that clearly show exactly one category per observation.\nAssume a qualitative variable has 3 categories: \\[\n\\begin{bmatrix}A\\\\B\\\\C\\end{bmatrix}\n\\] We can start by creating an indicator variable for category A that takes the value 1 if the observation has category A, and 0 otherwise:\n\\[\n\\begin{bmatrix}A\\\\B\\\\C\\end{bmatrix} = \\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}\n\\] With only one indicator variable, we cannot clearly identify whether an observation has category B or C, since both have the value 0. So we add another indicator variable that takes the value 1 if the observation has category B, and 0 otherwise:\n\\[\n\\begin{bmatrix}A\\\\B\\\\C\\end{bmatrix} = \\begin{bmatrix}1 & 0\\\\0 & 1\\\\0 & 0\\end{bmatrix}\n\\] It might seem natural to create an indicator for the last category as well, but it’s not necessary. If both indicators are 0, we’ve successfully identified that the observation has category C, and another variable would just be redundant.\nThe last category also becomes our reference category, the category against which the effects of the other indicator variables are interpreted. When interpreting slope parameters for indicator variables — e.g., the indicator for A — the change in \\(Y\\) is measured when \\(X = A\\) compared to when \\(X = C\\).\n\n\n\n\n\n\nImportant\n\n\n\nMathematically, creating three indicator variables would model a perfect relationship and cause singularity issues in the calculations.\n\n\nIn general, we create \\(\\text{number of categories} - 1\\) indicator variables for each qualitative variable to be included in a regression model. The choice of reference category is arbitrary, but typically the first or last category is used.\nTo complete the modeling from Equation 18.1, we’ll include Species and Sex in the model. We need to create two sets of indicator variables with two and one indicators respectively:\n\\[\\begin{align*}\n  Gentoo &= \\begin{cases}\n            1 \\quad \\text{if species is Gentoo}\\\\\n            0 \\quad \\text{otherwise}\n        \\end{cases}\\\\\n  Chinstrap &= \\begin{cases}\n      1 \\quad \\text{if species is Chinstrap}\\\\\n      0 \\quad \\text{otherwise}\n  \\end{cases}\n\\end{align*}\\]\nand\n\\[\\begin{align*}\n  Male &= \\begin{cases}\n            1 \\quad \\text{if male}\\\\\n            0 \\quad \\text{otherwise}\n        \\end{cases}\n\\end{align*}\\]\nto finally create the following model: \\[\n\\begin{aligned}\n  \\text{Bill Length} = &\\beta_0 + \\beta_1 \\cdot \\text{Body Weight} + \\beta_2 \\cdot \\text{Flipper Length} + \\\\\n  &\\beta_3 \\cdot \\text{Bill Depth} + \\beta_4 \\cdot \\text{Gentoo} + \\beta_5 \\cdot \\text{Chinstrap} + \\\\\n  &\\beta_6 \\cdot \\text{Male} + \\varepsilon\n\\end{aligned}\n\\tag{18.2}\\] where Adelie and Female act as the reference categories for each qualitative variable.\n\n\n18.1.5 Model estimation\nWe can denote the estimated model with its regression coefficients as:\n\\[\n\\begin{aligned}\n  \\hat{y}_i = &b_0 + b_1 \\cdot x_{1i} + b_2 \\cdot x_{2i} + b_3 \\cdot x_{3i} + \\\\\n  &b_4 \\cdot x_{4i} + b_5 \\cdot x_{5i} + b_6 \\cdot x_{6i}\n\\end{aligned}\n\\tag{18.3}\\]\nwhere \\[\\begin{align*}\n  \\hat{y}_i &= \\text{estimated value of the response variable for observation } i\\\\\n  b_0 &= \\text{estimate of the intercept}\\\\\n  b_1 - b_6 &= \\text{estimates of the slope parameters}\n\\end{align*}\\]\nTo estimate this linear regression model in R, we again make use of the lm() function but adjust the formula argument to encompass more than one variable. Generally, the format is y ~ x, where x consists of the explanatory variables, for example bill_length_mm ~ body_mass_g + bill_depth_mm but there exist a shorthand (~ .) that makes use of all other variables present in the data in the right hand side.\nIn order for this to work we need to select only the variables we want to include in the model before AND we must also ensure that all variables in the dataset have the correct variable type as expected. We identified in Section 17.1 that we had three quantitative variables and two qualitative variables, which in R correspond to the types numeric and Factor. Using Factor simplifies the transformation into indicator variables because R knows it must do so for the model to work. If the qualitative variables were of type character or coded as numeric, R might not create indicator variables and the estimated model would be misinterpreted. We can inspect the variable types in penguins using str().\n\n# Only include variables that were found to be related to the response variable\nmodelData &lt;- \n  penguins |&gt; \n  select(\n    species,\n    bill_depth_mm,\n    flipper_length_mm,\n    body_mass_g,\n    sex,\n    bill_length_mm\n  )\n\n# Fit the specified model\nsimpleModel &lt;- lm(formula = bill_length_mm ~ ., data = modelData)\n\n\nsummary(simpleModel) |&gt; \n  coef() |&gt; \n  as_tibble(rownames = NA) |&gt; \n  rownames_to_column() |&gt; \n  rename(\n    ` ` = rowname,\n    Estimate = Estimate,\n    StdError = `Std. Error`,\n    `t-value` = `t value`,\n    `p-value` = `Pr(&gt;|t|)`\n  ) |&gt; \n  kable(\n    digits = 4\n  ) |&gt; \n  kable_styling(\"striped\")\n\n\n\nTable 18.2: A clean display of the model’s estimated parameters\n\n\n\n\n \n  \n     \n    Estimate \n    StdError \n    t-value \n    p-value \n  \n \n\n  \n    (Intercept) \n    15.0166 \n    4.3742 \n    3.4330 \n    0.0007 \n  \n  \n    speciesChinstrap \n    9.5655 \n    0.3497 \n    27.3508 \n    0.0000 \n  \n  \n    speciesGentoo \n    6.4044 \n    1.0304 \n    6.2154 \n    0.0000 \n  \n  \n    bill_depth_mm \n    0.3130 \n    0.1541 \n    2.0316 \n    0.0430 \n  \n  \n    flipper_length_mm \n    0.0686 \n    0.0232 \n    2.9608 \n    0.0033 \n  \n  \n    body_mass_g \n    0.0011 \n    0.0004 \n    2.5617 \n    0.0109 \n  \n  \n    sexmale \n    2.0297 \n    0.3892 \n    5.2153 \n    0.0000 \n  \n\n\n\n\n\n\n\nTable 18.2 shows the estimated slope parameters (coefficients). For example, we can see that for each additional gram a penguin weighs, the bill length increases by approximately 0.0011 mm on average given that all other variables are held constant.\n\n\n\n\n\n\nImportant\n\n\n\nThis last part of the interpretation must be included in a multiple linear regression, as a change in multiple variables would result in a combined change in the response variable relative to each coefficient. In order to isolate one variable’s relationship with the response, we need to isolate its interpretation as well.\n\n\nIndicator variables are interpreted within each set of connected coefficients, relative to the assigned reference category. For example, Gentoo penguins have on average a 6.4 mm longer bill than the reference category Adelie penguins, given that all other variables are held constant.\n\n\n18.1.6 Model evaluation\nAfter estimating a model based on observations from visualizations and descriptive statistics, we move on to the residual analysis to determine if th model is appropriate and meets the model assumptions. Using the custom function we designed in Section 17.3.2 we simply input the estimated model:\n\ndiagnosticPlots(simpleModel)\n\n\n\n\n\n\n\nFigure 18.10: Residual analysis for the penguins model\n\n\n\n\n\nIn summary, Figure 18.10 shows that the residuals meet the assumptions of normal distribution with mean 0 and constant variance. There are no clear patterns in any plot that suggest otherwise or that the model fails to capture part of the relationship. A few outliers have been identified, specifically two large positive residuals which might warrant a closer look as to why they are so large, but nothing that deviates from the model assumptions.\n\n\n18.1.7 Model inference\nUsing the different sources of variation, we can calculate tests for the whole model or parts of it using various F-tests, while the individual parameter estimates and their associated standard errors can be used in tests for individual slope parameters.\n\n18.1.7.1 Sequential Sums of Squares\nThe calculations for an ANOVA table are performed automatically in R when we use lm(), and we can extract the table from the model object using anova() (see Tip 17.1).\n\nanova(simpleModel) |&gt; \n  round(4) |&gt; \n  kable() |&gt; \n  kable_styling(\"striped\")\n\n\n\nTable 18.3: ANOVA table from R\n\n\n\n\n \n  \n     \n    Df \n    Sum Sq \n    Mean Sq \n    F value \n    Pr(&gt;F) \n  \n \n\n  \n    species \n    2 \n    7015.3857 \n    3507.6929 \n    713.4929 \n    0 \n  \n  \n    bill_depth_mm \n    1 \n    818.5050 \n    818.5050 \n    166.4905 \n    0 \n  \n  \n    flipper_length_mm \n    1 \n    198.2269 \n    198.2269 \n    40.3210 \n    0 \n  \n  \n    body_mass_g \n    1 \n    160.3760 \n    160.3760 \n    32.6218 \n    0 \n  \n  \n    sex \n    1 \n    133.7191 \n    133.7191 \n    27.1995 \n    0 \n  \n  \n    Residuals \n    326 \n    1602.6899 \n    4.9162 \n     \n     \n  \n\n\n\n\n\n\n\nBy default, R breaks down the model’s sum of squares (SSR) into the individual explanatory variables using sequential (also called conditional) sums of squares. A sequential sum of squares describes how much variation an explanatory variable contributes given that the model already includes other explanatory variables.\nThe order presented in Table 18.3 is the order in which variables are added to the model. For example, the second row shows \\(SS(\\text{bill\\_depth\\_mm} | \\text{species})\\), meaning bill depth contributes 818.505 additional unique explained variation in the response variable that species has not already explained. The third row shows \\(SS(\\text{flipper\\_length\\_mm} | \\text{species}, \\text{bill\\_depth\\_mm})\\), i.e., how much additional unique variation flipper length explains in a model that includes bill depth and species.\n\n\n\n\n\n\nNote\n\n\n\nMathematically, the sequential sum of squares is calculated as the difference in either SSE or SSR between two models—one without the added variable and one with it included. Suppose we want to add variable \\(X^*\\) to a model that already has \\(k\\) other variables, then the calculation is:\n\\[\n\\begin{aligned}\nSS(X^*|X_1, \\ldots, X_k) &= SSE_{X_1, \\ldots, X_k} - SSE_{X_1, \\ldots, X_k, X^*} = \\\\\n&= SSR_{X_1, \\ldots, X_k, X^*} - SSR_{X_1, \\ldots, X_k}\n\\end{aligned}\n\\tag{18.4}\\]\nNote that SSR increases with each additional variable added to the model, while SSE always decreases. Variation must always be positive, hence we calculate \\(SSE_{reduced} - SSE_{full}\\) or \\(SSR_{full} - SSR_{reduced}\\).\n\n\nSequential sums of squares are affected by the order in which variables are added to the model. Let’s change the order of the explanatory variables when estimating the model:\n\nmodel &lt;- lm(formula = bill_length_mm ~ sex + ., data = modelData)\n\nanova(model) |&gt; \n  round(4) |&gt; \n  kable() |&gt; \n  kable_styling(\"striped\")\n\n\n\nTable 18.4: Different order of model variables\n\n\n\n\n \n  \n     \n    Df \n    Sum Sq \n    Mean Sq \n    F value \n    Pr(&gt;F) \n  \n \n\n  \n    sex \n    1 \n    1175.4780 \n    1175.4780 \n    239.1017 \n    0.0000 \n  \n  \n    species \n    2 \n    6975.5916 \n    3487.7958 \n    709.4457 \n    0.0000 \n  \n  \n    bill_depth_mm \n    1 \n    64.4987 \n    64.4987 \n    13.1196 \n    0.0003 \n  \n  \n    flipper_length_mm \n    1 \n    78.3815 \n    78.3815 \n    15.9434 \n    0.0001 \n  \n  \n    body_mass_g \n    1 \n    32.2629 \n    32.2629 \n    6.5625 \n    0.0109 \n  \n  \n    Residuals \n    326 \n    1602.6899 \n    4.9162 \n     \n     \n  \n\n\n\n\n\n\n\nIn Table 18.4 we see that \\(SS(\\text{sex}) = 1175.478\\), which is significantly higher than \\(SS(\\text{sex}|\\text{species}, \\text{bill\\_depth\\_mm}, \\text{flipper\\_length\\_mm}, \\text{body\\_mass\\_g}) = 133.7191\\) from Table 18.3. The variable sex contributes a lot of variation when it is alone in a model, but when added to a model that already includes other variables, it does not contribute as much unique information. This means that the variation explained by the variable also appears to be present in the other variables. We will revisit this observation in a later chapter.\nOne thing that remains the same in both tables is SSE. We have included the same variables in both models, which means SSY, SSR, and SSE are overall the same. The sum of all sequential sums of squares should still equal SSR regardless of the order of variables, and due to the additive property of variation, SST and SSE have not changed either.\n\n\n18.1.7.2 Partial F-test for groups of parameters\nSometimes we are interested in examining parts of the model, e.g. a group of slope parameters, whether several variables together contribute explained variation to the model. Instead of examining all slope parameters, we now test a subset which is reflected in the hypothesis:\n\\[\\begin{align*}\nH_0&: \\beta_1 = \\beta_2 = \\beta_3 = \\cdots = \\beta_s = 0\\\\\nH_a&: \\text{At least one of } \\beta_j \\text{ in } H_0 \\text{ is different from } 0\n\\end{align*}\\]\nwhere \\(s\\) is the number of parameters being tested.\nThe test statistic for a partial F-test requires a full model (denoted \\(_F\\)) and a reduced model (denoted \\(_R\\)). The full model includes all variables, while the reduced model assumes \\(H_0\\) is true and excludes the variables being tested. We can use either SSR or SSE to calculate how much explained variation is lost between the two models, following the same principle as Equation 18.4.\n\\[\nF_{test} = \\frac{(SSR_F - SSR_R) / s}{SSE_F / (n - (k+1))} = \\frac{(SSE_R - SSE_F) / s}{SSE_F / (n - (k+1))}\n\\tag{18.5}\\]\nThe test statistic still follows an F-distribution with \\(s\\) and \\(n - (k+1)\\) degrees of freedom.\nUsing Equation 18.4, Equation 18.5 can be reformulated in a third way that simplifies our analysis process. We can rewrite the difference in explained variation between the full and reduced models as a sequential sum of squares. For example, we may want to investigate whether the variable species is associated with the response variable. Since that variable is transformed into two indicator variables, the hypotheses involve two slope parameters.\n\\[\\begin{align*}\nH_0&: \\beta_{Chinstrap} = \\beta_{Gentoo} = 0\\\\\nH_a&: \\text{At least one of } \\beta_j \\text{ in } H_0 \\text{ is different from } 0\n\\end{align*}\\]\nThe reduced model is created assuming \\(H_0\\) is true, i.e., \\(\\beta_{Chinstrap} = \\beta_{Gentoo} = 0\\), and the explained variation for the two models is denoted:\n\\[\n\\begin{aligned}\n  SSR_{R} &= SSR_{bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g, sex} \\\\\n  SSR_{F} &= SSR_{bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g, sex, species}\n\\end{aligned}\n\\]\nWe can rewrite the numerator in Equation 18.5 as:\n\\[\nSS(species|bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g, sex)\n\\]\nIn the ANOVA tables presented earlier, we can obtain this sum of squares directly if species is added as the last variable in the model.\n\nmodel &lt;- lm(bill_length_mm ~ bill_depth_mm + flipper_length_mm + body_mass_g + sex + species, data = modelData)\n\nanova(model) |&gt; \n  round(4) |&gt; \n  kable() |&gt; \n  kable_styling(\"striped\")\n\n\n\nTable 18.5: ANOVA table from a model where species is added last\n\n\n\n\n \n  \n     \n    Df \n    Sum Sq \n    Mean Sq \n    F value \n    Pr(&gt;F) \n  \n \n\n  \n    bill_depth_mm \n    1 \n    518.9806 \n    518.9806 \n    105.5648 \n    0.0000 \n  \n  \n    flipper_length_mm \n    1 \n    4045.7248 \n    4045.7248 \n    822.9329 \n    0.0000 \n  \n  \n    body_mass_g \n    1 \n    6.1329 \n    6.1329 \n    1.2475 \n    0.2649 \n  \n  \n    sex \n    1 \n    68.4245 \n    68.4245 \n    13.9181 \n    0.0002 \n  \n  \n    species \n    2 \n    3686.9500 \n    1843.4750 \n    374.9776 \n    0.0000 \n  \n  \n    Residuals \n    326 \n    1602.6899 \n    4.9162 \n     \n     \n  \n\n\n\n\n\n\n\nAn ANOVA table with sequential sums of squares calculates a partial F-test for each variable (and its parameter(s)), examining whether the variable contributes a significant increase in explained variation to a model that already includes the variables above. Table 18.5 now calculates the partial F-test for species (\\(F_{test} = 374.9776\\)), which we were interested in, and we can directly interpret the p-value for the test (\\(p-value &lt; 0.001\\)) as indicating that at least one of the slope parameters is significantly different from 0.\nIf we conduct a partial F-test for multiple variables, we cannot use the p-values shown in the table, since the hypotheses involve more slope parameters/variables than the sequential sums of squares represent. Suppose we want to investigate whether species and sex together contribute to the model. The hypothesis test would then involve:\n\\[\n\\begin{aligned}\nH_0&: \\beta_{sexMale} = \\beta_{Chinstrap} = \\beta_{Gentoo} = 0\\\\\nH_a&: \\text{At least one of } \\beta_j \\text{ in } H_0 \\text{ is different from } 0\n\\end{aligned}\n\\]\nThe sequential sum of squares we want to use is denoted \\(SS(species, sex|bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g)\\), and we can calculate this value by summing the SS for the two variables from Table 18.5:\n\\[\n\\begin{aligned}\nSS(species, sex|bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g) = \\\\\nSS(species|bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g, sex) + \\\\\nSS(sex|bill\\_depth\\_mm, flipper\\_length\\_mm, body\\_mass\\_g)\n\\end{aligned}\n\\]\nAlternatively, we can fit two models in R—the full and reduced and use anova() on both to get a summary output we can interpret directly.\n\nfullModel &lt;- simpleModel\nreducedModel &lt;- lm(bill_length_mm ~ bill_depth_mm + flipper_length_mm + body_mass_g, data = modelData)\n  \nanova(reducedModel, fullModel) |&gt; \n  round(4) |&gt; \n  kable() |&gt; \n  kable_styling(\"striped\")\n\n\n \n  \n    Res.Df \n    RSS \n    Df \n    Sum of Sq \n    F \n    Pr(&gt;F) \n  \n \n\n  \n    329 \n    5358.064 \n     \n     \n     \n     \n  \n  \n    326 \n    1602.690 \n    3 \n    3755.374 \n    254.6244 \n    0 \n  \n\n\n\n\nSince the p-value is less than 0.05, we can conclude that at least one parameter from sex and/or species are relevant to keep in the model, hence we cannot remove them.\n\n\n\n\n\n\nImportant\n\n\n\nIf a parameter is not considered significant, it is a justification for removing the variable. We fit a reduced model and begin a new analysis. If a variable is removed, the other parameter estimates will change, and interpretations and inference need to be updated.\n\n\n\n\n\n18.1.8 Model metrics\nBecause SSR always increases as more variables are added to a model, we need to adjust the metric to compare models of different sizes. We should instead look at the adjusted coefficient of determination (\\(R^2_{a}\\)) to see which model is best. An improved \\(R^2_{a}\\) means the model has removed unnecessary complexity.\n\\[\nR^2_a = 1 - \\frac{SSE / (n - (k+1))}{SST / (n - 1)}\n\\]\n\nsummary(simpleModel)\n\n\nCall:\nlm(formula = bill_length_mm ~ ., data = modelData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.3939 -1.3424 -0.0421  1.2695 11.4274 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       1.502e+01  4.374e+00   3.433 0.000674 ***\nspeciesChinstrap  9.566e+00  3.497e-01  27.351  &lt; 2e-16 ***\nspeciesGentoo     6.404e+00  1.030e+00   6.215 1.56e-09 ***\nbill_depth_mm     3.130e-01  1.541e-01   2.032 0.043000 *  \nflipper_length_mm 6.856e-02  2.315e-02   2.961 0.003293 ** \nbody_mass_g       1.084e-03  4.231e-04   2.562 0.010864 *  \nsexmale           2.030e+00  3.892e-01   5.215 3.27e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.217 on 326 degrees of freedom\nMultiple R-squared:  0.8386,    Adjusted R-squared:  0.8356 \nF-statistic: 282.3 on 6 and 326 DF,  p-value: &lt; 2.2e-16\n\n\nThis can be found in the second to last row in the summary output: Adjusted R-squared:  0.8356. This number cannot be interpreted by itself, but comparing it against the adjusted coefficient for the reduced model (\\(R^2_a = 0.4554\\)), we see that a lot of explained variation has been lost when removing the two categorical variables, adjusted for the model size — another indication that the two variables provide information to the model and should not be removed.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Advanced regression topics</span>"
    ]
  },
  {
    "objectID": "42_Multiple_and_Nonlinear_regression.html#sec-theilsen",
    "href": "42_Multiple_and_Nonlinear_regression.html#sec-theilsen",
    "title": "18  Advanced regression topics",
    "section": "18.2 A non-parametric method: Theil–Sen regression",
    "text": "18.2 A non-parametric method: Theil–Sen regression\nAs we have seen for statistical inference when comparing means or proportions in Chapter 12 and Chapter 14, there are times when the assumptions for a parametric test or model are not fulfilled. Instead of using a linear regression model that would provide unreliable results, a non-parametric alternative to this method is Theil–Sen regression. This is generally much more robust against outliers than the least-squares method. It also does not require that the residuals are normally distributed. There are also two disadvantages, the main one being that it can only be used for simple regression (one single predictor). It can also be slower to compute, but with today’s computers, this is rarely an issue.\nThe way Theil–Sen regression works is simple:\n\nA line is fit between all possible pairs of points, and their slopes are recorded.\nThe overall regression slope m is the median of all these pairwise slopes.\nThe intercept b is the median of all yi – m xi values, where xi is the ith measurement of the predictor and yi the corresponding response.\n\nTo use the Theil–Sen regression, one has to install the package mblm (“median-based linear models”):\n\ninstall.packages(\"mblm\")\n\nlibrary(mblm)\n\nThe function performing the regression is itself called mblm. A note of caution: its data argument, for some reason, is not called data but dataframe. Let us apply it to set 3 in the Anscombe dataset (the one with the single strong outlier):\n\nmblm(y ~ x, dataframe = filter(ansLong, set == \"3\")) |&gt; summary()\n\n\nCall:\nmblm(formula = y ~ x, dataframe = filter(ansLong, set == \"3\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.0045 -0.0022  0.0000  0.0025  4.2435 \n\nCoefficients:\n             Estimate       MAD V value Pr(&gt;|V|)   \n(Intercept) 4.0050000 0.0074130      65  0.00501 **\nx           0.3455000 0.0007413      66  0.00380 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.415 on 9 degrees of freedom\n\n\nAs seen, the predicted intercept and slope are no longer 3 and 0.5, but 4 and 0.35 instead. Also, in the regression table above, the median absolute deviation (MAD) of the parameters is reported instead of their standard error, as the MAD is a non-parametric measure of spread.1 The p-values (Pr(&gt;|V|)) in the table are like those in the regression tables from applying summary to lm—however, since Theil–Sen regression is a non-parametric method, these p-values are based on a Wilcoxon rank sum test instead of a t-test.\nWe can visualize the Theil–Sen regression alongside the least-squares regression, for a better comparison of what they do:\n\nleastSquaresFit &lt;- lm(y ~ x, data = filter(ansLong, set == \"3\"))\nTheilSenFit &lt;- mblm(y ~ x, dataframe = filter(ansLong, set == \"3\"))\n\nansLong |&gt;\n  filter(set == \"3\") |&gt;\n  mutate(`least squares` = predict(leastSquaresFit),\n         `Theil-Sen` = predict(TheilSenFit)) |&gt;\n  pivot_longer(cols = c(\"least squares\", \"Theil-Sen\"),\n               names_to = \"type\", values_to = \"prediction\") |&gt;\n  ggplot() +\n  geom_point(aes(x = x, y = y), color = \"steelblue\") +\n  geom_line(aes(x = x, y = prediction), color = \"goldenrod\") +\n  facet_grid(. ~ type) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe Theil–Sen regression correctly recognizes the outlier for what it is, and remains unaffected by it.\n\n\n\n\n\n\nNote\n\n\n\nIn the code above, we relied on a function called predict. This simply returns the model-predicted results for each value of the predictor:\n\npredict(leastSquaresFit)\n\n       1        2        3        4        5        6        7        8 \n7.999727 7.000273 9.498909 7.500000 8.499455 9.998636 6.000818 5.001364 \n       9       10       11 \n8.999182 6.500545 5.501091 \n\npredict(TheilSenFit)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n7.4600 6.7690 8.4965 7.1145 7.8055 8.8420 6.0780 5.3870 8.1510 6.4235 5.7325",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Advanced regression topics</span>"
    ]
  },
  {
    "objectID": "42_Multiple_and_Nonlinear_regression.html#sec-nonlin-regression",
    "href": "42_Multiple_and_Nonlinear_regression.html#sec-nonlin-regression",
    "title": "18  Advanced regression topics",
    "section": "18.3 Nonlinear regression",
    "text": "18.3 Nonlinear regression\nLet us look at another more complex type of model, one where the assumption of linearity does not hold. The built-in CO2 data frame contains measurements from an experiment on the cold tolerance of the grass species Echinochloa crus-galli. The dataset has five columns:\n\nPlant: unique identifier for each plant individual.\nType: either Quebec or Mississippi, depending on the origin of the plant.\nTreatment: whether the plant individual was chilled or nonchilled for the experiment.\nconc: carbon dioxide concentration in the surrounding environment.\nuptake: carbon dioxide uptake rate.\n\nDuring the exploratory stage of the analysis we discover this relationship between the variables, concentration and uptake separated by each treatment and plant origin:\n\nCO2 |&gt; \n  ggplot() + aes(x = conc, y = uptake) +\n  geom_point(color = \"steelblue\") + \n  facet_grid(Type ~ Treatment) +\n  theme_bw() + \n  labs(x = \"Concentration\", y = \"Uptake Rate\")\n\n\n\n\n\n\n\nFigure 18.11: Example of non-linear relationships\n\n\n\n\n\nThe relationship between CO2 concentration and uptake rates are definitely not linear, regardless of treatment or type. So the question arises: can one fit a non-linear function to these data? As an example, let us focus on just Quebec and the non-chilled treatment, to better illustrate the ideas behind nonlinear regression. Here are the filtered data:\n\nas_tibble(CO2) |&gt;\n  filter(Type == \"Quebec\", Treatment == \"nonchilled\") |&gt;\n  ggplot(aes(x = conc, y = uptake)) +\n  geom_point(color = \"steelblue\", alpha = 0.8) +\n  labs(x = \"Concentration\", y = \"Uptake Rate\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 18.12: Relationship between concentration and uptake rate of CO2 in Quebec using the non-chilled treatment\n\n\n\n\n\nIf the function we wish to fit is not linear, we have to specify its shape. One commonly used shape for describing the above saturating pattern is the Michaelis–Menten curve. This is given by the following equation: \\[ \\rho = \\frac{V c}{K + c} \\] Here \\(\\rho\\) is the uptake rate, \\(c\\) is the concentration, and \\(V\\) and \\(K\\) are two parameters which can modify the shape of the function. The figure below illustrates what curves one can get by varying these parameters:\n\nexpand_grid(V = c(10, 20, 30), # This function creates a tibble with all\n            K = c(1, 5, 10),   # possible combinations of the inputs\n            concentration = seq(0, 60, l = 201)) |&gt;\n  group_by(V, K) |&gt;\n  mutate(uptake = V * concentration / (K + concentration)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = concentration, y = uptake)) +\n  geom_line(color = \"steelblue\") +\n  facet_grid(V ~ K, labeller = label_both) +\n  labs(x = \"Concentration\", y = \"Uptake Rate\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe task is to find the values of \\(V\\) and \\(K\\) that provide the best fit to the data. Like in the case of linear regression, this can be done via the least-squares criterion: the best fit is provided by the curve which minimizes the sum of the squared deviations of the observed points from it. Unlike with linear regression however, this curve can be very difficult to find. In fact, there is no known general procedure that would be able to minimize the sum of squares under all circumstances. What algorithms can do is to find the best fit, given some initial guesses for the parameters that are at least not violently off of the true values. Just how close the guess needs to be is context-dependent, and highlights an important problem: non-linear regression can be as much an art as it is a science. For the types of curves we will be fitting though, the more subtle problems will never come up, and a “good enough” initial guess can vary within a relatively wide range.\nSo, how can one guess the values of \\(V\\) and \\(K\\)? To do this, one has to have an understanding of how the parameters influence the curves. For \\(V\\), this interpretation is not difficult to infer. Notice that if concentrations are very, very large, then in the denominator of the formula \\(\\rho = V c / (K + c)\\), we might as well say that \\(K + c\\) is approximately equal to \\(c\\) (if \\(c\\) is a million and \\(K\\) is one, then one is justified in treating the sum as being about one million still). This means that for large \\(c\\), the formula reduces to \\(\\rho \\approx V c / c = V\\). In other words \\(V\\) is the saturation uptake rate: the maximum value of the uptake. This, incidentally, is clearly visible in the plots above: when \\(V\\) is 10 (top row), the curves always tend towards 10 for large concentrations; when \\(V\\) is 20, they tend towards 20 (middle row), and when \\(V\\) is 30, they tend towards 30.\nThe interpretation of \\(K\\) is slightly less straightforward, but still simple. To see what it means, let us ask what the uptake rate would be, were the concentration’s value equal to \\(K\\). In that case, we get \\(\\rho = V K / (K + K)\\) (we simply substituted \\(c = K\\) into the formula), or \\(\\rho = VK / (2K) = V/2\\). That is, \\(K\\) is the concentration at which the uptake rate reaches half its maximum.\nLooking at the data again, both these parameters can be roughly estimated:\n\nas_tibble(CO2) |&gt;\n  filter(Type == \"Quebec\", Treatment == \"nonchilled\") |&gt;\n  ggplot(aes(x = conc, y = uptake)) +\n  geom_point(color = \"steelblue\", alpha = 0.8) +\n  geom_hline(\n    yintercept = 43, \n    linetype = \"dashed\", \n    color = \"steelblue\"\n  ) +\n  annotate(\n    geom = \"segment\", \n    x = 0, y = 43/2, \n    xend = 125, yend = 43/2,\n    linetype = \"dashed\", \n    color = \"steelblue\"\n  ) +\n  annotate(\n    geom = \"segment\", \n    x = 125, y = 43/2, \n    xend = 125, yend = 0,\n    linetype = \"dashed\", \n    color = \"steelblue\"\n  ) +\n  scale_x_continuous(limits = c(0, NA), expand = c(0, 0)) +\n  scale_y_continuous(limits = c(0, NA), expand = c(0, 0)) +\n  labs(x = \"Concentration\", y = \"Uptake Rate\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nSo guessing that \\(V\\) is about 43 and \\(K\\) is about 125 seems to be close to the mark.\nTo actually perform the nonlinear regression, one can use the nls function (“Nonlinear Least Squares”). It begins much like lm, taking a formula and a data frame. However, the formula is no longer a shorthand notation for a linear model, and therefore has to be entered literally. Additionally, there is another argument to nls called start; this is where the starting values have to be specified. The start argument has to be in the form of a list. Lists are an important data structure, worth a little interlude to explain how they work.\n\n18.3.1 Interlude: lists\nLists are like vectors except they can hold arbitrary data in their entries. So unlike vectors which are composed of either all numbers or all character strings or all logical values, lists may have a combination of these. Furthermore, list entries are not restricted to elementary types: vectors, or even data frames may also be list entries. To define a list, all one needs to do is type list, and then give a sequence of named entries. For example, the following creates a list with three entries: a will be equal to 3, b to the string \"Hello!\", and myTable to a small tibble.\n\nlist(a = 3, b = \"Hello!\", myTable = tibble(x = c(1, 2), y = c(3, 4)))\n\n$a\n[1] 3\n\n$b\n[1] \"Hello!\"\n\n$myTable\n# A tibble: 2 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     3\n2     2     4\n\n\nOne can refer to the entries of the list either with the $ notation, or using double brackets. Assigning the above list to a variable called myList first:\n\nmyList &lt;- list(\n  a = 3,\n  b = \"Hello!\",\n  myTable = tibble(x = c(1, 2), y = c(3, 4))\n)\n\nWe can now access the entries of myList either as\n\nmyList$myTable # Access the entry called myTable in the list\n\n# A tibble: 2 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     3\n2     2     4\n\n\nOr as\n\nmyList[[3]] # Access the third entry (myTable) in the list\n\n# A tibble: 2 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     3\n2     2     4\n\n\nThe $ notation is the same as when one accesses columns of data frames. This is not a coincidence: internally, data frames are represented as lists of columns, with each column being a vector. It follows that the double-bracket notation can also be used in conjunction with data frames: to access the first column of CO2, we can not only write CO2$Plant, but also CO2[[1]].\nA list seems to be just a more flexible version of a vector—so why would we want to use vectors instead of lists in the first place? The answer has to do with efficiency: the price to pay for the increased flexibility offered by lists is that they are slower to do operations on. While this would not be a problem for the applications found in this book, it can become important when dealing with large datasets or heavy numerical computations. As a corollary, R has many useful functions which work on vectors but do not work by default on lists. To mention just the simplest examples: mean, median, sd, and sum will throw an error when applied to lists. That is,\n\nsum(c(1, 2, 3))\n\n[1] 6\n\n\nreturns the expected 6, because it was applied to a vector. But the same expression results in an error when the vector is replaced by a list:\n\nsum(list(1, 2, 3))\n\nError in sum(list(1, 2, 3)): invalid 'type' (list) of argument\n\n\n\n\n18.3.2 Back to nonlinear regression\nWith this brief introduction to lists, we now understand what it means that the start argument to nls must be a list, with the entries named after the parameters to be fitted. Using the previously-guessed values of \\(V\\) and \\(K\\) being around 43 and 125, respectively, means we can use start = list(V = 43, K = 125). We can now look at the nls function and use it to produce a fit of the Michaelis–Menten curve to our data:\n\nnonlinearFit &lt;- as_tibble(CO2) |&gt;\n  filter(Type == \"Quebec\", Treatment == \"nonchilled\") |&gt;\n  nls(uptake ~ V*conc/(K + conc), data = _, start = list(V = 43, K = 125))\n\nprint(nonlinearFit)\n\nNonlinear regression model\n  model: uptake ~ V * conc/(K + conc)\n   data: filter(as_tibble(CO2), Type == \"Quebec\", Treatment == \"nonchilled\")\n     V      K \n 51.36 136.32 \n residual sum-of-squares: 319.2\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 9.722e-06\n\n\nObserve that in the formula, we use the column name conc when we want to use the data inside that column, but use made-up names (in this case, V and K) for the unknown parameters we are trying to obtain. Their starting values were filled in from our earlier visual estimation. From these starting values, the best fitting solution is found. We see that their values are 51.36 for \\(V\\) and 136.32 for \\(K\\).\nThe result of nls can be used inside summary to get more information on the fitted parameters (note: the anova function is not applicable to nonlinear regression). Doing so results in the following regression table:\n\nsummary(nonlinearFit)\n\n\nFormula: uptake ~ V * conc/(K + conc)\n\nParameters:\n  Estimate Std. Error t value Pr(&gt;|t|)    \nV   51.359      2.831  18.140 1.86e-13 ***\nK  136.319     27.027   5.044 7.21e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.099 on 19 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 9.722e-06\n\n\nAs in the case of linear regression, the statistical analysis will only be reliable if the assumptions of the independence, normality, and homoscedasticity of the residuals are maintained.\nTo conclude this section, it can be useful to plot the data together with the fitted nonlinear curve, to make sure visually that the fit is reasonable. There are several possible ways of doing this; here we discuss two of them. First, one can rely on the predict function (Section 18.2) that will tell us, for each value of the predictor, what the model-predicted values are. In the same way as for models generated by lm or mblm,\n\npredict(nonlinearFit)\n\n [1] 21.09265 28.87030 33.23631 36.96287 40.35655 42.72982 45.19795 21.09265\n [9] 28.87030 33.23631 36.96287 40.35655 42.72982 45.19795 21.09265 28.87030\n[17] 33.23631 36.96287 40.35655 42.72982 45.19795\n\n\nreturns a vector for each conc in the original data, documenting what the model thinks the corresponding uptake rate ought to be. They can then be compared with the data:\n\nas_tibble(CO2) |&gt;\n  filter(Type == \"Quebec\", Treatment == \"nonchilled\") |&gt;\n  mutate(pred = predict(nonlinearFit)) |&gt;\n  ggplot() +\n  geom_point(aes(x = conc, y = uptake), color = \"steelblue\", alpha = 0.8) +\n  geom_line(aes(x = conc, y = pred), linetype = \"dashed\", alpha = 0.5) +\n  labs(x = \"Concentration\", y = \"Uptake Rate\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIn the above plot, the data points and the predictions each had their own set of aeshetics. For this reason, the aesthetic mappings were not defined separately, but locally inside each geom_. This is perfectly legal, and can help whenever different geometries take different aesthetics from the data. Second, notice that the prediction was drawn with a dashed, semi-transparent line. This is intentional, to make it distinct from data. It signals that the curve does not correspond to data we are plotting, but to a model’s predictions.\nThe second method can be useful if the data are sufficiently scarce that the fitted line looks “rugged”, a bit too piecewise (this can be seen in the above example as well if one looks carefully). In that case, it is possible to draw the curve of any function using geom_function. We can use the fitted parameters in drawing it, and it will not suffer from being too piecewise:\n\nV_fitted &lt;- coef(nonlinearFit)[\"V\"] # Get fitted values of V and K\nK_fitted &lt;- coef(nonlinearFit)[\"K\"] # from the vector of coefficients\n\nas_tibble(CO2) |&gt;\n  filter(Type == \"Quebec\", Treatment == \"nonchilled\") |&gt;\n  ggplot() +\n  geom_point(aes(x = conc, y = uptake), color = \"steelblue\", alpha = 0.8) +\n  geom_function(fun = function(x) V_fitted * x / (K_fitted + x),\n                linetype = \"dashed\", alpha = 0.5) +\n  labs(x = \"Concentration\", y = \"Uptake Rate\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe result is much the same as before, although carefully looking at the dashed line shows that the second curve is smoother than the first. In this case, this does not matter much, but it could be aesthetically more relevant in others.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Advanced regression topics</span>"
    ]
  },
  {
    "objectID": "42_Multiple_and_Nonlinear_regression.html#exercises",
    "href": "42_Multiple_and_Nonlinear_regression.html#exercises",
    "title": "18  Advanced regression topics",
    "section": "18.4 Exercises",
    "text": "18.4 Exercises\nThe following exercises focus on all three of the advanced topics of regression.\n\n18.4.1 Multiple linear regression\nUse the iris dataset from Section 5.3 and complete a regression analysis outlined below focusing on a model describing petal_length.\n\nInvestigate the type and scale of the variables.\nSummarize descriptive statistics for each variable.\nVisualize the distribution of each variable.\nCreate a scatter plot showing the relationship between the two variables in the sample. Interpret it based on the four key pieces of information a scatter plot can provide.\nSummarize your observations and justify whether or not you think a simple linear regression model is suitable.\nFit a linear regression model using lm() and interpret the regression coefficients.\nCreate diagnostic plots to check whether the assumptions of the model are satisfied.\nPerform an F-test for the model and interpret the result. Do the same for a t-test for the slope parameter.\nSummarize your findings along with the coefficient of determination and determine whether or not the model is suitable for predicting plant growth based on soil moisture content.\n\n\n\n18.4.2 Non-parametric regression\nUse the plant growth rate data set from Section 17.7 and estimate a Theil-Sen regression. How does the relationship differ between the two models?\n\n\n18.4.3 Non-linear regression\n\n18.4.3.1 Exponential growth\nLet \\(N(t)\\) be the population abundance of some organism at time \\(t\\). An exponentially growing population increases according to \\[ N(t) = N_0 \\mathrm{e}^{rt} \\] where \\(N_0\\) is the initial population size at time \\(t=0\\), and \\(r\\) is the exponential rate of increase.\n\nDownload the data file pop_growth_1.csv and load it in R.\nUse nls() to fit the above exponential growth model to this dataset. Do not treat \\(N_0\\) as a free parameter, but instead use the actual population size at time \\(t=0\\). This leaves \\(r\\) as the only parameter to be fitted. Do so, using an appropriate starting value.\nAssume that the data describes a population of water lilies, and that a single ‘individual’ weighs 1 gram. If the population would continue to grow unrestricted, what would be its biomass after nine months (about 280 days)? What object would have a comparable weight to this population, and what does that tell us about unrestricted population growth in general?\n\n\n\n18.4.3.2 Nitrogen uptake\nCedergreen and Madsen (2002) reported data from an experiment on nitrogen uptake by the duckweed Lemna minor, where the predictor variable is the initial substrate concentration and the response variable is the uptake rate. In this type of experiment, it is anticipated that the uptake will increase as the concentration increases, approaching a horizontal asymptote. The data are available in uptake.csv.\n\nCreate a plot of the data, with the nitrogen concentrations along the x-axis and the corresponding uptake rates along the y-axis.\nFit a Michaelis-Menten model (describing saturating dynamics) to the data. This model is given by \\[ \\rho = \\frac{V c}{K + c} \\] where \\(V\\) and \\(K\\) are two constants, \\(c\\) is the concentration, and \\(\\rho\\) the uptake rate. Make initial guesses for the two parameters \\(V\\) and \\(K\\) based on the graph, and perform the nonlinear regression.\nGiven your nonlinear regression results, what is the maximum possible nitrogen uptake rate of L. minor?\n\n\n\n18.4.3.3 Logistic growth\nThe simplest model illustrating population regulation and regulated growth is the logistic model, defined by the differential equation \\[ \\frac{\\mathrm{d} N(t)}{\\mathrm{d} t} = rN(t) \\left( 1 - \\frac{N(t)}{K} \\right) \\] Here \\(N(t)\\) is the population abundance at time \\(t\\), \\(r\\) is the exponential growth rate of the population when rare, and \\(K\\) is the maximum abundance it can sustainably achieve (the “carrying capacity”). It should be obvious that when \\(N(t) = K\\), the derivative vanishes, signalling that the population size no longer changes.\nThe above differential equation is one of the few which can be solved explicitly. Its solution reads \\[ N(t) = N_0 \\frac{\\mathrm{e}^{rt}}{1-(1-\\mathrm{e}^{rt})(N_0/K)} \\] where \\(N_0\\) is the initial population size at time \\(t=0\\). Let us fit this model to some population growth data.\n\nDownload the data file pop_growth_2.csv, load it in R, and plot the population abundances against time.\nFit the above model to the data using the nls() function, with appropriate guesses for the starting values of \\(r\\) and \\(K\\).\nPlot the data and the model prediction together. What are the estimated values of \\(r\\) and \\(K\\)?\n\n\n\n\n\nCedergreen, Nina, and Tom Vindbæk Madsen. 2002. “Nitrogen uptake by the floating macrophyte Lemna minor.” New Phytologist 155 (2): 285–92. https://doi.org/10.1046/j.1469-8137.2002.00463.x.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Advanced regression topics</span>"
    ]
  },
  {
    "objectID": "42_Multiple_and_Nonlinear_regression.html#footnotes",
    "href": "42_Multiple_and_Nonlinear_regression.html#footnotes",
    "title": "18  Advanced regression topics",
    "section": "",
    "text": "The median absolute deviation (MAD) over a set of data points \\(x_i\\) is defined as \\(\\text{MAD} = \\text{median}(|x_i - \\text{median}(x)|)\\), where \\(\\text{median}(x)\\) is the median of the data.↩︎",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Advanced regression topics</span>"
    ]
  },
  {
    "objectID": "43_Anova.html",
    "href": "43_Anova.html",
    "title": "19  Analysis of Variance",
    "section": "",
    "text": "19.1 One-way ANOVA\nIn the previous chapters we studied the relationship between a continuous response variable and one or more explanatory variables. Within multiple linear regression we allow the explanatory variables to be a mix of continuous and qualitative variables and can fit the regression model using indicator variables.\nIf a regression model solely contain qualitative explanatory variables, it is equivalent to compare means of different groups of those variables. This can be simplified into an Analysis of Variance, or ANOVA-model, which is often used in experimental studies. Within this family of models, we now call the explanatory variables factors and their different values levels.\nSomething to keep in mind when reading this chapter is that an ANOVA-model is just a simplified version of a regression model.\nAn ANOVA-model containing only one factor is called a one-way ANOVA. We can formulate the model mathematically similar to that of a regression model: \\[\n  \\begin{aligned}\n    Y_{ij} = \\beta_0 + \\beta_1 \\cdot X_{1j} + \\dots + \\beta_{A-1}\\cdot X_{(A-1)j} + \\varepsilon_{ij}\n  \\end{aligned}\n\\tag{19.1}\\]\nwhere\nThis model will in practice predict the same value for all observations of the same level because an indicator variable is the same value for all observations therein. We can simplify the predicted values as:\n\\[\n  \\begin{aligned}\n    Y_{i = 1} = \\mu_1 &= \\beta_0 + \\beta_1 \\cdot 1 + \\beta_2 \\cdot 0 + \\dots + \\beta_{A-1} \\cdot 0 \\\\\n    &= \\beta_0 + \\beta_1\\\\\n    Y_{i = 2} = \\mu_2 &= \\beta_0 + \\beta_1 \\cdot 0 + \\beta_2 \\cdot 1 + \\dots + \\beta_{A-1} \\cdot 0 \\\\\n    &= \\beta_0 + \\beta_2\\\\\n    &\\quad \\vdots \\\\\n    Y_{i = A-1} = \\mu_{A-1} &= \\beta_0 + \\beta_1 \\cdot 0 + \\beta_2 \\cdot 0 + \\dots + \\beta_{A-1} \\cdot 1 \\\\\n    &= \\beta_0 + \\beta_{A-1}\\\\\n    Y_{i = A} = \\mu_A &= \\beta_0 + \\beta_1 \\cdot 0 + \\beta_2 \\cdot 0 + \\dots + \\beta_{A-1} \\cdot 0 \\\\\n    &= \\beta_0\n  \\end{aligned}\n\\]\nAs mentioned multiple times but is worth repeating, an ANOVA-model is a special case of a regression model which means that the model assumptions \\(\\varepsilon_{ij} \\overset{\\mathrm{iid}}{\\sim} N(0, \\sigma^2)\\) still apply and that the model should be evaluated using residual analysis (see Section 17.3) before moving forward to statistical inference. One of the main differences with an ANOVA-model is that the residuals will be grouped to the different levels of the factor and that it can be a bit tough to assess if the homoscedasticity (equal variance) assumption hold, we will see an example of this a bit later in this text.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "43_Anova.html#one-way-anova",
    "href": "43_Anova.html#one-way-anova",
    "title": "19  Analysis of Variance",
    "section": "",
    "text": "\\(j\\) is the observation index for the \\(j\\):th observation in level \\(i\\) of the factor, ranging from 1 to \\(n_i\\), the number of observations in total within that level\n\\(A\\) is the number of levels within the factor,\nall \\(X_{ij}\\) are indicator coded variables that each indicate level \\(i\\) of the factor.\n\n\n\n\n\n\n\nNote\n\n\n\nThe difference in an ANOVA-model compared to a standard regression model is the fact we now have two indexes, \\(i\\) and \\(j\\). We will encounter more ANOVA-models with more factors that increase this even further, but one thing to keep in mind is that the last index is considered the observation index within a specific combination of factor levels.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nSome literature may use another variant of the model, called the cell means model.\n\\[\n  \\begin{aligned}\n    Y_{ij} = \\mu_i + \\varepsilon_{ij}\n  \\end{aligned}\n\\] where \\(j\\) is the observation index within each level, \\(\\mu_i\\) is the mean for level \\(i\\) of the factor. It can be shown using the above simplification that this is just another way to write the regression model.\n\n\n\n\n19.1.1 Alternative to indicator coding\nThere exist many other ways to code our variables that simplifies the mathematical calculations when fitting a model. One such way is using a factor effect coding system that has two main benefits; firstly it provides accurate parameter estimates in more complex ANOVA-models, secondly it changes the way we interpret an estimate for a specific level to something more consistent.\nIndicator coded variables are interpreted as “the effect of this level compared to the chosen reference level” and the estimated parameters will numerically change based on the, most often arbitrarily, chosen reference level.\nFactor effect coding instead provides the same numerical parameter estimates (for the same factor level) regardless of the level that is “skipped”. These effects are interpreted as “the effect of this level compared to the overall mean”.\nWe can define this new model in its regression form as: \\[\n  \\begin{aligned}\n    Y_{ij} = \\mu + \\beta_1 \\cdot X_{1j} + \\dots + \\beta_{A-1}\\cdot X_{(A-1)j} + \\varepsilon_{ij}\n  \\end{aligned}\n\\tag{19.2}\\]\nwhich is very similar to Equation 19.1 except that the intercept is now written as \\(\\mu\\). Mathematically the differences are:\n\n\\(\\mu\\) is the unweighted1 overall mean of \\(Y_{ij}\\) calculated as: \\[\n\\begin{aligned}\n\\mu &= \\frac{\\mu_1 + \\mu_2 + \\cdots + \\mu_A}{A}\n\\end{aligned}\n\\]\n\\(\\beta_i\\) is the difference or effect of level \\(i\\) on the overall mean calculated as:\n\\[\n\\begin{aligned}\n  \\beta_i = \\mu_i - \\mu\n\\end{aligned}\n\\]\n\\(X_{ij}\\) is factor effect coded as: \\[\n\\begin{aligned}\n  X_{ij} =\n  \\begin{cases}\n      1 \\quad &\\text{if level i}\\\\\n      -1      &\\text{if level A}\\\\\n      0       &\\text{otherwise}\n  \\end{cases}\n\\end{aligned}\n\\] With the help of this coding we can show that the effect of level \\(A\\) can be calculated as the negative sum of the other parameters, even though that effect is not specifically defined in the model.\n\n\\[\n\\begin{aligned}\n  \\beta_A = -(\\beta_1 + \\beta_2 + \\cdots + \\beta_{A-1}) = \\mu_A - \\mu\n\\end{aligned}\n\\]\n\n\n\n\n\n\nImportant 19.1\n\n\n\nSome literature starts at this time to change the labels of the model parameters, for example the factor effect for level \\(i\\) is defined as \\(\\alpha_i\\) and no reference to the coded variables are given: \\[\n  \\begin{aligned}\n    Y_{ij} = \\mu + \\sum_{i = 1}^{A-1}\\alpha_i + \\varepsilon_{ij}\n  \\end{aligned}\n\\]\nThroughout this book we will keep on using the regression formulation and define each instance of \\(X_{ij}\\) to clearly separate the indicator and factor effect coded models from one another.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "43_Anova.html#estimating-an-anova-model",
    "href": "43_Anova.html#estimating-an-anova-model",
    "title": "19  Analysis of Variance",
    "section": "19.2 Estimating an ANOVA-model",
    "text": "19.2 Estimating an ANOVA-model\nThe tools of Chapter 14 allow us to compare two groups of data. But what do we do when we have more than two groups? As an example, we might wish to know how different treatments influence plant growth, as measured by dry weight. If there are two treatments, then there will be three (instead of two) groups, because the treatments will be compared with a control group which does not receive treatment.\nSuch a dataset is in fact built into R, and is called PlantGrowth:\n\nlibrary(tidyverse)\n\nPlantGrowth |&gt; \n  as_tibble() |&gt; \n  print(n = Inf)\n\n# A tibble: 30 × 2\n   weight group\n    &lt;dbl&gt; &lt;fct&gt;\n 1   4.17 ctrl \n 2   5.58 ctrl \n 3   5.18 ctrl \n 4   6.11 ctrl \n 5   4.5  ctrl \n 6   4.61 ctrl \n 7   5.17 ctrl \n 8   4.53 ctrl \n 9   5.33 ctrl \n10   5.14 ctrl \n11   4.81 trt1 \n12   4.17 trt1 \n13   4.41 trt1 \n14   3.59 trt1 \n15   5.87 trt1 \n16   3.83 trt1 \n17   6.03 trt1 \n18   4.89 trt1 \n19   4.32 trt1 \n20   4.69 trt1 \n21   6.31 trt2 \n22   5.12 trt2 \n23   5.54 trt2 \n24   5.5  trt2 \n25   5.37 trt2 \n26   5.29 trt2 \n27   4.92 trt2 \n28   6.15 trt2 \n29   5.8  trt2 \n30   5.26 trt2 \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe PlantGrowth data are in the form of a data frame instead of a tibble; hence we convert it above by using as_tibble.\n\n\nWe see that each group consists of ten observations, and we have three groups: the control (ctrl) treatment 1 (trt1), and treatment 2 (trt2). As usual, we visualize the data first, before doing any modeling or tests:\n\nPlantGrowth |&gt;\n  ggplot(aes(x = group, y = weight)) +\n  geom_boxplot(color = \"steelblue\", fill = \"steelblue\",\n               alpha = 0.2, outlier.shape = NA) +\n  geom_jitter(alpha = 0.4, width = 0.05, color = \"steelblue\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThere are two questions one can ask, and they naturally build on each other.\nFirst, we might wonder if the observed differences between the distribution of the data in the different groups is meaningful. Could it be that all data points are actually drawn from the same underlying distribution, and the observed differences are simply due to chance? If so, what is the likelihood of that situation?\nAnd second, provided we can reject the idea that the data in the different groups are identically distributed, which groups are the ones that differ from one another? For example, based on the figure above, it appears reasonable to expect a real difference between treatment 1 and treatment 2, but it is unclear whether the control truly differs from any of the treatments. What we will in practice be calculating is the difference in means between all three groups, whether they is any at all, and if so, which ones differ from each other.\nAssuming we are using the factor effect coding, the model can be given as:\n\\[\n\\begin{aligned}\n    Y_{ij} = \\mu + \\beta_1 \\cdot X_{1j} + \\beta_{2}\\cdot X_{2j} + \\varepsilon_{ij}\n\\end{aligned}\n\\tag{19.3}\\]\nwhere\n\\[\n\\begin{aligned}\n    \\mu &= \\frac{\\mu_1 + \\mu_2 + \\mu_3}{3} \\\\\n    X_{1j} &=\n    \\begin{cases}\n        1 \\quad &\\text{if control}\\\\\n        -1      &\\text{if treatment 2}\\\\\n        0       &\\text{otherwise}\n    \\end{cases} \\\\\n    X_{2j} &=\n    \\begin{cases}\n        1 \\quad &\\text{if treatment 1}\\\\\n        -1      &\\text{if treatment 2}\\\\\n        0       &\\text{otherwise}\n    \\end{cases}\n\\end{aligned}\n\\] and \\(\\varepsilon_{ij} \\sim N(0, \\sigma^2)\\).\n\n\n\n\n\n\nTip\n\n\n\nThe order of the coding in this example, that control is defined as \\(X_{1j}\\), comes purely from the fact that R treats factors in alphabetical order by default and automatically uses the last level as the -1 code.\n\n\nBefore moving on the with model estimation we need to make sure that R reads the variable containing our factor of interest as a factor type. This can be seen in the tibble output where group is seen as a fct, a factor variable. In order to create the factor effect coding we make use of the function contr.sum(A) where A is the number of levels of the factor.\n\n## Shows the factor effect coding matrix\ncontr.sum(3)\n\n  [,1] [,2]\n1    1    0\n2    0    1\n3   -1   -1\n\n## Shows the current/default coding matrix for the factor\ncontrasts(PlantGrowth$group)\n\n     trt1 trt2\nctrl    0    0\ntrt1    1    0\ntrt2    0    1\n\n## Saves the factor effect coding matrix into the factor\ncontrasts(PlantGrowth$group) &lt;- contr.sum(3)\n\n## Shows the new coding matrix\ncontrasts(PlantGrowth$group)\n\n     [,1] [,2]\nctrl    1    0\ntrt1    0    1\ntrt2   -1   -1\n\n\nThe default order in contr.sum is that the last level is considered the “missing” one, but we could reorder the factor levels (using factor(levels = new order) and/or relevel()) before adjusting the coding matrix to get any order we like.\nPerhaps unsurprisingly, the function in R that estimates an ANOVA model is the same lm that we have used in Chapter 17 for performing linear regression:\n\nmodel &lt;- lm(weight ~ group, data = PlantGrowth) \n\nmodel |&gt; \n  summary()\n\n\nCall:\nlm(formula = weight ~ group, data = PlantGrowth)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0710 -0.4180 -0.0060  0.2627  1.3690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.0730     0.1138  44.573   &lt;2e-16 ***\ngroup1       -0.0410     0.1610  -0.255   0.8009    \ngroup2       -0.4120     0.1610  -2.560   0.0164 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6234 on 27 degrees of freedom\nMultiple R-squared:  0.2641,    Adjusted R-squared:  0.2096 \nF-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n\n\nThe output takes some care to interpret correctly: out of the three coefficients listed, (Intercept) is the overall mean weight of all the plants; group1 is the difference of the mean weight in the control from the overall mean; and group2 is the difference of the mean weight in treatment 1 from the overall mean. Since both of these coefficients are negative, the effect of treatment 2 against the overall mean would be positive, \\(-[(-0.041) + (-0.4120)] = 0.453\\).\n\ndiagnosticPlots(model, alpha = 1, bins = 5)\n\n\n\n\nResidual analysis of the PlantGrowth model\n\n\n\n\nIn these diagnostic plots we see the grouped observations clearly in the top right figure. Since all observations pertaining to a specific factor level will have the same estimated value of \\(Y\\), the points are grouped horizontally.\nGiven the fact that there are only 30 observations in total, the interpretation of these plots is going to be approximate. Unless there is clear indications of violations we usually assume that the assumptions are fulfilled. In this specific case, there are no clear indications that we are violating the normality assumption however there is an indication that the variance of the residuals are reduced for higher values of the weight.\nAt this point in the book we do not have any tools at our disposal to handle this so we will (unfortunately) disregard this observation and assume the residuals are homoscedastic (constant variance).",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "43_Anova.html#statistical-inference-in-anova",
    "href": "43_Anova.html#statistical-inference-in-anova",
    "title": "19  Analysis of Variance",
    "section": "19.3 Statistical inference in ANOVA",
    "text": "19.3 Statistical inference in ANOVA\nRecall back to the two questions we asked ourselves in the exploratory phase; are the observed differences between the groups meaningful, and which groups differ from each other. Assuming that the residual analysis enforces that the model assumptions hold, these two questions can be answered by statistical inference.\nTo get more information, one can pass the result of lm to a function called anova. Despite its name, the role of this function is not to actually estimate the ANOVA model (that was done by lm), but to display its results using the sum-of-squares table, which is also known as the ANOVA table. In this table, each factor, as well as their interactions (if present), get one row each:\n\nmodel |&gt; \n  anova()\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ngroup      2  3.7663  1.8832  4.8461 0.01591 *\nResiduals 27 10.4921  0.3886                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n19.3.1 Overall F-test\nAn F-test can be used to determine whether a factor as a whole has a significant impact on the response variable, similar to that of the F-test in Section 17.4. Using the model Equation 19.2 we can write the hypotheses as:\n\\[\n\\begin{aligned}\n    H_0&: \\beta_1 = \\beta_2 = \\dots = \\beta_{A-1} = 0 \\\\\n    H_A&: \\text{At least one of } \\beta_i \\text{ in } H_0 \\ne 0\n\\end{aligned}\n\\]\nThe null hypothesis can be interpreted as no factor level mean differ from the overall mean if all model parameters are 0. In practice this means that the factor has no effect on the response variable. The alternative hypothesis indicate that at least one factor level mean differ from the overall mean, which in contrast results in the factor having an effect on the response variable.\n\n\n\n\n\n\nNote\n\n\n\nSince we can simplify the model to a mean for each factor level, the hypotheses are equivalent to: \\[\n\\begin{aligned}\n    H_0&: \\mu_1 = \\mu_2 = \\dots = \\mu_{A-1} = \\mu_A \\\\\n    H_A&: \\text{At least two of } \\mu_i \\text{ in } H_0 \\text{ differ}\n\\end{aligned}\n\\] This can be seen as an extension of the two sample t-test presented in Section 14.1.\n\n\nThis is where we are starting to see the reason for the name “Analysis of Variance”. Even though we are comparing means between groups, the complexity of actually comparing the means as we would in a two sample t-test would be all to complicated. We can instead make use of the sources of variation introduced in Section 17.4.1 that simplified for an ANOVA-model summarize the differences in means.\n\\[\n\\begin{aligned}\n  SSY &= \\sum_{i=1}^A\\sum_{j=1}^{n_i}{(Y_{ij} - \\overline{Y})^2} \\\\\n  SSA &= \\sum_{i=1}^A{n_i \\cdot (\\overline{Y}_i - \\overline{Y})^2} \\\\\n  SSE &= \\sum_{i=1}^A\\sum_{j=1}^{n_i}{(Y_{ij} - \\overline{Y}_i)^2}\n\\end{aligned}\n\\tag{19.4}\\] where\n\n\\(SSY\\) is the total variation of the response variable written as the difference between each observation \\(Y_{ij}\\) and the total mean \\(\\overline{Y}\\).\n\\(SSA\\) is the explained variation of each of the factor level means written as the difference between the level mean \\(\\overline{Y}_{i}\\) and the total mean \\(\\overline{Y}\\).\n\\(SSE\\) is the unexplained variation written as the difference between each observation \\(Y_{ij}\\) and the level mean \\(\\overline{Y}_{i}\\).\n\nIf \\(H_0\\) was true the difference in factor level mean and overall mean would be 0, so \\(SSA\\) would be 0. On the other hand if \\(H_A\\) was true we would see a non-zero sum in \\(SSA\\). In order to assess whether the difference in means is large enough to not attribute it to chance, we define the test variable as:\n\\[\n  \\begin{aligned}\n    F_{test} = \\frac{SSA / df_A}{SSE / df_E} = \\frac{MSA}{MSE}\n  \\end{aligned}\n\\] where \\(df_A\\) and \\(df_E\\) are the degrees of freedom of the factor and error respectively. In the ANOVA table output we can see the result of this calculation in the F value column and its corresponding p-value in the Pr(&gt;F) column.\n\n\n\n\n\n\nTip\n\n\n\nBelow is an interactive visualization using simulations of the relationship between the two sources of variation. The simulation assumes we have four groups with known population means2 we draw a sample from, visualize each group’s distribution and mean (black line) against the overall mean (red line), and finally calculate the sum of squares, the test variable and the p-value.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 1000\n\nrequire(shiny)\nrequire(bslib)\nrequire(ggplot2)\nrequire(tidyr)\nrequire(dplyr)\nrequire(stringr)\nrequire(munsell)\n\n# Define UI for app that draws a histogram ----\nui &lt;- page_sidebar(\n  sidebar = sidebar(open = \"open\",\n    sliderInput(\"group_1\", \"Population mean Group 1\", min = -1, max = 1, value = 0, step = 0.1),\n    sliderInput(\"group_2\", \"Population mean Group 2\", min = -1, max = 1, value = 0, step = 0.1),\n    sliderInput(\"group_3\", \"Population mean Group 3\", min = -1, max = 1, value = 0, step = 0.1),\n    sliderInput(\"group_4\", \"Population mean Group 4\", min = -1, max = 1, value = 0, step = 0.1),\n    numericInput(\"sample_size\", \"Sample size\", value = 30, min = 1, step = 1),\n    actionButton(\"new_sample\", \"Draw a new sample\"),\n    uiOutput(\"variances\"),\n  ),\n  plotOutput(\"plot\", height = \"600px\", width = \"60%\")\n)\n\nserver &lt;- function(input, output, session) {\n  data_input &lt;- reactive({\n    means &lt;- c(input$group_1, input$group_2, input$group_3, input$group_4)\n    seed &lt;- input$new_sample\n    set.seed(seed = seed)\n    samples &lt;- sapply(means, FUN = function(x) {\n        rnorm(n = input$sample_size, mean = x, sd = 1)\n    }) %&gt;% \n      as.data.frame()\n    data_samples &lt;- pivot_longer(samples, cols = everything()) %&gt;% \n      mutate(\n        name = str_replace(name, \"V\", \"Group \")\n      )\n  })\n  \n  output$plot &lt;- renderPlot({\n    data_samples &lt;- data_input()\n    sample_means &lt;- aggregate(value ~ name, data_samples, mean)\n\n    ggplot2::ggplot(data = data_samples) + aes(x = value) + \n    geom_histogram(binwidth = 0.25, color = \"black\", fill = \"steelblue\") +\n    geom_vline(data = sample_means, aes(xintercept = value), linetype = 2, linewidth = 1.2) +\n    geom_vline(aes(xintercept = mean(value)), color = \"#d9230f\", linewidth = 1) +\n            scale_x_continuous(breaks = seq(-20, 20, by = 1)) +\n            facet_grid(rows = vars(name)) + theme_bw() +\n            theme(strip.text.y = element_text(angle = 0, color = \"white\", size = 14),\n                  strip.background.y = element_rect(fill = \"black\"),\n                  axis.title.y = element_blank()) +\n            labs(x = \"Y\") \n  })\n\n  output$variances &lt;- renderUI({\n    data_samples &lt;- data_input()\n    sample_means &lt;- aggregate(value ~ name, data_samples, mean)\n    sample_sizes &lt;- aggregate(value ~ name, data_samples, length)\n\n    SSY &lt;- sum((data_samples$value - mean(data_samples$value))^2)\n\n    SSR &lt;- sum(sample_sizes$value * (sample_means$value - mean(data_samples$value))^2)\n    MSR &lt;- SSR/(nrow(sample_means) - 1)\n\n    SSE &lt;- SSY-SSR\n    MSE &lt;- SSE/(nrow(data_samples) - nrow(sample_means))\n    \n    pvalue &lt;- pf(\n      round(MSR / MSE, 3), \n      df1 = (nrow(sample_means) - 1), \n      df2 = (nrow(data_samples) - nrow(sample_means)),\n      lower.tail = FALSE\n    )\n\n    withMathJax(\n      paste(\"$$SSY = \", round(SSY, 3), \"\\\\\\\\\",\n            \"SSA = \", round(SSR, 3), \"\\\\\\\\\",\n            \"SSE = \", round(SSE, 3), \"\\\\\\\\\",\n            \"F_{test} = \", round(MSR / MSE, 3), \"\\\\\\\\\",\n            \"\\\\text{p-value} = \", pvalue %&gt;% round(3), \"$$\")\n    )\n  })\n}\n\n# Create Shiny app ----\nshinyApp(ui = ui, server = server)\n\nInitially the population means of the four groups are set at the same level and the sample drawn show factor level means very close to the overall mean and each other. If we adjust one (or more) of the population means to something else, we directly see the impact on the drawn sample. The factor level mean is now further away than the overall mean as well as the other level means. By this logic we can see that a change in at least one mean also changes the relation between the explained and unexplained variation.\n\n\nIn our model, the p-value of the overall F-test is \\(0.01591\\) which is lower than a significance level of 5%. This means that we can conclude that the observed differences are significant and we expect at least two of the three population means to differ from one another.\n\n\n19.3.2 Multiple comparisons\nIf \\(H_0\\) is rejected, we do not yet know which factor levels differ from each other just that at least two does. One could look at this problem as doing multiple pairwise t-tests between each pair of factor levels but this causes two main issues if we want to draw conclusions from all tests at once.\nFirst the method of using repeated t-tests works fine in our example, there are some problems with this approach. One is that it can quickly get out of hand, because having \\(n\\) groups means there will be \\(n (n - 1) / 2\\) pairs to consider. For instance, if the number of groups is 12 (not a particularly large number), then there are 66 unique pairs already. It would not be pleasant to perform this many tests, even if any single test is quite simple to run.\nThe second relates to the risk of making the wrong decision. Recall the Type I and II errors discussed in Section 12.2.1.3. Using a 5% level of significance, we have a 5% chance of our sample being one of the infinite amount of samples that rejects this specific \\(H_0\\) even though it is true. In practice this means that we risk having observed differences that occur purely by chance. Very simply put, the problem is that if sufficiently many groups are compared, then we might find at least one pair with a low p-value—not because the null hypothesis is false, but because across a large number of observations some p-values will turn out lower than others just by chance. The p-values measure, in effect, the probability that the observed differences are too stark to be due to simple coincidence. But if we create sufficiently many opportunities for such a coincidence to arise, then of course one eventually will. One of the best explanations of this point is in the following cartoon by xkcd:\n\nFortunately, there is a way of solving both the problem of automating many pairwise comparisons, as well as adjusting the p-values to account for multiple testing. The way forward is to perform a post hoc (Latin “after this”) test.\n\n19.3.2.1 Tukey family confidence\nThe Tukey(-Kramer) method is used to adjust the p-values of all pairwise comparisions of factor level means. We will make use of the package emmeans and its different functions to first create a table of the estimated means and their corresponding standard errors and then the pairwise comparisons with the Tukey post-hoc adjustment.\n\nrequire(emmeans)\n\n## Calculates the factor level means of the specified factor\nfactorMeans &lt;- emmeans(model, specs = ~ group)\n\n## Calculates the pairwise comparisons with a Tukey adjustment\ntukey &lt;- pairs(factorMeans, adjust = \"tukey\") \n\n## Presents the pairwise comparisons sorted by the estimated difference\ntukey |&gt; \n  as_tibble()  |&gt;  \n  arrange(\n    abs(estimate) |&gt;\n      desc()\n  )\n\n# A tibble: 3 × 6\n  contrast    estimate    SE    df t.ratio p.value\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 trt1 - trt2   -0.865 0.279    27   -3.10  0.0120\n2 ctrl - trt2   -0.494 0.279    27   -1.77  0.198 \n3 ctrl - trt1    0.371 0.279    27    1.33  0.391 \n\n\nThe table shows three contrasts, which is the three pairwise comparisons between each factor level and the adjusted p-values of each comparison. The hypotheses tested are:\n\\[\n  \\begin{aligned}\n  H_0&: \\mu_i - \\mu_{i'} = 0\\\\\n  H_A&: \\mu_i - \\mu_{i'} \\ne 0\\\\\n  \\end{aligned}\n\\] where \\(i \\ne i'\\).\nWhat has the post hoc Tukey test revealed? Precisely what we have been suspecting: that the only difference worth noting is the one between the two treatments (first row of the table, where the adjusted p-value is sufficiently small to have a chance of pointing at a real difference).\n\n\n19.3.2.2 Dunnett’s test against control\nConsidering we have a control group, there exist a specific test that looks at all pairwise comparisons of factor levels only against the control group. This is called a Dunnett’s test.\nChanging the method argument to dunnett and specifying which factor level contains the control group in ref we now focus on the two comparisons against the control.\n\ncontrast(factorMeans, method = \"dunnett\", ref = 1) |&gt; \n  as_tibble() |&gt; \n  arrange(\n    abs(estimate) |&gt; \n      desc()\n  )\n\n# A tibble: 2 × 6\n  contrast    estimate    SE    df t.ratio p.value\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 trt2 - ctrl    0.494 0.279    27    1.77   0.158\n2 trt1 - ctrl   -0.371 0.279    27   -1.33   0.330\n\n\nIt shows that neither treatment has significant differences against the control on a 5% significance level.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "43_Anova.html#the-kruskalwallis-and-dunn-tests",
    "href": "43_Anova.html#the-kruskalwallis-and-dunn-tests",
    "title": "19  Analysis of Variance",
    "section": "19.4 The Kruskal–Wallis and Dunn tests",
    "text": "19.4 The Kruskal–Wallis and Dunn tests\nWhat happens if the regression model assumptions are not fulfilled? Starting with the first of these questions, there exists an analogue to the Wilcoxon rank sum test which works when there are more than two groups of data. This is the Kruskal–Wallis test3 , which can be used with any number of groups as long as those groups vary within a single factor.4\nThe Kruskal–Wallis test is non-parametric, and therefore does not rely on assumptions such as the normality of the residuals. Its implementation in R, kruskal.test, is analogous to wilcox.test, t.test lm, or mblm: it takes a formula and the data as inputs. Therefore, to perform the test on the PlantGrowth data, we write:\n\nkruskal.test(weight ~ group, data = PlantGrowth)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  weight by group\nKruskal-Wallis chi-squared = 7.9882, df = 2, p-value = 0.01842\n\n\nThe null hypothesis of the Kruskal–Wallis test is that the observations from all groups were sampled from the same underlying distribution—that is, that there are no differences between the groups other than those attributed to random noise. Consequently, when the p-value is low (like above), this means that it is unlikely that the data in all groups come from the same distribution, and thus that at least one group differs from the others.\nThe p-value above, in the row belonging to the factor group, under the column Pr(&gt;F), is the analogue of the p-value calculated by the F-test (which was 0.01591). We can see that the two tests agree qualitatively.\nIn this case, the Dunn test is the non-parametric post hoc test we want to use to determine which group(s) differ. This test is implemented in R, but not in any of the basic packages. To use it, one must first install the FSA package:\n\ninstall.packages(\"FSA\")\n\nOnce it is installed, the package should be loaded:\n\nrequire(FSA)\n\nAnd then, the Dunn test (dunnTest) follows the familiar syntax of receiving a formula and the data:\n\ndunnTest(weight ~ group, data = PlantGrowth)\n\nDunn (1964) Kruskal-Wallis multiple comparison\n\n\n  p-values adjusted with the Holm method.\n\n\n   Comparison         Z    P.unadj      P.adj\n1 ctrl - trt1  1.117725 0.26368427 0.26368427\n2 ctrl - trt2 -1.689290 0.09116394 0.18232789\n3 trt1 - trt2 -2.807015 0.00500029 0.01500087\n\n\nThe table above is the output of the test, and has four columns. The first column shows which two groups are being compared. The next column, called Z, is the value of the test statistic, which we need not concern ourselves with here. Next, we have the unadjusted p-values; and finally, the adjusted p-values (P.adj), which have been corrected to account for the multiple testing problem mentioned above. Therefore, the adjusted p-values will always be as large or larger than the unadjusted ones.\nWhat has the post hoc Dunn test revealed? Precisely what we have been suspecting: that the only difference worth noting is the one between the two treatments (last row of the table, where the adjusted p-value is sufficiently small to have a chance of pointing at a real difference).\nIn this case, the results from the Dunn test and the Tukey test are in agreement: only the difference between the two treatment groups stands out as having a reasonable chance of being real.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "43_Anova.html#exercises",
    "href": "43_Anova.html#exercises",
    "title": "19  Analysis of Variance",
    "section": "19.5 Exercises",
    "text": "19.5 Exercises\nThe file daphnia_growth.csv contains data on the growth rate of Daphnia populations that are infected with various parasites. There are four groups of observations: the control (no parasites), infection with Metschnikowia bicuspidata, infection with Pansporella perplexa, and finally, infection with Pasteuria ramosa. Each group has ten replicate observations. Are growth rates affected by parasite load?\n\nBefore doing any tests, visualize and explore the data, and make sure you have a solid expectation/guide for the results of any statistical analysis.\nAnswer the question whether growth rates affected by parasite load by first applying a non-parametric test (and a subsequent non-parametric post-hoc test if needed).\nEstimate a parametric ANOVA model and create diagnostic plots to see if the assumptions behind the parametric test are satisfied to an acceptable degree.\nApply a parametric test in the same way as 2.: by applying the test and running post-hoc tests if needed. Do the results from the parametric and non-parametric tests agree with one another?\n\nIn ponds.csv, measured acidity data (pH) is reported from four different ponds. Do the ponds differ in acidity, and if so, which ones from which others? Answer using both non-parametric and parametric tests, with appropriate post-hoc analyses. Check whether these different methods of analysis agree, and make sure that the assumptions behind the parametric test are satisfied using diagnostic plots. (Note: in this dataset, some values are missing.)",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "43_Anova.html#footnotes",
    "href": "43_Anova.html#footnotes",
    "title": "19  Analysis of Variance",
    "section": "",
    "text": "Unweighted in this case simply means that we take the average of all factor level means regardless of the number of observations within them. If all factor levels contain the same number of observations, this corresponds to the standard mean of all observations.↩︎\nEach group is actually \\(N(\\mu_i, \\sigma^2 = 1)\\).↩︎\nThe Kruskal–Wallis test is sometimes referred to as “non-parametric ANOVA”. While this is perfectly fine as a label, one should be aware that it is, strictly speaking, a misnomer: the Kruskal–Wallis test does not rely on computing variances at all.↩︎\nIn Chapter 20 we will see examples where multiple independent factors are varied, and each possible combination results in a separate group. For example, if the effects of three different dosages of vitamin C are examined on the tooth growth of Guinea pigs, and the vitamin is also supplied in two distinct forms of either orange juice or raw ascorbic acid, then there will be \\(3 \\cdot 2 = 6\\) groups, defined by the two factors of dosage and form of supplement.↩︎",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "44_Anova_two_way.html",
    "href": "44_Anova_two_way.html",
    "title": "20  Two-way ANOVA and the Scheirer–Ray–Hare test",
    "section": "",
    "text": "20.1 Two-way ANOVA\nChapter 19 discussed techniques for analyzing data which fall into multiple categories, but those categories are levels of a single factor. Here we go further and work with data classified by two independent factors.\nA good example is provided by the built-in dataset ToothGrowth, which contains data on the tooth growth of Guinea pigs in response to receiving vitamin C.\nlibrary(tidyverse)\n\nas_tibble(ToothGrowth) |&gt; \n  print(n = Inf)\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n11  16.5 VC      1  \n12  16.5 VC      1  \n13  15.2 VC      1  \n14  17.3 VC      1  \n15  22.5 VC      1  \n16  17.3 VC      1  \n17  13.6 VC      1  \n18  14.5 VC      1  \n19  18.8 VC      1  \n20  15.5 VC      1  \n21  23.6 VC      2  \n22  18.5 VC      2  \n23  33.9 VC      2  \n24  25.5 VC      2  \n25  26.4 VC      2  \n26  32.5 VC      2  \n27  26.7 VC      2  \n28  21.5 VC      2  \n29  23.3 VC      2  \n30  29.5 VC      2  \n31  15.2 OJ      0.5\n32  21.5 OJ      0.5\n33  17.6 OJ      0.5\n34   9.7 OJ      0.5\n35  14.5 OJ      0.5\n36  10   OJ      0.5\n37   8.2 OJ      0.5\n38   9.4 OJ      0.5\n39  16.5 OJ      0.5\n40   9.7 OJ      0.5\n41  19.7 OJ      1  \n42  23.3 OJ      1  \n43  23.6 OJ      1  \n44  26.4 OJ      1  \n45  20   OJ      1  \n46  25.2 OJ      1  \n47  25.8 OJ      1  \n48  21.2 OJ      1  \n49  14.5 OJ      1  \n50  27.3 OJ      1  \n51  25.5 OJ      2  \n52  26.4 OJ      2  \n53  22.4 OJ      2  \n54  24.5 OJ      2  \n55  24.8 OJ      2  \n56  30.9 OJ      2  \n57  26.4 OJ      2  \n58  27.3 OJ      2  \n59  29.4 OJ      2  \n60  23   OJ      2\nAs seen, there are three dosage levels (0.5, 1, and 2) and two types of supplement (VC for vitamin C in the form of raw ascorbic acid, and OJ for orange juice). As usual, we first visualize the data. In doing so, it is important to convert dose to a factor (Section 8.5): the three dosage levels play the role of a categorical variable (“low”, “medium” ,and “high” levels of vitamin C dosage), and we are not so interested in the actual magnitudes of those dosages.\nas_tibble(ToothGrowth) |&gt;\n  mutate(dose = as_factor(dose)) |&gt;\n  ggplot(aes(x = supp, y = len)) +\n  geom_boxplot(alpha = 0.2, outlier.shape = NA,\n               color = \"steelblue\", fill = \"steelblue\") +\n  geom_jitter(alpha = 0.4, width = 0.05, color = \"steelblue\") +\n  labs(x = \"vitamin C dosage [mg/day]\", y = \"tooth length [mm]\") +\n  facet_grid(. ~ dose, labeller = label_both) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 20.1: Distribution of tooth growth for all combinations of factors\nOur interpretation of the figure show that in general the higher the dosage the longer the teeth become, indicating there is some kind of effect. We would also expect an effect of supplement type, because orange juice seems to perform better (at least no worse) than raw ascorbic acid in facilitating tooth growth.\nContinuing with the linear models from Chapter 19, we should first provide a factor effect coding to the two variables and then estimate a model:\nToothGrowth &lt;- \n  ToothGrowth |&gt; \n  mutate(dose = as_factor(dose))\n\ncontrasts(ToothGrowth$supp) &lt;- contr.sum(2)\ncontrasts(ToothGrowth$dose) &lt;- contr.sum(3)\n\nToothGrowth |&gt;\n  lm(len ~ dose + supp, data = _) |&gt;\n  anova()\n\nAnalysis of Variance Table\n\nResponse: len\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \ndose       2 2426.43 1213.22  82.811 &lt; 2.2e-16 ***\nsupp       1  205.35  205.35  14.017 0.0004293 ***\nResiduals 56  820.43   14.65                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nThe new feature above is the inclusion of dose + supp as the predictor, instead of just a single one. Mathematically, this translates to the following model:\n\\[\n  \\begin{aligned}\n    Y_{ijk} = \\mu + \\underbrace{\\beta_1 \\cdot X_{1jk} + \\beta_{2}\\cdot X_{2jk}}_{\\text{Factor Dose}} + \\underbrace{\\beta_3 \\cdot X_{i1k}}_{\\text{Factor Supplement}} + \\varepsilon_{ijk}\n  \\end{aligned}\n\\tag{20.1}\\]\nwhere\n\\[\n\\begin{aligned}\n    X_{1jk} &=\n    \\begin{cases}\n        1 \\quad &\\text{if dose 0.5}\\\\\n        -1      &\\text{if dose 2}\\\\\n        0       &\\text{otherwise}\n    \\end{cases} \\\\\n    X_{2jk} &=\n    \\begin{cases}\n        1 \\quad &\\text{if dose 1}\\\\\n        -1      &\\text{if dose 2}\\\\\n        0       &\\text{otherwise}\n    \\end{cases} \\\\\n    X_{i1k} &=\n    \\begin{cases}\n        1 \\quad &\\text{if supp OJ}\\\\\n        -1      &\\text{if supp VC}\\\\\n        0       &\\text{otherwise}\n    \\end{cases}\n\\end{aligned}\n\\] As seen from the ANOVA table above, both dosage and supplement type appear to have a real effect on tooth growth using a 5% level of significance.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-way ANOVA and the Scheirer--Ray--Hare test</span>"
    ]
  },
  {
    "objectID": "44_Anova_two_way.html#two-way-anova",
    "href": "44_Anova_two_way.html#two-way-anova",
    "title": "20  Two-way ANOVA and the Scheirer–Ray–Hare test",
    "section": "",
    "text": "20.1.1 Interactions\nHowever, this model ignores something that might be potentially relevant: the interaction between the two factors. This means that the nature of the relationship between tooth length and one of the predictors depends on the value of the other predictor. For the Guinea pig data, a case can be made based on Figure 20.1 that the effect of the supplement type depends on dosage: when the dosage level is either 0.5 or 1 mg/day, orange juice leads to longer teeth than ascorbic acid—but this benefit disappears at the highest dosage level of 2 mg/day.\nThis is more easily seen in an interaction plot that uses the cell means as a representation of the effect for each combination of factor levels. If the lines diverge or come together — i.e. they are not parallel — we have an indication that there might be some interaction effects happening.\n\n# Calculates cell means\nmeans &lt;- \n  ToothGrowth |&gt; \n  group_by(supp, dose) |&gt; \n  summarize(\n    mean = mean(len)\n  ) |&gt; \n  ungroup()\n\n# Shows each cell mean\nggplot(ToothGrowth) + \n  aes(x = dose, y = len, group = supp, color = supp) + \n  geom_point(alpha = 0.5) + theme_bw() + \n  scale_color_manual(\"Supplement\", values = c(\"steelblue\", \"#d9230f\")) + \n  geom_line(\n    data = means,\n    aes(y = mean), \n    linewidth = 1.2\n  ) + \n  labs(x = \"Dose\", y = \"Length\")\n\n\n\n\n\n\n\nFigure 20.2: Interaction plot of the cell means\n\n\n\n\n\nAccounting for interaction terms in a regression model is easy. All one needs to do is add one more term to the formula, denoted dose:supp:\n\nToothGrowth |&gt;\n  lm(len ~ dose + supp + dose:supp, data = _) |&gt;\n  anova()\n\nAnalysis of Variance Table\n\nResponse: len\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \ndose       2 2426.43 1213.22  92.000 &lt; 2.2e-16 ***\nsupp       1  205.35  205.35  15.572 0.0002312 ***\ndose:supp  2  108.32   54.16   4.107 0.0218603 *  \nResiduals 54  712.11   13.19                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe model result confirms that our intuition was likely correct: there does appear to be a real interaction effect between the two factors. Mathematically, the model reads:\n\\[\n  \\begin{aligned}\n    Y_{ijk} = &\\mu + \\underbrace{\\beta_1 \\cdot X_{1jk} + \\beta_{2} \\cdot X_{2jk}}_{\\text{Factor Dose}} + \\underbrace{\\beta_3 \\cdot X_{i1k}}_{\\text{Factor Supplement}} + \\\\\n    &\\underbrace{\\beta_4 \\cdot X_{1jk} \\cdot X_{i1k} + \\beta_5 \\cdot X_{2jk} \\cdot X_{i1k}}_{\\text{Interaction}} + \\varepsilon_{ijk}\n  \\end{aligned}\n\\tag{20.2}\\]\nwhere \\(\\beta_4\\) and \\(\\beta_5\\) are the effects of the different products of coded variables. The number of added interactions can be calculated using the product of the number of coded variables for each factor, in this case 2 \\(\\cdot\\) 1.\nThe inclusion of two factors with their interaction is so common in linear models that there is a shorthand notation to make it easier. Writing dose * supp is exactly the same as the above dose + supp + dose:supp. Let us see this in action:\n\nmodel &lt;- \n  ToothGrowth |&gt;\n  lm(len ~ dose * supp, data = _) \n\nmodel |&gt;\n  anova()\n\nAnalysis of Variance Table\n\nResponse: len\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \ndose       2 2426.43 1213.22  92.000 &lt; 2.2e-16 ***\nsupp       1  205.35  205.35  15.572 0.0002312 ***\ndose:supp  2  108.32   54.16   4.107 0.0218603 *  \nResiduals 54  712.11   13.19                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe result is identical to what we had before as we don’t change the variation within either variable. The only difference we would see with factor effect coding is the values of the estimated parameters as they are now interpreted against the overall mean.\n\n\n20.1.2 Residual analysis\nAs in the case of one-way ANOVA, diagnostic plots and post-hoc testing (Tukey test) are useful tools. The diagnostic plots look ok, there is some differing variance in the residuals depending on the estimated value, but we can be relatively confident about interpreting the p-values and other statistics of the linear model correctly:\n\nmodel |&gt;\n  diagnosticPlots()\n\n\n\n\n\n\n\n\nThe F-test is now split into three different components, the interaction and the two main factor effects. Formally they each test the \\(\\beta\\) parameters tied to each of the components from Equation 20.2, for example checking for interaction effects means:\n\\[\n  \\begin{aligned}\n  &H_0: \\beta_4 = \\beta_5 = 0\\\\\n  &H_a: \\text{At least one } \\beta \\text{ in } H_0 \\ne 0\n  \\end{aligned}\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to start the inference with the interaction terms because the result of that test changes how we interpret the others.\nIf the interaction is found to be significant, we do not interpret the main factor effects because the interaction affects these main effects. Independently testing for the main effect would not include these interactions and we would most likely draw the wrong conclusions.\nIf the interaction is not found to be significant, we can move on to interpreting each of the individual main factor effects of factor A and B, in the example “dose” and “supplement”.\n\n\nLooking back at the ANOVA-table and the interaction row, the corresponding p-value is less than 5%. This means that there exist a significant interaction between dose and supplement at a 5% level of significance. As such we do not interpret the tests for the main effects.\n\n\n20.1.3 Multiple comparisons in two-way ANOVA\nSimilar to the interpretations of the F-tests, multiple comparisons are affected by the presence — or absence — of an interaction. If the interaction is deemed significant we are no longer interested in comparing the factor level means of each respective factor as we did in the one-way ANOVA model. Instead we need to look at the pairwise differences of the cell means as they include the interaction effect.\nThe Tukey test can be used to compare each factor in isolation, as well as their combinations by specifying which factor you want to analyze in specs of the emmeans function. In the case of cell means we specify the interaction between the two factors:\n\nmeans &lt;- emmeans(model, specs = ~supp*dose)\n\nmeans |&gt;\n  pairs(adjust = \"tukey\") |&gt; \n  as_tibble() |&gt; \n  arrange(\n    abs(estimate) |&gt; \n      desc()\n  )\n\n# A tibble: 15 × 6\n   contrast                estimate    SE    df  t.ratio   p.value\n   &lt;chr&gt;                      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 VC dose0.5 - VC dose2   -18.2     1.62    54 -11.2    4.82 e-13\n 2 VC dose0.5 - OJ dose2   -18.1     1.62    54 -11.1    4.86 e-13\n 3 VC dose0.5 - OJ dose1   -14.7     1.62    54  -9.06   2.99 e-11\n 4 OJ dose0.5 - VC dose2   -12.9     1.62    54  -7.95   1.77 e- 9\n 5 OJ dose0.5 - OJ dose2   -12.8     1.62    54  -7.90   2.13 e- 9\n 6 OJ dose0.5 - OJ dose1    -9.47    1.62    54  -5.83   4.61 e- 6\n 7 VC dose1 - VC dose2      -9.37    1.62    54  -5.77   5.77 e- 6\n 8 VC dose1 - OJ dose2      -9.29    1.62    54  -5.72   6.91 e- 6\n 9 VC dose0.5 - VC dose1    -8.79    1.62    54  -5.41   2.10 e- 5\n10 OJ dose1 - VC dose1       5.93    1.62    54   3.65   7.39 e- 3\n11 OJ dose0.5 - VC dose0.5   5.25    1.62    54   3.23   2.43 e- 2\n12 OJ dose0.5 - VC dose1    -3.54    1.62    54  -2.18   2.64 e- 1\n13 OJ dose1 - VC dose2      -3.44    1.62    54  -2.12   2.94 e- 1\n14 OJ dose1 - OJ dose2      -3.36    1.62    54  -2.07   3.19 e- 1\n15 OJ dose2 - VC dose2      -0.0800  1.62    54  -0.0493 1.000e+ 0\n\n\nFormally we test for each row the following hypotheses: \\[\n  \\begin{aligned}\n  &H_0: \\mu_{ij} -  \\mu_{i'j'} = 0\\\\\n  &H_A: \\mu_{ij} -  \\mu_{i'j'} \\ne 0\n  \\end{aligned}\n\\] where \\(i = i'\\) and \\(j = j'\\) cannot both be true at the same time1.\nSince we have sorted the table in descending order of magnitude for the differences we can say that the first 11 pairwise differences of cell means are significant.\n\n\n\n\n\n\nNote\n\n\n\nIf the interaction term was not significant, we would instead look at the factor level means similar to the one-way ANOVA model. Using a list of each of the factors the means object now contains two different tables, one for each factor. We can then do pairwise comparisons for each using a Tukey adjustment.\n\nmeans &lt;- emmeans(model, specs = list(\"dose\", \"supp\"))\n\nNOTE: Results may be misleading due to involvement in interactions\nNOTE: Results may be misleading due to involvement in interactions\n\nmeans[[1]] |&gt;\n  pairs(adjust = \"tukey\") |&gt; \n  as_tibble() |&gt; \n  arrange(\n    abs(estimate) |&gt; \n      desc()\n  )\n\n# A tibble: 3 × 6\n  contrast        estimate    SE    df t.ratio  p.value\n  &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 dose0.5 - dose2   -15.5   1.15    54  -13.5  4.38e-13\n2 dose0.5 - dose1    -9.13  1.15    54   -7.95 3.55e-10\n3 dose1 - dose2      -6.37  1.15    54   -5.54 2.71e- 6\n\nmeans[[2]] |&gt;\n  pairs(adjust = \"tukey\") |&gt; \n  as_tibble() |&gt; \n  arrange(\n    abs(estimate) |&gt; \n      desc()\n  )\n\n# A tibble: 1 × 6\n  contrast estimate    SE    df t.ratio  p.value\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 OJ - VC      3.70 0.938    54    3.95 0.000231\n\n\nNote that the function will warn that the model contains an interaction term which might impact the results, but as long as we have concluded the interaction not have a significant effect we can disregard this warning.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-way ANOVA and the Scheirer--Ray--Hare test</span>"
    ]
  },
  {
    "objectID": "44_Anova_two_way.html#the-scheirerrayhare-test",
    "href": "44_Anova_two_way.html#the-scheirerrayhare-test",
    "title": "20  Two-way ANOVA and the Scheirer–Ray–Hare test",
    "section": "20.2 The Scheirer–Ray–Hare test",
    "text": "20.2 The Scheirer–Ray–Hare test\nFor the sake of completeness, we mention that much like in the case of one-way ANOVA, there is a non-parametric version of the two-way ANOVA as well. This is the Scheirer–Ray–Hare test, which is therefore the two-way analogue of the Kruskal–Wallis test. To use this test, one must install and load the package rcompanion:\n\ninstall.packages(\"rcompanion\")\n\nlibrary(rcompanion)\n\nAnd now, we can use the function scheirerRayHare much like kruskal.test or lm:\n\nToothGrowth |&gt;\n  scheirerRayHare(len ~ dose * supp, data = _)\n\n\nDV:  len \nObservations:  60 \nD:  0.999222 \nMS total:  305 \n\n\n          Df  Sum Sq      H p.value\ndose       2 12394.4 40.669 0.00000\nsupp       1  1050.0  3.445 0.06343\ndose:supp  2   515.5  1.692 0.42923\nResiduals 54  4021.1               \n\n\nNote that this test is skeptical about the role of the supplement type, and definitely thinks that the interaction between it and dosage is not different from what one might get by pure chance. This illustrates one problem with the test: it is not very powerful in detecting patterns, even when they are there. To make matters worse, there is no appropriate post-hoc test available in conjunction with the Scheirer–Ray–Hare test. For these reasons, its use is more restricted than of other non-parametric tests, like the Wilcoxon rank sum and Kruskal–Wallis tests. It is good to know about it as an option, but often one must rely on other methods, such as the parametric two-way ANOVA.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-way ANOVA and the Scheirer--Ray--Hare test</span>"
    ]
  },
  {
    "objectID": "44_Anova_two_way.html#sec-ancova",
    "href": "44_Anova_two_way.html#sec-ancova",
    "title": "20  Two-way ANOVA and the Scheirer–Ray–Hare test",
    "section": "20.3 Interactions in a regression model",
    "text": "20.3 Interactions in a regression model\nIn Section 18.1 we introduced adding multiple quantitative and qualitative explanatory variables to a model but we did not include the use of interactions. In Section 20.1.1 we allowed for the two factors to interact and together help explain the response variable and in a multiple regression model we can do the same for any and all combination of quantitative and qualitative variables. The purpose of interactions is to explain more complex — most often non-linear — relationships in a linear model. In fact, one can build arbitrarily complicated linear models from an arbitrary combination of quantitative and qualitative variables, and their interactions.\nLet us consider the built-in CO2 dataset as an example, which was already used before in Section 18.3. Briefly, the data contain measurements from an experiment on the cold tolerance of the grass species Echinochloa crus-galli. The dataset has five variables:\n\nPlant (a unique identifier for each plant individual),\nType (either Quebec or Mississippi depending on the origin of the plant),\nTreatment (whether the plant individual was chilled or nonchilled for the experiment),\nconc (ambient carbon dioxide concentration),\nuptake (carbon dioxide uptake rate by the plant).\n\n\nlibrary(tidyverse)\nas_tibble(CO2)\n\n# A tibble: 84 × 5\n   Plant Type   Treatment   conc uptake\n   &lt;ord&gt; &lt;fct&gt;  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 Qn1   Quebec nonchilled    95   16  \n 2 Qn1   Quebec nonchilled   175   30.4\n 3 Qn1   Quebec nonchilled   250   34.8\n 4 Qn1   Quebec nonchilled   350   37.2\n 5 Qn1   Quebec nonchilled   500   35.3\n 6 Qn1   Quebec nonchilled   675   39.2\n 7 Qn1   Quebec nonchilled  1000   39.7\n 8 Qn2   Quebec nonchilled    95   13.6\n 9 Qn2   Quebec nonchilled   175   27.3\n10 Qn2   Quebec nonchilled   250   37.1\n# ℹ 74 more rows\n\n\nWe can plot the observed distributions of CO2 uptake rates for each type and treatment:\n\nas_tibble(CO2) |&gt;\n  ggplot(aes(x = 0, y = uptake)) +\n  geom_boxplot(color = \"steelblue\", fill = \"steelblue\",\n               alpha = 0.2, outlier.shape = NA) +\n  geom_jitter(color = \"steelblue\", alpha = 0.5, width = 0.05) +\n  facet_grid(Type ~ Treatment) +\n  labs(y = \"uptake rate\") +\n  theme_bw() +\n  theme(axis.title.x = element_blank(), # The x-axis is meaningless here,\n        axis.ticks.x = element_blank(), # so remove title, tick marks,\n        axis.text.x = element_blank())  # and labels from it\n\n\n\n\n\n\n\n\nHowever this is only part of the story, as becomes obvious if we also plot the ambient CO2 concentrations (conc) along the x-axis:\n\nas_tibble(CO2) |&gt;\n  ggplot(aes(x = conc, y = uptake)) +\n  geom_point(color = \"steelblue\", alpha = 0.8) +\n  facet_grid(Type ~ Treatment) +\n  labs(x = \"concentration\", y = \"uptake rate\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe see that there is also a clear, saturating relationship between CO2 concentration and uptake rates that is definitely not linear. This does not mean that a linear model is useless for analyzing these data: the trend of whether the data increase or decrease can still be captured (although it is not recommended to use the model for numerical prediction purposes). One model that may come to mind is as follows:\n\nlm(uptake ~ conc + Type * Treatment, data = CO2) |&gt; \n  summary()\n\n\nCall:\nlm(formula = uptake ~ conc + Type * Treatment, data = CO2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.4240  -2.3674   0.7641   3.8749   9.6278 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      27.620528   1.627945  16.966  &lt; 2e-16 ***\nconc                              0.017731   0.002225   7.969 1.00e-11 ***\nTypeMississippi                  -9.380952   1.851185  -5.068 2.59e-06 ***\nTreatmentchilled                 -3.580952   1.851185  -1.934   0.0566 .  \nTypeMississippi:Treatmentchilled -6.557143   2.617972  -2.505   0.0143 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.999 on 79 degrees of freedom\nMultiple R-squared:  0.7072,    Adjusted R-squared:  0.6923 \nF-statistic: 47.69 on 4 and 79 DF,  p-value: &lt; 2.2e-16\n\n\nIn other words, the uptake rates are modeled via a combination of the effect of concentration (a continuous variable) plus the interaction of type and treatment (two categorical variables). Recall that Type * Treatment is shorthand for Type + Treatment + Type:Treatment, the sum of the main effects and the interaction between them. Mathematically, the equation for the model reads: \\[\n\\begin{split}\n(\\text{uptake})_i &\n= \\beta_0\n+ \\beta_1 \\cdot (\\text{conc})_i\n+ \\beta_2 \\cdot (\\text{Type is Mississippi})_i \\\\ &\n+ \\beta_3 \\cdot (\\text{Treatment is chilled})_i \\\\ &\n+ \\beta_4 \\cdot (\\text{Type is Mississippi})_i \\cdot(\\text{Treatment is chilled})_i\n+ \\varepsilon_i\n\\end{split}\n\\tag{20.3}\\] where \\((\\text{conc})_i\\) is a continuous predictor and not an indicator variable—that is, it takes on the actual value of the CO2 concentration in observation \\(i\\). By contrast, \\((\\text{Type is Mississippi})_i\\) and \\((\\text{Treatment is chilled})_i\\) are indicator variables that take on the value 1 if data point \\(i\\) belongs in their category and 0 otherwise.\n\n\n\n\n\n\nNote\n\n\n\nWe could just as easily use factor effect coding even in a regression model. The choice of coding is generally at the will of the analyst but we must take into account how the coding changes the interpretations of the parameter estimates.\n\n\nThe rationale for having chosen the model uptake ~ conc + Type * Treatment is that the box plots above reveal a potential interaction between the two factors Type and Treatment (the effect of changing Treatment from chilled to nonchilled depends on whether the Type was Quebec or Mississippi), and on top of this, we also want to capture the positive dependence on CO2 concentration. The coefficient table tend to concur: the only non-significant parameter at 5% significance is Treatment but that is probably on account of the significant interaction term having an influence on the main effect. If the interaction is significant, the main effect needs to be included in the model to help build the interaction.\nTo make sure that the assumptions on which this interpretation rests are held, we look at the diagnostic plots:\n\nlm(uptake ~ conc + Type * Treatment, data = CO2) |&gt;\n  diagnosticPlots(alpha = 0.7)\n\n\n\n\n\n\n\n\nThe Q-Q plot and histogram are not good: in the lower quantiles, the observed residuals are consistently larger in magnitude — more negative — than the theoretical expectation based on the assumption of normality. This, of course, is a consequence of the data depending on concentrations in a manifestly nonlinear way. We can also be somewhat skeptical about the assumption of equal variance as the residuals seem to be closer together for smaller estimated values and further apart for larger values.\nRegardless of how this model looks, one can argue based on the plot of the data that there could also be an interaction between conc and the other two factors. After all, the saturation levels of the uptake rate are always higher in Quebec than in Mississippi, and the effect of chilling also depends on Type. A model which accounts for all these effects and their interactions is uptake ~ conc * Type * Treatment. Mathematically: \\[\n\\begin{split}\n(\\text{uptake})_i &\n= \\beta_0\n+ \\beta_1 \\cdot (\\text{conc})_i\n+ \\beta_2 \\cdot (\\text{Type is Mississippi})_i \\\\ &\n+ \\beta_3 \\cdot (\\text{Treatment is chilled})_i \\\\ &\n+ \\beta_4 \\cdot (\\text{conc})_i \\cdot (\\text{Type is Mississippi})_i \\\\ &\n+ \\beta_5 \\cdot (\\text{conc})_i \\cdot (\\text{Treatment is chilled})_i \\\\ &\n+ \\beta_6 \\cdot (\\text{Type is Mississippi})_i \\cdot (\\text{Treatment is chilled})_i \\\\ &\n+ \\beta_7 \\cdot (\\text{conc})_i \\cdot (\\text{Type is Mississippi})_i\n\\cdot (\\text{Treatment is chilled})_i\n+ \\varepsilon_i\n\\end{split}\n\\tag{20.4}\\]\n(The \\(\\beta_7\\) term is multiplied by a three-way interaction of concentration, type, and treatment.) Fitting the model and creating diagnostic plots:\n\nthreewayModel &lt;- lm(uptake ~ conc * Type * Treatment, data = CO2) \n\nthreewayModel |&gt; \n  summary()\n\n\nCall:\nlm(formula = uptake ~ conc * Type * Treatment, data = CO2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.3773  -2.7602   0.9517   3.7368  10.7414 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                           25.585034   2.255256  11.345  &lt; 2e-16 ***\nconc                                   0.022410   0.004295   5.218 1.52e-06 ***\nTypeMississippi                       -7.131741   3.189414  -2.236   0.0283 *  \nTreatmentchilled                      -4.163993   3.189414  -1.306   0.1956    \nconc:TypeMississippi                  -0.005171   0.006074  -0.851   0.3973    \nconc:Treatmentchilled                  0.001340   0.006074   0.221   0.8259    \nTypeMississippi:Treatmentchilled      -1.747509   4.510513  -0.387   0.6995    \nconc:TypeMississippi:Treatmentchilled -0.011057   0.008589  -1.287   0.2019    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.789 on 76 degrees of freedom\nMultiple R-squared:  0.7376,    Adjusted R-squared:  0.7134 \nF-statistic: 30.52 on 7 and 76 DF,  p-value: &lt; 2.2e-16\n\nthreewayModel |&gt; \n  anova()\n\nAnalysis of Variance Table\n\nResponse: uptake\n                    Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nconc                 1 2285.0  2285.0  68.1766 3.545e-12 ***\nType                 1 3365.5  3365.5 100.4164 1.516e-15 ***\nTreatment            1  988.1   988.1  29.4821 6.512e-07 ***\nconc:Type            1  208.0   208.0   6.2060   0.01491 *  \nconc:Treatment       1   31.9    31.9   0.9509   0.33258    \nType:Treatment       1  225.7   225.7   6.7350   0.01134 *  \nconc:Type:Treatment  1   55.5    55.5   1.6570   0.20192    \nResiduals           76 2547.2    33.5                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nthreewayModel |&gt;\n  diagnosticPlots(alpha = 0.7)\n\n\n\n\n\n\n\n\nThis confirms what we saw on the plot of the data: that the basic shape of the relationship between concentration and uptake is unaffected by either Type or Treatment (i.e., the term conc:Type:Treatment in the ANOVA table has a high associated p-value). It also illustrates the general point that there are very often multiple candidate models, and choosing between them is a question of judgment, trial-and-error, and successively improving the model structure based on results from earlier modeling attempts.\n\n\n\n\n\n\nImportant\n\n\n\nWhen estimating a multiple regression model, especially one with a variety of interactions, we tend to encounter something called multicollinearity. Simply put, this phenomenon means that we have more than one variable (component) in the model that explains the same “part” of the response and the model overestimates the standard error of its parameters.\nThe consequence is that inference for parameters tend to not reject the null hypothesis even though we know the variable has a pairwise effect on the response. We also see this effect in the discrepancy between partial F-tests and simple t-tests for parameters, essentially them not agreeing with one another.\nWe will not delve deeper into the topic in this literature, but there are ways of both identifying and mitigating the problems of multicollinearity that should be investigated when estimating a multiple regression model.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-way ANOVA and the Scheirer--Ray--Hare test</span>"
    ]
  },
  {
    "objectID": "44_Anova_two_way.html#sec-exercises-anova-two-way",
    "href": "44_Anova_two_way.html#sec-exercises-anova-two-way",
    "title": "20  Two-way ANOVA and the Scheirer–Ray–Hare test",
    "section": "20.4 Exercises",
    "text": "20.4 Exercises\n\nThe file cow_growth.csv has data on the growth of individual cows which have received different grains (wheat, oats, or barley) and, independently, one of four different dietary supplements (one of which is no supplement, for control). Each of these diet combinations (twelve diets: three grains, times four supplements) had four cows observed. Is there any effect of these treatments on cow growth? Is there any interaction between the grain and the supplement given to the cows—some secret super-combination which makes the cows grow especially well (or poorly)?\n\nAs usual, before doing any tests, visualize and explore the data, and make sure you have a solid expectation for the results of any statistical analysis.\nAnswer the question by applying a parametric test. Run post-hoc tests as well if needed. Do not forget to create diagnostic plots, to see if the assumptions behind the parametric test are satisfied to an acceptable degree.\n\nThe built-in CO2 data frame contains measurements from an experiment on the cold tolerance of the grass species Echinochloa crus-galli. The dataset has five columns:\n\nPlant: unique identifier for each plant individual.\nType: either Quebec or Mississippi, depending on the origin of the plant.\nTreatment: whether the plant individual was chilled or nonchilled for the experiment.\nconc: carbon dioxide concentration in the surrounding environment.\nuptake: carbon dioxide uptake rate.\n\nHow do uptake rates depend on Type, Treatment, and their interaction? (For this exercise, you can ignore Plant and conc.) Start by forming a hypothesis based on visualizing the data. Then perform a parametric test and a corresponding post-hoc test. Make sure to use diagnostic plots to gauge the quality of the test’s assumptions.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-way ANOVA and the Scheirer--Ray--Hare test</span>"
    ]
  },
  {
    "objectID": "44_Anova_two_way.html#footnotes",
    "href": "44_Anova_two_way.html#footnotes",
    "title": "20  Two-way ANOVA and the Scheirer–Ray–Hare test",
    "section": "",
    "text": "As that would mean we are testing the same cell mean against itself.↩︎",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-way ANOVA and the Scheirer--Ray--Hare test</span>"
    ]
  },
  {
    "objectID": "45_Intro_map.html",
    "href": "45_Intro_map.html",
    "title": "21  Higher-order functions and mapping",
    "section": "",
    "text": "21.1 Introduction\nlibrary(tidyverse) # Loading the tidyverse, before doing anything else\nIn Chapter 3 we learned how to create user-defined functions. An example was provided in ?sec-onewayanova, where we made our life easier by eliminating the need to always call aov before performing a Tukey test with TukeyHSD. Without the function, we must write:\nlm(weight ~ group, data = PlantGrowth) |&gt; aov() |&gt; TukeyHSD()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = lm(weight ~ group, data = PlantGrowth))\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\nInstead, we can write a simple function that takes a linear model fit object as its input and produces the Tukey test as its output:\ntukeyTest &lt;- function(modelFit) modelFit |&gt; aov() |&gt; TukeyHSD()\nUsing this function, we can now simply write:\nlm(weight ~ group, data = PlantGrowth) |&gt; tukeyTest()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = modelFit)\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\nAnother useful function we can write helps use statistical procedures within a pipeline. As we have seen, most statistical functions take a formula as their first argument and the data as their second (and then they may take further, method-specific arguments as well, like conf.int in the function wilcox.test). Since the pipe operator |&gt; is often used in conjunction with functions like select, mutate, pivot_longer, summarize, etc. which all return a data frame, it would be convenient to reverse the order of arguments in all statistical functions, with the data coming first and the formula coming second. In fact, such a function is easy to write. We could call it tidystat:\ntidystat &lt;- function(data, formula, method) method(formula, data)\nHere method is the statistical function we wish to use. For example:\nPlantGrowth |&gt;\n  filter(group != \"ctrl\") |&gt;\n  tidystat(weight ~ group, wilcox.test)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\nThis is now fully equivalent to:\nPlantGrowth |&gt;\n  filter(group != \"ctrl\") |&gt;\n  wilcox.test(weight ~ group, data = _)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\nWe can add a further improvement to the tidystat function. As it is, it can only take the formula and the data as inputs, but not any other, function-specific arguments. In R, there is a way of passing arbitrary extra arguments, using the ellipsis (...). We can redefine tidystat this way:\ntidystat &lt;- function(data, formula, method, ...) {\n  method(formula, data, ...) # The ... means \"possibly more arguments\"\n}\nAnd now, we can pass extra arguments that we would not have been able to do before. For instance, we can request confidence intervals from wilcox.test:\nPlantGrowth |&gt;\n  filter(group != \"ctrl\") |&gt;\n  tidystat(weight ~ group, wilcox.test, conf.int = TRUE, conf.level = 0.99)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n99 percent confidence interval:\n -1.70 -0.03\nsample estimates:\ndifference in location \n                -0.945\nFrom now on, we can use tidystat for performing various statistical procedures:\nlibrary(FSA) # For the Dunn test\nPlantGrowth |&gt; tidystat(weight ~ group, lm) |&gt; anova()\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ngroup      2  3.7663  1.8832  4.8461 0.01591 *\nResiduals 27 10.4921  0.3886                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPlantGrowth |&gt; tidystat(weight ~ group, lm) |&gt; tukeyTest()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = modelFit)\n\n$group\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\nPlantGrowth |&gt; tidystat(weight ~ group, kruskal.test)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  weight by group\nKruskal-Wallis chi-squared = 7.9882, df = 2, p-value = 0.01842\n\nPlantGrowth |&gt; tidystat(weight ~ group, dunnTest)\n\n   Comparison         Z    P.unadj      P.adj\n1 ctrl - trt1  1.117725 0.26368427 0.26368427\n2 ctrl - trt2 -1.689290 0.09116394 0.18232789\n3 trt1 - trt2 -2.807015 0.00500029 0.01500087",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Higher-order functions and mapping</span>"
    ]
  },
  {
    "objectID": "45_Intro_map.html#introduction",
    "href": "45_Intro_map.html#introduction",
    "title": "21  Higher-order functions and mapping",
    "section": "",
    "text": "Note\n\n\n\nMake sure to review Chapter 3, especially Section 3.1.1, if you need a refresher on how to define functions in R.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Higher-order functions and mapping</span>"
    ]
  },
  {
    "objectID": "45_Intro_map.html#higher-order-functions",
    "href": "45_Intro_map.html#higher-order-functions",
    "title": "21  Higher-order functions and mapping",
    "section": "21.2 Higher-order functions",
    "text": "21.2 Higher-order functions\nIf we think about the tidystat function above, something strange has happened. We are used to the idea of functions taking numbers, character strings, logical values, or even data frames as their arguments. What we have not paid much attention to before is what happens if the input to a function is itself another function. Yet this is precisely what tidystat does: its method argument is a function object.\nIn R, this is perfectly legal, and its use and interpretation is every bit as natural as it was in tidystat. Functions which take other functions as arguments are often called higher-order functions.1 To emphasize again: there really is nothing special about such functions, and they can be used in much the same way as “ordinary” functions.\nOne very natural example for a higher-order function is integration. An integral (at least in simple cases) takes three inputs: a function to integrate, a lower limit of integration, and an upper limit of integration. The output is the (sign-weighted) area under the function’s curve, evaluated between the lower and upper limits. This is stressed even in the usual mathematical notation for integrals: when we write \\[ \\int_0^1 x^2 \\,\\text{d} x = \\frac{1}{3}\\] (a true statement), we show the lower and upper limits of 0 and 1 at the bottom and top of the integral sign, and the function to be integrated (in this case, \\(f(x) = x^2\\)) in between the integral sign and \\(\\text{d} x\\).\nIf you do not know how integrals do their magic, there is no need to worry, because R has a built-in function called integrate to do the calculations for you. integrate takes the three arguments described above: the function to integrate, and the lower and upper limits of integration. To perform the above integral, we can write:\n\n# The squaring function: sqr(2) returns 4, sqr(4) returns 16, etc.\nsqr &lt;- function(x) x^2\n# Perform the integral between 0 and 1:\nintegrate(sqr, 0, 1)\n\n0.3333333 with absolute error &lt; 3.7e-15\n\n\nThe answer is indeed one-third.2 But there was no obligation to use the square function above. We could have used any other one. For instance, to compute the integral of the cosine function \\(\\cos(x)\\) between \\(0\\) and \\(2\\pi\\), we can type:\n\nintegrate(cos, 0, 2*pi)\n\n4.359836e-16 with absolute error &lt; 4.5e-14\n\n\nWe get the expected result of zero, within numerical error.\nOne thing to know about function objects like sqr is that they do not need a name to be used. In the definition sqr &lt;- function(x) x^2, we assigned the function object function(x) x^2 to the symbol sqr, so we wouldn’t have to write it out all the time. But since sqr is just a name that stands for function(x) x^2, calling (function(x) x^2)(4) is the same as calling sqr(4), both returning 16. If a function is used only once within another (higher-order) function, then we might not wish to bother with naming the function separately. Thus, the following is exactly equivalent to integrating the sqr function:\n\nintegrate(function(x) x^2, 0, 1)\n\n0.3333333 with absolute error &lt; 3.7e-15\n\n\nFunctions without names are often called anonymous functions. They are commonly used within other, higher-order functions. Their use is not mandatory: it is always possible to first define the function with a name, and then use that name instead (e.g., using sqr instead of function(x) x^2, after defining sqr &lt;- function(x) x^2). However, they can be convenient, and it is also important to recognize them in R code written by others.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Higher-order functions and mapping</span>"
    ]
  },
  {
    "objectID": "45_Intro_map.html#sec-mapfamily",
    "href": "45_Intro_map.html#sec-mapfamily",
    "title": "21  Higher-order functions and mapping",
    "section": "21.3 The map family of functions",
    "text": "21.3 The map family of functions\nThe purrr package is a standard, automatically-loaded part of the tidyverse. It contains a large family of mapping functions. These allow one to perform repetitive tasks by applying the same function to all elements of a vector, list, or column in a data frame.\nTo illustrate their use, how would we obtain the squares of all integers from 1 to 10? Using our earlier sqr function, we could painstakingly type out sqr(1), then sqr(2), and so on, up until sqr(10) (we ought to be grateful that the task was to obtain the squares of the first ten integers, instead of the first ten thousand). But there is no need to do this, as this is exactly what map can do. map takes two arguments: some data (e.g., a vector of values), and a function. It then applies that function to all data entries. So a much quicker way of obtaining the squares of all integers from 1 to 10 is this:\n\nmap(1:10, sqr)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n[[6]]\n[1] 36\n\n[[7]]\n[1] 49\n\n[[8]]\n[1] 64\n\n[[9]]\n[1] 81\n\n[[10]]\n[1] 100\n\n\nOr, in case we prefer anonymous functions and do not want to bother with defining our own sqr routine:\n\nmap(1:10, function(x) x^2)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n[[6]]\n[1] 36\n\n[[7]]\n[1] 49\n\n[[8]]\n[1] 64\n\n[[9]]\n[1] 81\n\n[[10]]\n[1] 100\n\n\nThe results are all there, although they are prefaced by double-bracketed indices [[1]], [[2]], and so on. You may recall from Section 18.3.1 that this is the notation used to reference the entries of lists. That is correct: map returns a list of values, not a vector. We will see momentarily that this can be very useful behavior, but here it can feel overkill. Fortunately, it is easy to get back a vector instead of a list. Since the entries of vectors must have a well-defined, uniform type (numeric, character string, logical, etc.), we have to tell R what kind of result we want. In our case, we want numeric results. The function to do this is called map_dbl (“map into double-precision numerical values”). It can be used just like map; the only difference between the two is that the output type changes from list to numeric vector:\n\nmap_dbl(1:10, function(x) x^2)\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\nSimilarly, there are functions map_chr, map_lgl, and some others, which create vectors of the appropriate type. For example, to append the agglutination “-ing” to various verbs, we can do:\n\nc(\"attend\", \"visit\", \"support\", \"help\", \"savour\") |&gt;\n  map_chr(function(text) paste0(text, \"ing\"))\n\n[1] \"attending\"  \"visiting\"   \"supporting\" \"helping\"    \"savouring\" \n\n\nInterestingly, we could also try\n\nmap_chr(1:10, function(x) x^2)\n\nWarning: Automatic coercion from double to character was deprecated in purrr 1.0.0.\nℹ Please use an explicit call to `as.character()` within `map_chr()` instead.\n\n\n [1] \"1.000000\"   \"4.000000\"   \"9.000000\"   \"16.000000\"  \"25.000000\" \n [6] \"36.000000\"  \"49.000000\"  \"64.000000\"  \"81.000000\"  \"100.000000\"\n\n\nand see that, although the computations were performed correctly, the output was converted from numbers to character strings encoding those numbers.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Higher-order functions and mapping</span>"
    ]
  },
  {
    "objectID": "45_Intro_map.html#making-use-of-map-when-vectorization-fails",
    "href": "45_Intro_map.html#making-use-of-map-when-vectorization-fails",
    "title": "21  Higher-order functions and mapping",
    "section": "21.4 Making use of map when vectorization fails",
    "text": "21.4 Making use of map when vectorization fails\nOne perhaps obvious criticism of map as we have used it is that its use was not really needed. Early on, we learned that when simple functions are applied to a vector of values, they get applied element-wise. This is called vectorization, and it is a very useful property of R. So (1:10)^2, in our case, achieves the same thing as map_dbl(1:10, function(x) x^2). Similarly, if we simply write cos(1:100), we get the cosine of all integers between 1 and 100 without having to type out map_dbl(1:100, cos). So why bother with map then?\nThe answer is that map can handle cases where vectorization is not available. Most simple functions in R are vectorized, but there are plenty of non-vectorizable operations. To give an example, let us start from a simple dataset: the PlantGrowth table we looked at before, but without the control ctrl group. This leaves just the two treatment groups trt1 and trt2:\n\nplantTrt &lt;- filter(PlantGrowth, group != \"ctrl\")\nprint(plantTrt)\n\n   weight group\n1    4.81  trt1\n2    4.17  trt1\n3    4.41  trt1\n4    3.59  trt1\n5    5.87  trt1\n6    3.83  trt1\n7    6.03  trt1\n8    4.89  trt1\n9    4.32  trt1\n10   4.69  trt1\n11   6.31  trt2\n12   5.12  trt2\n13   5.54  trt2\n14   5.50  trt2\n15   5.37  trt2\n16   5.29  trt2\n17   4.92  trt2\n18   6.15  trt2\n19   5.80  trt2\n20   5.26  trt2\n\n\nWe might want to perform a Wilcoxon rank sum test on these data, but with a number of different confidence levels. A naive approach would be to supply the required confidence levels as a vector:\n\nwilcox.test(weight ~ group, data = plantTrt,\n            conf.int = TRUE, conf.level = c(0.8, 0.9, 0.95, 0.99))\n\nError in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): 'conf.level' must be a single number between 0 and 1\n\n\nThis generates an error: because of the way wilcox.test is internally implemented in R, it does not allow or recognize a vector input in place of conf.level. It must be a single number instead. This, however, can be overcome if we just use map:\n\nc(0.8, 0.9, 0.95, 0.99) |&gt; # The vector of confidence levels\n  map(function(confLevel) wilcox.test(weight ~ group, data = plantTrt,\n                            conf.int = TRUE, conf.level = confLevel))\n\n[[1]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n80 percent confidence interval:\n -1.33 -0.56\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[2]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n90 percent confidence interval:\n -1.43 -0.44\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[3]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -1.50 -0.31\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[4]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n99 percent confidence interval:\n -1.70 -0.03\nsample estimates:\ndifference in location \n                -0.945 \n\n\nNotice that we used map and not map_dbl or map_chr, because wilcox.test returns a complex model object which cannot be coerced into a vector. This is precisely when map, which generates a list whose entries can be arbitrary, is especially useful. (Try the above with map_dbl; it will throw an error.) As a final comment, it would of course have been possible to define a function separately, instead of using the anonymous function above:\n\nwilcoxConf &lt;- function(confLevel) {\n  wilcox.test(weight ~ group, data = plantTrt,\n              conf.int = TRUE, conf.level = confLevel)\n}\n\nc(0.8, 0.9, 0.95, 0.99) |&gt; map(wilcoxConf)\n\n[[1]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n80 percent confidence interval:\n -1.33 -0.56\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[2]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n90 percent confidence interval:\n -1.43 -0.44\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[3]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -1.50 -0.31\nsample estimates:\ndifference in location \n                -0.945 \n\n\n[[4]]\n\n    Wilcoxon rank sum exact test\n\ndata:  weight by group\nW = 16, p-value = 0.008931\nalternative hypothesis: true location shift is not equal to 0\n99 percent confidence interval:\n -1.70 -0.03\nsample estimates:\ndifference in location \n                -0.945",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Higher-order functions and mapping</span>"
    ]
  },
  {
    "objectID": "45_Intro_map.html#exercises",
    "href": "45_Intro_map.html#exercises",
    "title": "21  Higher-order functions and mapping",
    "section": "21.5 Exercises",
    "text": "21.5 Exercises\n\nStart from the following sequence of values: values &lt;- seq(-5*pi, 5*pi, by = 0.01) (a vector with values going from \\(-5\\pi\\) to \\(5\\pi\\), in steps of 0.01). Now do the following:\n\nDefine a new vector, x, which contains the cosine (cos) of each value in values. Use map_dbl.\nNow define another vector, y, and again using map_dbl, compute the function sin(t) + cos(14 * t / 5) / 5 for every value t in values. You can either define this function separately to use inside map_dbl, or create it anonymously.\nFinally, create a tibble whose two columns are x and y, and plot them against each other using geom_path. See what you get!\n\nHow does the integral of \\(\\cos(x)\\) change if the lower limit of integration is fixed at zero, but the upper limit gradually increases from \\(0\\) to \\(2\\pi\\)? Define a sequence of upper limits upper &lt;- seq(0, 2*pi, by = 0.1). Then, using map_dbl, create a vector integrals whose entries are the integral of \\(\\cos(x)\\) from zero to each upper limit. Finally, plot integrals against upper, using geom_point or geom_line. What is the function you see? (Note: the integrate function returns a complicated list object instead of just a single number. To access just the value of the integral, you can use integrate(...)$value, where ... means all the arguments to the function you are supposed to write when solving the problem.)",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Higher-order functions and mapping</span>"
    ]
  },
  {
    "objectID": "45_Intro_map.html#footnotes",
    "href": "45_Intro_map.html#footnotes",
    "title": "21  Higher-order functions and mapping",
    "section": "",
    "text": "Functions that return a function as their output are also called higher-order functions. For an example which both takes functions as arguments and produces a function as its output, check out the compose function from the purrr package (part of the tidyverse).↩︎\nThe error is included in the output because R’s integration routine is purely numeric, so it algorithmically approximates the integral, and such procedures always have finite precision.↩︎",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Higher-order functions and mapping</span>"
    ]
  },
  {
    "objectID": "46_Multiple_analysis.html",
    "href": "46_Multiple_analysis.html",
    "title": "22  Nested data and multiple analysis",
    "section": "",
    "text": "22.1 Motivating example\nThis chapter is on performing statistical (or other) analyses en masse. To motivate the problem, let us start from a dataset, fruit_fly_wings.csv, that has been adapted from Bolstad et al. (2015):\nlibrary(tidyverse)\n\nfly &lt;- read_csv(\"fruit_fly_wings.csv\")\nprint(fly)\n\n# A tibble: 10,327 × 6\n   Species   ID          Date      Sex   WingSize L2Length\n   &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 D_acutila ACU1006.TIF 24_Jul_01 F        0.131    0.497\n 2 D_acutila ACU1009.TIF 24_Jul_01 F        0.136    0.488\n 3 D_acutila ACU1010.TIF 24_Jul_01 F        0.195    0.540\n 4 D_acutila ACU1013.TIF 24_Jul_01 F        0.277    0.646\n 5 D_acutila ACU1018.TIF 24_Jul_01 F        0.152    0.498\n 6 D_acutila ACU1021.TIF 24_Jul_01 F        0.175    0.492\n 7 D_acutila ACU1048.TIF 24_Jul_01 F        0.230    0.600\n 8 D_acutila ACU1049.TIF 24_Jul_01 F        0.174    0.546\n 9 D_acutila ACU1054.TIF 05_Sep_01 F        0.108    0.429\n10 D_acutila ACU1059.TIF 05_Sep_01 F        0.205    0.580\n# ℹ 10,317 more rows\nThe data contain measurements on individual fruit flies, belonging to various species and either sex as indicated by the Species and Sex columns. Each individual is uniquely identified (ID), and the date of the measurement has also been recorded (Date). Most importantly, the length of the wing (WingSize) and the length of the L2 vein that runs across the wing (L2Length) have been recorded.\nWhat is the distribution of wing sizes across species and sexes? To begin answering this question, we can start with a plot:\nfly |&gt;\n  ggplot(aes(x = WingSize, y = Species, color = Sex, fill = Sex)) +\n  geom_boxplot(alpha = 0.2) +\n  scale_color_manual(values = c(\"steelblue\", \"goldenrod\")) +\n  scale_fill_manual(values = c(\"steelblue\", \"goldenrod\")) +\n  labs(x = \"Wing size\") +\n  theme_bw()\nLooking at this figure, it does appear as if females often had larger wings than males within the same species. The main question in this chapter is how we can test this—for instance, how would it be possible to perform a Wilcoxon rank sum test for all 55 species in the data?",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Nested data and multiple analysis</span>"
    ]
  },
  {
    "objectID": "46_Multiple_analysis.html#nested-data-frames",
    "href": "46_Multiple_analysis.html#nested-data-frames",
    "title": "22  Nested data and multiple analysis",
    "section": "22.2 Nested data frames",
    "text": "22.2 Nested data frames\nWe are by now used to the idea that the columns of tibbles can hold numbers, character strings, logical values, or even factors. But here is an interesting question: can a column of a tibble hold other tibbles?\nThe short answer is yes. The somewhat longer answer is that this is possible, but not directly so. Instead, one has to make the column into a list (Section 18.3.1).1 While this could be done by hand using the list function, there are other options in the tidyverse which facilitate creating tibbles which have other tibbles in their columns. One of these is called nest. This function receives a name first, which will become the name of the newly-created column holding the sub-tibbles. Then, after an equality sign, one lists the columns, in a vector, which one would like to package into those sub-tibbles. For example, to keep Species as a separate column and wrap everything else into sub-tibbles, we can do:\n\nfly |&gt; nest(data = c(ID, Date, Sex, WingSize, L2Length))\n\n# A tibble: 55 × 2\n   Species      data              \n   &lt;chr&gt;        &lt;list&gt;            \n 1 D_acutila    &lt;tibble [205 × 5]&gt;\n 2 D_algonqu    &lt;tibble [237 × 5]&gt;\n 3 D_texana     &lt;tibble [215 × 5]&gt;\n 4 Z_Sg.Anaprio &lt;tibble [200 × 5]&gt;\n 5 D_athabasca  &lt;tibble [79 × 5]&gt; \n 6 D_bifasci    &lt;tibble [217 × 5]&gt;\n 7 D_busckii    &lt;tibble [105 × 5]&gt;\n 8 I_crucige    &lt;tibble [211 × 5]&gt;\n 9 Det_nigro    &lt;tibble [15 × 5]&gt; \n10 H_duncani    &lt;tibble [219 × 5]&gt;\n# ℹ 45 more rows\n\n\nWhat do we see? We ended up with a tibble that has two columns. One is Species and has type character string. The other is data and has the type of list, as indicated by the &lt;list&gt; tag. The entries in this column are tibbles, with varying numbers of rows (as many as the number of individuals for the given species), and five columns; namely, those that we specified we wanted to nest. We can check and see what is inside these tibbles. For example, the contents of the first row (species: D. acutila) are:\n\nfly |&gt;\n  nest(data = c(ID, Date, Sex, WingSize, L2Length)) |&gt;\n  pull(data) |&gt; # Get contents of just the \"data\" column\n  pluck(1) # Take the first of all those tibbles\n\n# A tibble: 205 × 5\n   ID          Date      Sex   WingSize L2Length\n   &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 ACU1006.TIF 24_Jul_01 F        0.131    0.497\n 2 ACU1009.TIF 24_Jul_01 F        0.136    0.488\n 3 ACU1010.TIF 24_Jul_01 F        0.195    0.540\n 4 ACU1013.TIF 24_Jul_01 F        0.277    0.646\n 5 ACU1018.TIF 24_Jul_01 F        0.152    0.498\n 6 ACU1021.TIF 24_Jul_01 F        0.175    0.492\n 7 ACU1048.TIF 24_Jul_01 F        0.230    0.600\n 8 ACU1049.TIF 24_Jul_01 F        0.174    0.546\n 9 ACU1054.TIF 05_Sep_01 F        0.108    0.429\n10 ACU1059.TIF 05_Sep_01 F        0.205    0.580\n# ℹ 195 more rows\n\n\nSo this sub-table contains the information that pertains to just D. acutila individuals.\nWhen choosing which columns to wrap into sub-tibbles with nest, all the tidy selection conventions and functionalities apply that one can use with the select function (Section 5.1.1). So the above nesting could be equivalently and more simply be performed with:\n\nfly |&gt; nest(data = !Species)\n\n# A tibble: 55 × 2\n   Species      data              \n   &lt;chr&gt;        &lt;list&gt;            \n 1 D_acutila    &lt;tibble [205 × 5]&gt;\n 2 D_algonqu    &lt;tibble [237 × 5]&gt;\n 3 D_texana     &lt;tibble [215 × 5]&gt;\n 4 Z_Sg.Anaprio &lt;tibble [200 × 5]&gt;\n 5 D_athabasca  &lt;tibble [79 × 5]&gt; \n 6 D_bifasci    &lt;tibble [217 × 5]&gt;\n 7 D_busckii    &lt;tibble [105 × 5]&gt;\n 8 I_crucige    &lt;tibble [211 × 5]&gt;\n 9 Det_nigro    &lt;tibble [15 × 5]&gt; \n10 H_duncani    &lt;tibble [219 × 5]&gt;\n# ℹ 45 more rows\n\n\nThat is: apply the nesting to all columns that are not called Species. This can be used in more complicated cases as well. For example, if we wish to nest data pertaining to particular species-sex combinations, we can do the following:\n\nfly |&gt; nest(data = !Species & !Sex)\n\n# A tibble: 110 × 3\n   Species      Sex   data              \n   &lt;chr&gt;        &lt;chr&gt; &lt;list&gt;            \n 1 D_acutila    F     &lt;tibble [104 × 4]&gt;\n 2 D_acutila    M     &lt;tibble [101 × 4]&gt;\n 3 D_algonqu    F     &lt;tibble [144 × 4]&gt;\n 4 D_algonqu    M     &lt;tibble [93 × 4]&gt; \n 5 D_texana     F     &lt;tibble [108 × 4]&gt;\n 6 D_texana     M     &lt;tibble [107 × 4]&gt;\n 7 Z_Sg.Anaprio F     &lt;tibble [95 × 4]&gt; \n 8 Z_Sg.Anaprio M     &lt;tibble [105 × 4]&gt;\n 9 D_athabasca  F     &lt;tibble [20 × 4]&gt; \n10 D_athabasca  M     &lt;tibble [59 × 4]&gt; \n# ℹ 100 more rows\n\n\nwhere nest(data = !Species & !Sex) (nest all columns that are not Species and not Sex) is equivalent to the longer nest(data = c(ID, Date, WingSize, L2Length)).\nFinally, columns holding nested data can be unnested, meaning that their contents are expanded back into the original data frame. The function to do this with is called unnest:\n\nfly |&gt; nest(data = !Species & !Sex) |&gt; unnest(data)\n\n# A tibble: 10,327 × 6\n   Species   Sex   ID          Date      WingSize L2Length\n   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 D_acutila F     ACU1006.TIF 24_Jul_01    0.131    0.497\n 2 D_acutila F     ACU1009.TIF 24_Jul_01    0.136    0.488\n 3 D_acutila F     ACU1010.TIF 24_Jul_01    0.195    0.540\n 4 D_acutila F     ACU1013.TIF 24_Jul_01    0.277    0.646\n 5 D_acutila F     ACU1018.TIF 24_Jul_01    0.152    0.498\n 6 D_acutila F     ACU1021.TIF 24_Jul_01    0.175    0.492\n 7 D_acutila F     ACU1048.TIF 24_Jul_01    0.230    0.600\n 8 D_acutila F     ACU1049.TIF 24_Jul_01    0.174    0.546\n 9 D_acutila F     ACU1054.TIF 05_Sep_01    0.108    0.429\n10 D_acutila F     ACU1059.TIF 05_Sep_01    0.205    0.580\n# ℹ 10,317 more rows",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Nested data and multiple analysis</span>"
    ]
  },
  {
    "objectID": "46_Multiple_analysis.html#performing-statistical-tests-using-nested-data",
    "href": "46_Multiple_analysis.html#performing-statistical-tests-using-nested-data",
    "title": "22  Nested data and multiple analysis",
    "section": "22.3 Performing statistical tests using nested data",
    "text": "22.3 Performing statistical tests using nested data\nHow can we test for all 55 fly species in this dataset whether there is a significant difference between average male and female wing lengths? The answer is to first nest the data using fly |&gt; nest(data = !Species), so that we end up with a table which has one row per each species. We then need to run a Wilcoxon rank sum test on each of them. But this we know how to do from Chapter 21: we can rely on the map function.\n\nfly |&gt;\n  nest(data = !Species) |&gt;\n  mutate(test = map(data, function(x) wilcox.test(L2Length ~ Sex, data = x)))\n\n# A tibble: 55 × 3\n   Species      data               test   \n   &lt;chr&gt;        &lt;list&gt;             &lt;list&gt; \n 1 D_acutila    &lt;tibble [205 × 5]&gt; &lt;htest&gt;\n 2 D_algonqu    &lt;tibble [237 × 5]&gt; &lt;htest&gt;\n 3 D_texana     &lt;tibble [215 × 5]&gt; &lt;htest&gt;\n 4 Z_Sg.Anaprio &lt;tibble [200 × 5]&gt; &lt;htest&gt;\n 5 D_athabasca  &lt;tibble [79 × 5]&gt;  &lt;htest&gt;\n 6 D_bifasci    &lt;tibble [217 × 5]&gt; &lt;htest&gt;\n 7 D_busckii    &lt;tibble [105 × 5]&gt; &lt;htest&gt;\n 8 I_crucige    &lt;tibble [211 × 5]&gt; &lt;htest&gt;\n 9 Det_nigro    &lt;tibble [15 × 5]&gt;  &lt;htest&gt;\n10 H_duncani    &lt;tibble [219 × 5]&gt; &lt;htest&gt;\n# ℹ 45 more rows\n\n\nAnd voilà: we now have the Wilcoxon rank sum test results in the column test, for each species! All we need to do is retrieve this information.\nDoing so is not completely straightforward, because wilcox.test does not return a data frame or tibble. Instead, it returns a complicated model fit object which cannot be unnested into the outer table. If we try, we get an error:\n\nfly |&gt;\n  nest(data = !Species) |&gt;\n  mutate(test = map(data, function(x) wilcox.test(L2Length~Sex, data=x))) |&gt;\n  unnest(test)\n\nError in `list_sizes()`:\n! `x[[1]]` must be a vector, not a &lt;htest&gt; object.\n\n\nFortunately, there is an easy way to convert the output of the Wilcoxon rank sum test into a tibble. The broom package is designed to do exactly this. It is part of the tidyverse, though it does not get automatically loaded with it. We load this package first:\n\nlibrary(broom)\n\n\nAttaching package: 'broom'\n\n\nThe following object is masked from 'package:bslib':\n\n    bootstrap\n\n\nThe function in this package that creates a tibble out of the results of statistical models (almost any model in fact, not just the Wilcoxon rank sum test) is called tidy. Let us see how it works. If we do a Wilcoxon rank sum test between females and males for the whole data (without distinguishing between species), we get:\n\nwilcox.test(WingSize ~ Sex, data = fly, conf.int = TRUE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  WingSize by Sex\nW = 16695507, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n 0.07910817 0.09418589\nsample estimates:\ndifference in location \n            0.08664608 \n\n\nBy applying the tidy function to this result, it gets converted into a tibble:\n\nwilcox.test(WingSize ~ Sex, data = fly, conf.int = TRUE) |&gt; tidy()\n\n# A tibble: 1 × 7\n  estimate statistic   p.value conf.low conf.high method             alternative\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;      \n1   0.0866  16695507 2.54e-109   0.0791    0.0942 Wilcoxon rank sum… two.sided  \n\n\nSo we can insert a step into our analysis pipeline which converts the test column into a list of data frames:\n\nfly |&gt;\n  nest(data = !Species) |&gt;\n  mutate(test = map(data, function(x) wilcox.test(L2Length~Sex, data=x))) |&gt;\n  mutate(test = map(test, tidy)) |&gt;\n  unnest(test)\n\n# A tibble: 55 × 6\n   Species      data               statistic  p.value method         alternative\n   &lt;chr&gt;        &lt;list&gt;                 &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;      \n 1 D_acutila    &lt;tibble [205 × 5]&gt;      9640 5.03e-25 Wilcoxon rank… two.sided  \n 2 D_algonqu    &lt;tibble [237 × 5]&gt;     12494 2.34e-29 Wilcoxon rank… two.sided  \n 3 D_texana     &lt;tibble [215 × 5]&gt;      9189 7.55e-14 Wilcoxon rank… two.sided  \n 4 Z_Sg.Anaprio &lt;tibble [200 × 5]&gt;      6330 1.03e- 3 Wilcoxon rank… two.sided  \n 5 D_athabasca  &lt;tibble [79 × 5]&gt;       1116 3.13e- 9 Wilcoxon rank… two.sided  \n 6 D_bifasci    &lt;tibble [217 × 5]&gt;     11447 2.56e-33 Wilcoxon rank… two.sided  \n 7 D_busckii    &lt;tibble [105 × 5]&gt;      1654 7.63e- 2 Wilcoxon rank… two.sided  \n 8 I_crucige    &lt;tibble [211 × 5]&gt;      5238 4.63e- 1 Wilcoxon rank… two.sided  \n 9 Det_nigro    &lt;tibble [15 × 5]&gt;         20 8.51e- 1 Wilcoxon rank… two.sided  \n10 H_duncani    &lt;tibble [219 × 5]&gt;     10195 2.25e-19 Wilcoxon rank… two.sided  \n# ℹ 45 more rows\n\n\nAnd now we have the results. As an example, we can check the distribution of p-values: how often is there a statistically significant sex difference? Let us visualize this, by ordering the species based on p-values:\n\nfly |&gt;\n  nest(data = !Species) |&gt;\n  mutate(test = map(data, function(x) wilcox.test(L2Length~Sex, data=x))) |&gt;\n  mutate(test = map(test, tidy)) |&gt;\n  unnest(test) |&gt;\n  arrange(p.value) |&gt;\n  mutate(Species = as_factor(Species)) |&gt;\n  ggplot(aes(x = p.value, y = Species)) +\n  geom_col(color = \"steelblue\", fill = \"steelblue\", alpha = 0.3) +\n  scale_x_continuous(name = \"p-value\", limits = c(0, 1)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThis graph shows that most species have very small p-values, but that there are four clear outliers. We can extract the names of these outlier species:\n\noutlierSpecies &lt;- fly |&gt;\n  nest(data = !Species) |&gt;\n  mutate(test = map(data, function(x) wilcox.test(L2Length~Sex, data=x))) |&gt;\n  mutate(test = map(test, tidy)) |&gt;\n  unnest(test) |&gt;\n  arrange(desc(p.value)) |&gt;\n  slice(1:4) |&gt; # Choose first 4 rows from the sorted table\n  pull(Species) # Get the 4 species names\n\nprint(outlierSpecies)\n\n[1] \"Det_nigro\" \"N_sordida\" \"I_crucige\" \"D_busckii\"\n\n\nAnd then we can plot the wing length data for just these ones:\n\nfly |&gt;\n  filter(Species %in% outlierSpecies) |&gt;\n  ggplot(aes(x = Sex, y = WingSize)) +\n  geom_boxplot(color = \"steelblue\", fill = \"steelblue\",\n               alpha = 0.2, outlier.shape = NA) +\n  geom_jitter(color = \"steelblue\", alpha = 0.4) +\n  facet_grid(. ~ Species) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFor D_busckii, I_crucige, and N_sordida, it is difficult not to conclude that the low p-values indicate the lack of a meaningful sex difference in wing length. For Det_nigro on the other hand, there are very few sampled individuals. Therefore one would most likely need more data to say.\nIn summary, a very powerful way of analyzing data is to perform many analyses at once. To do so, one first has to nest the data. Then the analysis can be performed for every row with the help of the map function. Finally, one unnests the data and interprets the results. Often, an overview of the data will lead to insights that would have been difficult to gain otherwise. In our case, we saw that all but a handful of species exhibit a significant sex difference in wing length. For the few outliers, we saw that one of them lacks sufficient data, and therefore conclusions about this species should be postponed until more data are acquired.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Nested data and multiple analysis</span>"
    ]
  },
  {
    "objectID": "46_Multiple_analysis.html#exercises",
    "href": "46_Multiple_analysis.html#exercises",
    "title": "22  Nested data and multiple analysis",
    "section": "22.4 Exercises",
    "text": "22.4 Exercises\n\nOur analysis on the sex differences in the wing length of fruit flies (fruit_fly_wings.csv) revealed whether there was a significant difference within each species. However, it did not say anything about which sex tends to have a longer wing. Perform this analysis here.\n\nCreate a graph with the difference between mean female and mean male wing lengths along the x-axis, and the species along the y-axis. You can represent the difference of means in each species by a point (geom_point).\nHow many species are there where females have longer wings on average? How many where males do?\nWhich species shows the largest degree of sexual dimorphism?\nList the species where males have, on average, longer wings than females.\n\nThe goal of the original study by Bolstad et al. (2015) was to see the allometric relationship between wing length and the length of the L2 vein that runs across the wing, in different species of fruit flies.\n\nObtain the slope from a linear regression between wing size and L2 vein length (which has been logged in the data) for each species and sex.\nCreate a histogram of the regression slopes. What is the (approximate) distribution of the slopes?\nWhat is the mean and the standard deviation of the distribution of slopes? What is their range? Are the slopes all positive, all negative, or vary between the two? What does this tell you about the relationship between wing size and L2 vein length in general?\nPlot your results, with the regression slopes along the x-axis and the species-sex combination along the y-axis, sorted in the order of the regression slopes. Which species-sex combination has the largest slope? Which one has the smallest?\n\nThe gapminder package contains information on the population size, average life expectancy, and per capita GDP for 142 countries, from 1952 to 2007 (in steps of 5 years). Download and install this package via install.packages(\"gapminder\"), then load it with library(gapminder). If you now type gapminder in the console, you should see a table with six columns. Here we will be focusing on the columns country, continent, year, and pop (the population size of the country in the given year). Now do the following exercises.\n\nLet us see if and when population growth has been exponential in these countries. If the growth of the population size is exponential, then the growth of its logarithm is linear. Therefore, as a first step, take the logarithms of all population sizes in the pop column.\nNest the data by country and continent, and obtain a linear fit of log population size against year for each.\nExtract from this, not the slope or p-value, but a different measure of the model’s quality: the proportion of variance explained (R2). Hint: you can do this with the glance function, which is part of the broom package. It works much in the same way as tidy, but extracts information about model quality instead of model parameters. The R2 value is contained in the column called r.squared.\nMake a plot with R2 along the x-axis and country along the y-axis, showing the R2 values by points. Colour the points based on the continent of the country.\nMake the same plot but first reorder the countries by r.squared.\nWhich handful of countries stand out as having a particularly poor fit with the linear model? Make a plot of just the seven countries with the lowest R2 values. Let year be along the x-axis, the log population size along the y-axis, and the different countries be in separate facets. Bonus exercise: alongside these population curves, display also the predictions from the linear regressions.\nWhich are the countries with the worst model fit? Do they tend to come from a particular continent or region? Given your knowledge of recent history, can you speculate on what the reasons could be for their deviations from exponential growth?\n\nIn this exercise, we explore the goodness-of-fit of linear models between sepal and petal lengths in the iris dataset.\n\nFirst, visualize the data. Create a plot of the iris dataset, using points whose x-coordinate is sepal length and y-coordinate is petal length. Let them be colored by species. Finally, show linear regressions on the points belonging to each species, using geom_smooth.\nObtain the slope of the fit for each species, and the associated p-value. Do this by first nesting the data by species, then fitting a linear model to each of them (with map), and extracting slopes and p-values by applying the tidy function in the broom package. Finally, unnest the data. What are the slopes? And are they significantly different from zero?\n\n\n\n\n\n\nBolstad, Geir H., Jason A. Cassara, Eladio Márquez, Thomas F. Hansen, Kim van der Linde, David Houle, and Christophe Pélabon. 2015. “Complex constraints on allometry revealed by artificial selection on the wing of Drosophila melanogaster.” Proceedings of the National Academy of Sciences 112 (43): 13284–89. https://doi.org/10.1073/pnas.1505357112.",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Nested data and multiple analysis</span>"
    ]
  },
  {
    "objectID": "46_Multiple_analysis.html#footnotes",
    "href": "46_Multiple_analysis.html#footnotes",
    "title": "22  Nested data and multiple analysis",
    "section": "",
    "text": "The reason is that columns of tibbles must always hold elementary pieces of data. The way lists work is that they do not actually hold the information corresponding to their entries. Instead, they only contain references, or pointers, to where the information can be found in memory. Since lists only store these pointers (which can be represented by simple numbers) instead of the tibbles or other data structures inside them, they can be used without problems within tibbles.↩︎",
    "crumbs": [
      "Part III: Advanced statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Nested data and multiple analysis</span>"
    ]
  },
  {
    "objectID": "99_References.html",
    "href": "99_References.html",
    "title": "23  References",
    "section": "",
    "text": "Anscombe, Francis J. 1973. “Graphs in\nStatistical Analysis.” American Statistician 27\n(1): 17–21. https://doi.org/10.1080/00031305.1973.10478966.\n\n\nBarabás, György, Christine Parent, Andrew Kraemer, Frederik Van de\nPerre, and Frederik De Laender. 2022. “The\nevolution of trait variance creates a tension between species diversity\nand functional diversity.” Nature Communications\n13 (2521): 1–10. https://doi.org/10.1038/s41467-022-30090-4.\n\n\nBolstad, Geir H., Jason A. Cassara, Eladio Márquez, Thomas F. Hansen,\nKim van der Linde, David Houle, and Christophe Pélabon. 2015.\n“Complex constraints on allometry revealed by\nartificial selection on the wing of Drosophila\nmelanogaster.” Proceedings of the National Academy of\nSciences 112 (43): 13284–89. https://doi.org/10.1073/pnas.1505357112.\n\n\n“CDC -\nNCHS -\nNational Center for Health\nStatistics — Cdc.gov.” https://www.cdc.gov/nchs/.\n\n\nCedergreen, Nina, and Tom Vindbæk Madsen. 2002. “Nitrogen uptake by the floating macrophyte Lemna\nminor.” New Phytologist 155 (2): 285–92. https://doi.org/10.1046/j.1469-8137.2002.00463.x.\n\n\nColquhoun, David. 2014. “An investigation of\nthe false discovery rate and the misinterpretation of\np-values.” Royal Society Open Science 1 (3):\n140216. https://doi.org/10.1098/rsos.140216.\n\n\n“Create a Model of a Commercial Airplane Where Some Parts Are\nTaken from a Car or a Boat.” 2024. OpenAI. https://chat.openai.com/chat.\n\n\nFauchald, Per, Taejin Park, Hans Tømmervik, Ranga Myneni, and Vera\nHelene Hausner. 2017. “Arctic greening from\nwarming promotes declines in caribou populations.”\nScience Advances 3 (4): e1601365. https://doi.org/10.1126/sciadv.1601365.\n\n\nGalton, Francis. 1886. “Regression Towards\nMediocrity in Hereditary Stature.” Journal of the\nAnthropological Institute of Great Britain and Ireland 15: 246–63.\n\n\nGillespie, John H. 2004. Population Genetics: A Concise Guide.\nBaltimore, MD, USA: Johns Hopkins University Press.\n\n\nGoldberg, Emma E., Joshua R. Kohn, Russell Lande, Kelly A. Robertson,\nStephen A. Smith, and Boris Igić. 2010. “Species Selection\nMaintains Self-Incompatibility.” Science 330\n(6003): 493–95. https://doi.org/10.1126/science.1194513.\n\n\nGu, Qiuping, Vicki L. Burt, Ryne Paulose-Ram, and Charles F. Dillon.\n2008. “Gender Differences in Hypertension\nTreatment, Drug Utilization Patterns, and Blood Pressure Control Among\nUS Adults With Hypertension: Data From the National Health and Nutrition\nExamination Survey 1999–2004.” American Journal of\nHypertension 21 (7): 789–98. https://doi.org/10.1038/ajh.2008.185.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020.\nPalmerpenguins: Palmer Archipelago (Antarctica) Penguin Data.\nhttps://doi.org/10.5281/zenodo.3960218.\n\n\nKendall, Maurice G. 1955. Rank Correlation Methods, 2nd Ed.\nOxford, England: Hafner Publishing Co.\n\n\nKraemer, Andrew C., C. W. Philip, A. M. Rankin, and C. E. Parent. 2018.\n“Trade-Offs Direct the Evolution of Coloration in\nGalápagos Land Snails.” Proceedings\nof the Royal Society B 286: 20182278.\n\n\nKraemer, Andrew C., Yannik E. Roell, Nate F. Shoobs, and Christine E.\nParent. 2022. “Does island ontogeny dictate\nthe accumulation of both species richness and functional\ndiversity?” Global Ecology and Biogeography 31\n(1): 123–37. https://doi.org/10.1111/geb.13420.\n\n\nLack, D., E. C. Van Dyke, R. T. Orr, R. H. Alden, J. D. Ifft, and\nBerkeley. Museum of Vertebrate Zoology University of California. 1945.\nThe Galapagos Finches (Geospizinae) a Study in Variation, by David\nLack. Early Naturalists in the Far West, nos. 19-22. California\nacademy of sciences. https://books.google.se/books?id=gISquwEACAAJ.\n\n\n“Nicolas Cage | Actor,\nProducer, Director — Imdb.com.” https://www.imdb.com/name/nm0000115/.\n\n\nParent, C. E., and B. J. Crespi. 2009. “Ecological Opportunity in\nAdaptive Radiation of Galápagos Endemic Land\nSnails.” American Naturalist 174: 898–905.\n\n\nSmith, Felisa A., S. Kathleen Lyons, S. K. Morgan Ernest, Kate E. Jones,\nDawn M. Kaufman, Tamar Dayan, Pablo A. Marquet, James H. Brown, and John\nP. Haskell. 2003. “Body Mass of Late\nQuaternary Mammals.” Ecology 84 (12): 3403. https://doi.org/10.1890/02-9003.\n\n\nSpearman, C. 1904. “The Proof and Measurement of Association\nBetween Two Things.” The American Journal of Psychology\n15 (1): 72–101. http://www.jstor.org/stable/1412159.\n\n\nSvensson, Linn, Petter Wabakken, Erling Maartmann, Kristoffer Nordli,\nØystein Flagstad, Anna Danielsson, Henrikke Hensel, Katarina Pöchhacker,\nand Mikael Åkesson. 2023. Inventering av varg\nvintern 2022-2023. Bestandsstatus for Store Rovdyr i\nSkandinavia;1-2023. Rovdata (NINA) og SLU Viltskadecenter. https://hdl.handle.net/11250/3068933.\n\n\nWickham, Hadley. 2014. “Tidy Data.”\nJournal of Statistical Software 59 (September): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\nWilkinson, Leland. 2006. The Grammar of\nGraphics. Secaucus, NJ, USA: Springer Science & Business\nMedia.\n\n\nWilson, Edwin B. 1927. “Probable Inference, the Law of Succession,\nand Statistical Inference.” Journal of the American\nStatistical Association 22 (158): 209–12. https://doi.org/10.1080/01621459.1927.10502953.",
    "crumbs": [
      "References",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>References</span>"
    ]
  }
]