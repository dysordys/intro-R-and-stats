# More general linear models; nonlinear regression UNDER CONSTRUCTION {#sec-nls}

**given that all other variables are held constant**. This last part of the interpretation is important to include, as a change in multiple variables would result in a different change in the response variable relative to each coefficient.

### Sequential Sums of Squares {#sec-seq-ss}

The calculations for an ANOVA table are performed automatically in R when we use `lm()`, and we can extract the table from the model object using `anova()` (see @tip-lm-objects).

```{r}
#| tbl-cap: ANOVA table from R
#| tbl-cap-location: top
#| label: tbl-anova-example-R

anova(simpleModel) |> 
  round(4) |> 
  kable() |> 
  kable_styling("striped")
```

By default, R breaks down the model’s sum of squares (SSR) into the individual explanatory variables using sequential (also called conditional) sums of squares. A sequential sum of squares describes how much variation an explanatory variable contributes given that the model already includes other explanatory variables.

The order presented in @tbl-anova-example-R is the order in which variables are added to the model. For example, the second row shows $SS(\text{bill\_depth\_mm} | \text{species})$, meaning bill depth contributes `r anova(simpleModel)[2,2] |> round(4)` additional unique explained variation in the response variable that species has not already explained. The third row shows $SS(\text{flipper\_length\_mm} | \text{species}, \text{bill\_depth\_mm})$, i.e., how much additional unique variation flipper length explains in a model that includes bill depth and species.

Mathematically, the sequential sum of squares is calculated as the difference in either SSE or SSR between two models—one without the added variable and one with it included. Suppose we want to add variable $X^*$ to a model that already has $k$ other variables, then the calculation is:

$$
\begin{aligned}
SS(X^*|X_1, \ldots, X_k) &= SSE_{X_1, \ldots, X_k} - SSE_{X_1, \ldots, X_k, X^*} = \\
&= SSR_{X_1, \ldots, X_k, X^*} - SSR_{X_1, \ldots, X_k}
\end{aligned}
$$ {#eq-seq-ss}

Note that SSR increases with each additional variable added to the model, while SSE always decreases. Variation must always be positive, hence we calculate $SSE_{reduced} - SSE_{full}$ or $SSR_{full} - SSR_{reduced}$.

Sequential sums of squares are affected by the order in which variables are added to the model. Let’s change the order of the explanatory variables when fitting the model:

```{r}
#| tbl-cap: Different order of model variables
#| tbl-cap-location: top
#| label: tbl-anova-example-sex

model <- lm(formula = bill_length_mm ~ sex + ., data = modelData)

anova(model) |> 
  round(4) |> 
  kable() |> 
  kable_styling("striped")
```

In @tbl-anova-example-sex we see that $SS(\text{sex}) = `r anova(model)[1,2] |> round(4)`$, which is significantly higher than $SS(\text{sex}|\text{species}, \text{bill\_depth\_mm}, \text{flipper\_length\_mm}, \text{body\_mass\_g}) = `r anova(simpleModel)[5,2] |> round(4)`$ from @tbl-anova-example-R. The variable sex contributes a lot of variation when it is alone in a model, but when added to a model that already includes other variables, it does not contribute as much unique information. This means that the variation explained by the variable also appears to be present in the other variables. We will revisit this observation in a later chapter.

One thing that remains the same in both tables is SSE. We have included the same variables in both models, which means SST, SSR, and SSE are overall the same. The sum of all sequential sums of squares should still equal SSR regardless of the order of variables, and due to the additive property of variation, SST and SSE have not changed either.

### Partial F-test for groups of parameters

Sometimes we are interested in examining parts of the model, e.g. a group of slope parameters, whether several variables together contribute explained variation to the model. Instead of examining all slope parameters, we now test a subset which is reflected in the hypothesis:

\begin{align*}
H_0&: \beta_1 = \beta_2 = \beta_3 = \cdots = \beta_s = 0\\
H_a&: \text{At least one of } \beta_j \text{ in } H_0 \text{ is different from } 0
\end{align*}

where $s$ is the number of parameters being tested.

The test statistic for a partial F-test requires a full model (denoted $_F$) and a reduced model (denoted $_R$). The full model includes all variables, while the reduced model assumes $H_0$ is true and excludes the variables being tested. We can use either SSR or SSE to calculate how much explained variation is lost between the two models, following the same principle as @eq-seq-ss.

$$
F_{test} = \frac{(SSR_F - SSR_R) / s}{SSE_F / (n - (k+1))} = \frac{(SSE_R - SSE_F) / s}{SSE_F / (n - (k+1))}
$$ {#eq-partial-f}

The test statistic still follows an F-distribution with $s$ and $n - (k+1)$ degrees of freedom.

#### Shortcut for Partial F-tests

Using @eq-seq-ss, @eq-partial-f can be reformulated in a third way that simplifies our analysis process. We can rewrite the difference in explained variation between the full and reduced models as a sequential sum of squares. For example, we may want to investigate whether the variable species is associated with the response variable. Since that variable is transformed into two indicator variables, the hypotheses involve two slope parameters.

\begin{align*}
H_0&: \beta_{Chinstrap} = \beta_{Gentoo} = 0\\
H_a&: \text{At least one of } \beta_j \text{ in } H_0 \text{ is different from } 0
\end{align*}

The reduced model is created assuming $H_0$ is true, i.e., $\beta_{Chinstrap} = \beta_{Gentoo} = 0$, and the explained variation for the two models is denoted:

$$
\begin{aligned}
  SSR_{R} &= SSR_{bill\_depth\_mm, flipper\_length\_mm, body\_mass\_g, sex} \\
  SSR_{F} &= SSR_{bill\_depth\_mm, flipper\_length\_mm, body\_mass\_g, sex, species}
\end{aligned}
$$

We can rewrite the numerator in @eq-partial-f as:

$$
SS(species|bill\_depth\_mm, flipper\_length\_mm, body\_mass\_g, sex)
$$

In the ANOVA tables presented earlier, we can obtain this sum of squares directly if species is added as the last variable in the model.

```{r}
#| tbl-cap: ANOVA table from a model where species is added last
#| tbl-cap-location: top
#| label: tbl-anova-example-species

model <- lm(bill_length_mm ~ bill_depth_mm + flipper_length_mm + body_mass_g + sex + species, data = modelData)

anova(model) |> 
  round(4) |> 
  kable() |> 
  kable_styling("striped")
```

An ANOVA table with sequential sums of squares calculates a partial F-test for each variable (and its parameter(s)), examining whether the variable contributes a significant increase in explained variation to a model that already includes the variables above. @tbl-anova-example-species now calculates the partial F-test for species ($F_{test} = `r anova(model)[5, 4] |> round(4)`$), which we were interested in, and we can directly interpret the p-value for the test ($p-value < 0.001$) as indicating that at least one of the slope parameters is significantly different from 0.

If we conduct a partial F-test for **multiple variables**, we cannot use the p-values shown in the table, since the hypotheses involve more slope parameters/variables than the sequential sums of squares represent. Suppose we want to investigate whether species and sex together contribute to the model. The hypothesis test would then involve:

$$
\begin{aligned}
H_0&: \beta_{sexMale} = \beta_{Chinstrap} = \beta_{Gentoo} = 0\\
H_a&: \text{At least one of } \beta_j \text{ in } H_0 \text{ is different from } 0
\end{aligned}
$$

The sequential sum of squares we want to use is denoted $SS(species, sex|bill\_depth\_mm, flipper\_length\_mm, body\_mass\_g)$, and we can calculate this value by summing the SS for the two variables from @tbl-anova-example-species:

$$
\begin{aligned}
SS(species, sex|bill\_depth\_mm, flipper\_length\_mm, body\_mass\_g) = \\
SS(species|bill\_depth\_mm, flipper\_length\_mm, body\_mass\_g, sex) + \\
SS(sex|bill\_depth\_mm, flipper\_length\_mm, body\_mass\_g)
\end{aligned}
$$

Alternatively, we can fit two models in R—the full and reduced—and read off SSE or sum SSR from the respective ANOVA tables.

::: {.callout-important}
If a parameter is not considered significant, it is a justification for removing the variable. We fit a reduced model and begin a new analysis. If a variable is removed, the other parameter estimates will change, and interpretations and inference need to be updated.
:::

Because SSR always increases as more variables are added to a model, we need to adjust the metric to compare models of different sizes. When we want to compare models of different sizes, we should instead look at the *adjusted coefficient of determination* ($R^2_{a}$) to see which model is best. An improved $R^2_{a}$ means the model has removed unnecessary complexity.

$$
R^2_a = 1 - \frac{SSE / (n - (k+1))}{SST / (n - 1)}
$$

## A non-parametric method: Theil--Sen regression {#sec-theilsen}

The method of linear regression discussed so far is called *least-squares regression*, due to the fact that it relies on minimizing the sum of squared residuals $\sum_i \epsilon_i^2$. A non-parametric alternative to this method is *Theil--Sen regression*. This is generally much more robust against outliers than the least-squares method. It also does not require that the residuals are normally distributed. There are also two disadvantages, the main one being that it can only be used for simple regression (one single predictor). It can also be slower to compute, but with today's computers, this is rarely an issue.

The way Theil--Sen regression works is simple:

* A line is fit between all possible pairs of points, and their slopes are recorded.
* The overall regression slope *m* is the median of all these pairwise slopes.
* The intercept *b* is the median of all *y*~*i*~ -- *m* *x*~*i*~ values, where *x*~*i*~ is the *i*th measurement of the predictor and *y*~*i*~ the corresponding response.

To use the Theil--Sen regression, one has to install the package `mblm` ("median-based linear models"):

```{r}
#| echo: false
library(mblm)
```

```{r}
#| eval: false
install.packages("mblm")

library(mblm)
```

The function performing the regression is itself called `mblm`. A note of caution: its data argument, for some reason, is not called `data` but `dataframe`. Let us apply it to set 3 in the Anscombe dataset (the one with the single strong outlier):

```{r}
#| warning: false
mblm(y ~ x, dataframe = filter(ansLong, set == "3")) |> summary()
```

As seen, the predicted intercept and slope are no longer 3 and 0.5, but 4 and 0.35 instead. Also, in the regression table above, the median absolute deviation (MAD) of the parameters is reported instead of their standard error, as the MAD is a non-parametric measure of spread.^[The median absolute deviation (MAD) over a set of data points $x_i$ is defined as $\text{MAD} = \text{median}(|x_i - \text{median}(x)|)$, where $\text{median}(x)$ is the median of the data.] The p-values (`Pr(>|V|)`) in the table are like those in the regression tables from applying `summary` to `lm`---however, since Theil--Sen regression is a non-parametric method, these p-values are based on a Wilcoxon rank sum test instead of a t-test.

We can visualize the Theil--Sen regression alongside the least-squares regression, for a better comparison of what they do:

```{r}
leastSquaresFit <- lm(y ~ x, data = filter(ansLong, set == "3"))
TheilSenFit <- mblm(y ~ x, dataframe = filter(ansLong, set == "3"))

ansLong |>
  filter(set == "3") |>
  mutate(`least squares` = predict(leastSquaresFit),
         `Theil-Sen` = predict(TheilSenFit)) |>
  pivot_longer(cols = c("least squares", "Theil-Sen"),
               names_to = "type", values_to = "prediction") |>
  ggplot() +
  geom_point(aes(x = x, y = y), color = "steelblue") +
  geom_line(aes(x = x, y = prediction), color = "goldenrod") +
  facet_grid(. ~ type) +
  theme_bw()
```

The Theil--Sen regression correctly recognizes the outlier for what it is, and remains unaffected by it.

:::{.callout-note}
In the code above, we relied on a function called `predict`. This simply returns the model-predicted results for each value of the predictor:

```{r}
predict(leastSquaresFit)
predict(TheilSenFit)
```
:::

## Combining categorical and continuous variables in linear models {#sec-ancova}

So far we have used at most two predictors when dealing with linear models (`lm`). This was in @sec-SRH, where we looked the effects of two categorical variables, as well as their interaction. @sec-linreg introduced the idea of using a continuous, instead of a categorical, predictor. But we have not been combining these.

In fact, one can build arbitrarily complicated linear models from an arbitrary combination of continuous and categorical variables, and their interactions. Let us consider the built-in `CO2` dataset as an example, which was already used before in an exercise (@sec-exercises-anova-two-way). Briefly, the data contain measurements from an experiment on the cold tolerance of the grass species [*Echinochloa crus-galli*](https://en.wikipedia.org/wiki/Echinochloa_crus-galli). The dataset has five columns: `Plant` (a unique identifier for each plant individual), `Type` (either `Quebec` or `Mississippi` depending on the origin of the plant), `Treatment` (whether the plant individual was `chilled` or `nonchilled` for the experiment), `conc` (ambient carbon dioxide concentration), and `uptake` (carbon dioxide uptake rate by the plant).

```{r}
#| message: false
library(tidyverse)
as_tibble(CO2)
```

We can plot the observed distributions of CO~2~ uptake rates for each type and treatment:

```{r}
as_tibble(CO2) |>
  ggplot(aes(x = 0, y = uptake)) +
  geom_boxplot(color = "steelblue", fill = "steelblue",
               alpha = 0.2, outlier.shape = NA) +
  geom_jitter(color = "steelblue", alpha = 0.5, width = 0.05) +
  facet_grid(Type ~ Treatment) +
  labs(y = "uptake rate") +
  theme_bw() +
  theme(axis.title.x = element_blank(), # The x-axis is meaningless here,
        axis.ticks.x = element_blank(), # so remove title, tick marks,
        axis.text.x = element_blank())  # and labels from it
```

This, however, is only part of the story, as becomes obvious if we also plot the ambient CO~2~ concentrations (`conc`) along the x-axis:

```{r}
as_tibble(CO2) |>
  ggplot(aes(x = conc, y = uptake)) +
  geom_point(color = "steelblue", alpha = 0.8) +
  facet_grid(Type ~ Treatment) +
  labs(x = "concentration", y = "uptake rate") +
  theme_bw()
```

We see that there is also a clear, saturating relationship between CO~2~ concentration and uptake rates that is definitely not linear. This does not mean that a linear model is useless for analyzing these data: the trend of whether the data increase or decrease can still be captured (although it is not recommended to use the model for numerical prediction purposes). One model that may come to mind is as follows:

```{r}
lm(uptake ~ conc + Type * Treatment, data = CO2) |> anova()
```

In other words, the uptake rates are modeled via a combination of the effect of concentration (a continuous variable) plus the interaction of type and treatment (two categorical variables). Recall that `Type * Treatment` is shorthand for `Type + Treatment + Type:Treatment`, the sum of the main effects and the interaction between them. Mathematically, the equation for the model reads:
$$
\begin{split}
(\text{uptake})_i &
= \beta_0
+ \beta_1 \cdot (\text{conc})_i
+ \beta_2 \cdot (\text{Type is Mississippi})_i \\ &
+ \beta_3 \cdot (\text{Treatment is chilled})_i \\ &
+ \beta_4 \cdot (\text{Type is Mississippi})_i \cdot(\text{Treatment is chilled})_i
+ \varepsilon_i
\end{split}
$$ {#eq-linreg-multiway}
where $(\text{conc})_i$ is a continuous predictor and not an indicator variable---that is, it takes on the actual value of the CO~2~ concentration in observation $i$. By contrast, $(\text{Type is Mississippi})_i$ and $(\text{Treatment is chilled})_i$ are indicator variables that take on the value 1 if data point $i$ belongs in their category and 0 otherwise.

The rationale for having chosen the model `uptake ~ conc + Type * Treatment` is that the box plots above reveal a potential interaction between the two factors `Type` and `Treatment` (the effect of changing `Treatment` from chilled to nonchilled depends on whether the `Type` was Quebec or Mississippi), and on top of this, we also want to capture the positive dependence on CO~2~ concentration. The ANOVA table above concurs: each of these categories come out with low p-values, indicating that what we see is unlikely to be due to just chance. To make sure that the assumptions on which this interpretation rests are held, we look at the diagnostic plots:

```{r}
#| warning: false
#| fig-height: 6
library(ggfortify)

lm(uptake ~ conc + Type * Treatment, data = CO2) |>
  autoplot(smooth.colour = NA, colour = "steelblue", alpha = 0.7) +
  theme_bw()
```

The Q-Q plot is not good: in the lower quantiles, the realized residuals are consistently larger in magnitude than the theoretical expectation based on the assumption of normality. This, of course, is a consequence of the data depending on concentrations in a manifestly nonlinear way. Apart from the Q-Q plot however, the diagnostics look surprisingly good. We will come back to the point about nonlinearity in @sec-nonlin-regression.

It is also informative to apply the function `summary` on the model fit in addition to `anova`, to obtain the regression slopes and intercept (the $\beta$ parameters of @eq-linreg-multiway):

```{r}
lm(uptake ~ conc + Type * Treatment, data = CO2) |> summary()
```

Regardless of how good this model looks, one can argue based on the plot of the data that there could also be an interaction between `conc` and the other two factors. After all, the saturation levels of the uptake rate are always higher in Quebec than in Mississippi, and the effect of chilling also depends on `Type`. A model which accounts for all these effects and their interactions is `uptake ~ conc * Type * Treatment`. Mathematically:
$$
\begin{split}
(\text{uptake})_i &
= \beta_0
+ \beta_1 \cdot (\text{conc})_i
+ \beta_2 \cdot (\text{Type is Mississippi})_i \\ &
+ \beta_3 \cdot (\text{Treatment is chilled})_i \\ &
+ \beta_4 \cdot (\text{conc})_i \cdot (\text{Type is Mississippi})_i \\ &
+ \beta_5 \cdot (\text{conc})_i \cdot (\text{Treatment is chilled})_i \\ &
+ \beta_6 \cdot (\text{Type is Mississippi})_i \cdot (\text{Treatment is chilled})_i \\ &
+ \beta_7 \cdot (\text{conc})_i \cdot (\text{Type is Mississippi})_i
\cdot (\text{Treatment is chilled})_i
+ \varepsilon_i
\end{split}
$$ {#eq-linreg-multiway-2}

(The $\beta_7$ term is multiplied by a three-way interaction of concentration, type, and treatment.) Fitting the model and creating diagnostic plots:

```{r}
#| warning: false
#| fig-height: 6
lm(uptake ~ conc * Type * Treatment, data = CO2) |> anova()
lm(uptake ~ conc * Type * Treatment, data = CO2) |> summary()
lm(uptake ~ conc * Type * Treatment, data = CO2) |>
  autoplot(smooth.colour = NA, colour = "steelblue", alpha = 0.7) +
  theme_bw()
```

This confirms what we saw on the plot of the data: that the basic shape of the relationship between concentration and uptake is unaffected by either `Type` or `Treatment` (i.e., the term `conc:Type:Treatment` in the ANOVA table has a high associated p-value). It also illustrates the general point that there are very often multiple candidate models, and choosing between them is a question of judgment, trial-and-error, and successively improving the model structure based on results from earlier modeling attempts.


## Nonlinear regression {#sec-nonlin-regression}

The relationship between CO~2~ concentration and uptake rates are definitely not linear, regardless of treatment or type. So the question arises: can one fit a *nonlinear* function to these data? As an example, let us focus on just Quebec and the nonchilled treatment, to better illustrate the ideas behind nonlinear regression. Here are the data:

```{r}
as_tibble(CO2) |>
  filter(Type == "Quebec", Treatment == "nonchilled") |>
  ggplot(aes(x = conc, y = uptake)) +
  geom_point(color = "steelblue", alpha = 0.8) +
  labs(x = "concentration", y = "uptake rate") +
  theme_bw()
```

If the function we wish to fit is not linear, we have to specify its shape. One commonly used shape for describing the above saturating pattern is the *Michaelis--Menten curve*. This is given by the following equation:
$$ \rho = \frac{V c}{K + c} $$
Here $\rho$ is the uptake rate, $c$ is the concentration, and $V$ and $K$ are two parameters which can modify the shape of the function. The figure below illustrates what curves one can get by varying these parameters:

```{r}
#| message: false
expand_grid(V = c(10, 20, 30), # This function creates a tibble with all
            K = c(1, 5, 10),   # possible combinations of the inputs
            concentration = seq(0, 60, l = 201)) |>
  group_by(V, K) |>
  mutate(uptake = V * concentration / (K + concentration)) |>
  ungroup() |>
  ggplot(aes(x = concentration, y = uptake)) +
  geom_line(color = "steelblue") +
  facet_grid(V ~ K, labeller = label_both) +
  theme_bw()
```

The task is to find the values of $V$ and $K$ that provide the best fit to the data. Like in the case of linear regression, this can be done via the least-squares criterion: the best fit is provided by the curve which minimizes the sum of the squared deviations of the observed points from it. Unlike with linear regression however, this curve can be very difficult to find. In fact, there is no known general procedure that would be able to minimize the sum of squares under all circumstances. What algorithms *can* do is to find the best fit, given some initial guesses for the parameters that are at least not violently off of the true values. Just how close the guess needs to be is context-dependent, and highlights an important problem: nonlinear regression can be as much an art as it is a science. For the types of curves we will be fitting though, the more subtle problems will never come up, and a "good enough" initial guess can vary within a relatively wide range.

So, how can one guess the values of $V$ and $K$? To do this, one has to have an understanding of how the parameters influence the curves. For $V$, this interpretation is not difficult to infer. Notice that if concentrations are very, very large, then in the denominator of the formula $\rho = V c / (K + c)$, we might as well say that $K + c$ is approximately equal to $c$ (if $c$ is a million and $K$ is one, then one is justified in treating the sum as being about one million still). This means that for large $c$, the formula reduces to $\rho \approx V c / c = V$. In other words $V$ is the *saturation uptake rate:* the maximum value of the uptake. This, incidentally, is clearly visible in the plots above: when $V$ is 10 (top row), the curves always tend towards 10 for large concentrations; when $V$ is 20, they tend towards 20 (middle row), and when $V$ is 30, they tend towards 30.

The interpretation of $K$ is slightly less straightforward, but still simple. To see what it means, let us ask what the uptake rate would be, were the concentration's value equal to $K$. In that case, we get $\rho = V K / (K + K)$ (we simply substituted $c = K$ into the formula), or $\rho = VK / (2K) = V/2$. That is, $K$ is the concentration at which the uptake rate reaches half its maximum.

Looking at the data again, both these parameters can be roughly estimated:

```{r}
as_tibble(CO2) |>
  filter(Type == "Quebec", Treatment == "nonchilled") |>
  ggplot(aes(x = conc, y = uptake)) +
  geom_point(color = "steelblue", alpha = 0.8) +
  geom_hline(yintercept = 43, linetype = "dashed", color = "steelblue") +
  annotate(geom = "segment", x = 0, y = 43/2, xend = 125, yend = 43/2,
           linetype = "dashed", color = "steelblue") +
  annotate(geom = "segment", x = 125, y = 43/2, xend = 125, yend = 0,
           linetype = "dashed", color = "steelblue") +
  scale_x_continuous(name = "concentration",
                     limits = c(0, NA), expand = c(0, 0)) +
  scale_y_continuous(name = "uptake rate",
                     limits = c(0, NA), expand = c(0, 0)) +
  theme_bw()
```

So guessing that $V$ is about 43 and $K$ is about 125 seems to be close to the mark.

To actually perform the nonlinear regression, one can use the `nls` function ("Nonlinear Least Squares"). It begins much like `lm`, taking a formula and a data frame. However, the formula is no longer a shorthand notation for a linear model, and therefore has to be entered literally. Additionally, there is another argument to `nls` called `start`; this is where the starting values have to be specified. The `start` argument has to be in the form of a *list*. Lists are an important data structure, worth a little interlude to explain how they work.


### Interlude: lists {#sec-lists}

Lists are like vectors except they can hold arbitrary data in their entries. So unlike vectors which are composed of either all numbers or all character strings or all logical values, lists may have a combination of these. Furthermore, list entries are not restricted to elementary types: vectors, or even data frames may also be list entries. To define a list, all one needs to do is type `list`, and then give a sequence of named entries. For example, the following creates a list with three entries: `a` will be equal to 3, `b` to the string `"Hello!"`, and `myTable` to a small tibble.

```{r}
list(a = 3, b = "Hello!", myTable = tibble(x = c(1, 2), y = c(3, 4)))
```

One can refer to the entries of the list either with the `$` notation, or using double brackets. Assigning the above list to a variable called `myList` first:

```{r}
myList <- list(
  a = 3,
  b = "Hello!",
  myTable = tibble(x = c(1, 2), y = c(3, 4))
)
```

We can now access the entries of `myList` either as

```{r}
myList$myTable # Access the entry called myTable in the list
```

Or as

```{r}
myList[[3]] # Access the third entry (myTable) in the list
```

The `$` notation is the same as when one accesses columns of data frames. This is not a coincidence: internally, data frames are represented as lists of columns, with each column being a vector. It follows that the double-bracket notation can also be used in conjunction with data frames: to access the first column of `CO2`, we can not only write `CO2$Plant`, but also `CO2[[1]]`.

A list seems to be just a more flexible version of a vector---so why would we want to use vectors instead of lists in the first place? The answer has to do with efficiency: the price to pay for the increased flexibility offered by lists is that they are slower to do operations on. While this would not be a problem for the applications found in this book, it can become important when dealing with large datasets or heavy numerical computations. As a corollary, R has many useful functions which work on vectors but do not work by default on lists. To mention just the simplest examples: `mean`, `median`, `sd`, and `sum` will throw an error when applied to lists. That is,

```{r}
sum(c(1, 2, 3))
```

returns the expected 6, because it was applied to a vector. But the same expression results in an error when the vector is replaced by a list:

```{r}
#| error: true
sum(list(1, 2, 3))
```


### Back to nonlinear regression

With this brief introduction to lists, we now understand what it means that the `start` argument to `nls` must be a list, with the entries named after the parameters to be fitted. Using the previously-guessed values of $V$ and $K$ being around 43 and 125, respectively, means we can use `start = list(V = 43, K = 125)`. We can now look at the `nls` function and use it to produce a fit of the Michaelis--Menten curve to our data:

```{r}
nonlinearFit <- as_tibble(CO2) |>
  filter(Type == "Quebec", Treatment == "nonchilled") |>
  nls(uptake ~ V*conc/(K + conc), data = _, start = list(V = 43, K = 125))

print(nonlinearFit)
```

Observe that in the formula, we use the column name `conc` when we want to use the data inside that column, but use made-up names (in this case, `V` and `K`) for the unknown parameters we are trying to obtain. Their starting values were filled in from our earlier visual estimation. From these starting values, the best fitting solution is found. We see that their values are 51.36 for $V$ and 136.32 for $K$.

The result of `nls` can be used inside `summary` to get more information on the fitted parameters (note: the `anova` function is not applicable to nonlinear regression). Doing so results in the following regression table:

```{r}
summary(nonlinearFit)
```

As in the case of linear regression, the statistical analysis will only be reliable if the assumptions of the independence, normality, and homoscedasticity of the residuals are maintained.

To conclude this section, it can be useful to plot the data together with the fitted nonlinear curve, to make sure visually that the fit is reasonable. There are several possible ways of doing this; here we discuss two of them. First, one can rely on the `predict` function (@sec-theilsen) that will tell us, for each value of the predictor, what the model-predicted values are. In the same way as for models generated by `lm` or `mblm`,

```{r}
predict(nonlinearFit)
```

returns a vector for each `conc` in the original data, documenting what the model thinks the corresponding uptake rate ought to be. They can then be compared with the data:

```{r}
as_tibble(CO2) |>
  filter(Type == "Quebec", Treatment == "nonchilled") |>
  mutate(pred = predict(nonlinearFit)) |>
  ggplot() +
  geom_point(aes(x = conc, y = uptake), color = "steelblue", alpha = 0.8) +
  geom_line(aes(x = conc, y = pred), linetype = "dashed", alpha = 0.5) +
  labs(x = "concentration", y = "uptake rate") +
  theme_bw()
```

In the above plot, the data points and the predictions each had their own set of aeshetics. For this reason, the aesthetic mappings were not defined separately, but locally inside each `geom_`. This is perfectly legal, and can help whenever different geometries take different aesthetics from the data. Second, notice that the prediction was drawn with a dashed, semi-transparent line. This is intentional, to make it distinct from data. It signals that the curve does not correspond to data we are plotting, but to a model's predictions.

The second method can be useful if the data are sufficiently scarce that the fitted line looks "rugged", a bit too piecewise (this can be seen in the above example as well if one looks carefully). In that case, it is possible to draw the curve of any function using `geom_function`. We can use the fitted parameters in drawing it, and it will not suffer from being too piecewise:

```{r}
V_fitted <- coef(nonlinearFit)["V"] # Get fitted values of V and K
K_fitted <- coef(nonlinearFit)["K"] # from the vector of coefficients

as_tibble(CO2) |>
  filter(Type == "Quebec", Treatment == "nonchilled") |>
  ggplot() +
  geom_point(aes(x = conc, y = uptake), color = "steelblue", alpha = 0.8) +
  geom_function(fun = function(x) V_fitted * x / (K_fitted + x),
                linetype = "dashed", alpha = 0.5) +
  labs(x = "concentration", y = "uptake rate") +
  theme_bw()
```

The result is much the same as before, although carefully looking at the dashed line shows that the second curve is smoother than the first. In this case, this does not matter much, but it could be aesthetically more relevant in others.


## Exercises

#### Exponential growth

Let $N(t)$ be the population abundance of some organism at time $t$. An exponentially growing population increases according to
$$ N(t) = N_0 \mathrm{e}^{rt} $$
where $N_0$ is the initial population size at time $t=0$, and $r$ is the exponential rate of increase.

1. Download the data file [`pop_growth_1.csv`](https://raw.githubusercontent.com/dysordys/intro-R-and-stats/main/data/pop_growth_1.zip) and load it in R.
2. Use `nls()` to fit the above exponential growth model to this dataset. Do not treat $N_0$ as a free parameter, but instead use the actual population size at time $t=0$. This leaves $r$ as the only parameter to be fitted. Do so, using an appropriate starting value.
4. Assume that the data describes a population of water lilies, and that a single 'individual' weighs 1 gram. If the population would continue to grow unrestricted, what would be its biomass after nine months (about 280 days)? What object would have a comparable weight to this population, and what does that tell us about unrestricted population growth in general?


#### Nitrogen uptake

@CedergreenMadsen2002 reported data from an experiment on nitrogen uptake by the duckweed *Lemna minor*, where the predictor variable is the initial substrate concentration and the response variable is the uptake rate. In this type of experiment, it is anticipated that the uptake will increase as the concentration increases, approaching a horizontal asymptote. The data are available in [`uptake.csv`](https://raw.githubusercontent.com/dysordys/intro-R-and-stats/main/data/uptake.zip).

1. Create a plot of the data, with the nitrogen concentrations along the x-axis and the corresponding uptake rates along the y-axis.
2. Fit a Michaelis-Menten model (describing saturating dynamics) to the data. This model is given by
$$ \rho = \frac{V c}{K + c} $$
where $V$ and $K$ are two constants, $c$ is the concentration, and $\rho$ the uptake rate. Make initial guesses for the two parameters $V$ and $K$ based on the graph, and perform the nonlinear regression.
2. Given your nonlinear regression results, what is the maximum possible nitrogen uptake rate of *L. minor*? 


#### Logistic growth

The simplest model illustrating population regulation and regulated growth is the *logistic model*, defined by the differential equation
$$ \frac{\mathrm{d} N(t)}{\mathrm{d} t} = rN(t) \left( 1 - \frac{N(t)}{K} \right) $$
Here $N(t)$ is the population abundance at time $t$, $r$ is the exponential growth rate of the population when rare, and $K$ is the maximum abundance it can sustainably achieve (the "carrying capacity"). It should be obvious that when $N(t) = K$, the derivative vanishes, signalling that the population size no longer changes.

The above differential equation is one of the few which can be solved explicitly. Its solution reads
$$ N(t) = N_0 \frac{\mathrm{e}^{rt}}{1-(1-\mathrm{e}^{rt})(N_0/K)} $$
where $N_0$ is the initial population size at time $t=0$. Let us fit this model to some population growth data.

1. Download the data file [`pop_growth_2.csv`](https://raw.githubusercontent.com/dysordys/intro-R-and-stats/main/data/pop_growth_2.zip), load it in R, and plot the population abundances against time.
2. Fit the above model to the data using the `nls()` function, with appropriate guesses for the starting values of $r$ and $K$.
3. Plot the data and the model prediction together. What are the estimated values of $r$ and $K$?
