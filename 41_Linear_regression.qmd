# Simple linear regression {#sec-linreg}

<!-- Creates a shortcommand for sums -->
\newcommand{\Sum}[1]{\sum_{i=1}^#1}

The first step in a regression analysis is to explore the dataset. Regardless of whether we conducted an observational or experimental study, we have likely selected a response variable beforehand that we want to describe or predict using other variables. The goal of exploring the data is to gain insights that can help us in later steps; which variables are relevant to include in a model, what each relationship looks like, and whether there are any issues with the collected data that need to be corrected.

Some key questions we need to answer are:

- What type and scale do the variables have?
- Around which values do the variables range? What are their minimum and maximum values, their measures of central tendency and spread?
- How are the variables distributed? What type of distribution seems to describe the variable; normal, uniform, skewed?
- Are there any outliers present?

Once we’ve answered these questions, it becomes much easier to build a correct model structure and evaluate any estimated models. 

We can investigate these points using simple functions (e.g., `mean()`, `min()`, `summary(variable)`), but we can also gain relevant information through individual and pairwise visualizations of the variables.

## Example data {#sec-example-data}
As an introductory example, let us load the Galápagos land snail data from Floreana Island (@sec-snail). We look only at individuals belonging to the species *Naesiotus nux*, and plot the shell shape measurement against the shell size measurement for each of them:

```{r}
#| message: false
#| fig-cap: Scatter plot showing the relationship between shell shape and size
#| fig-height: 3
#| fig-width: 5
#| label: fig-shape-size

snails <- read_csv("island-FL.csv")

nux <- snails |>
  filter(species == "nux")

nux |> 
  ggplot(aes(x = size, y = shape)) +
  geom_point(color = "steelblue") +
  theme_bw()
```

Both of these variables are continuous quantitative, which means we can create a scatter plot where each observation is represented by a point. The explanatory variable is placed on the x-axis and the response variable on the y-axis. The point cloud in the scatter plot can give us information about the relationship between the two variables. We focus on four main points:

1. Is the relationship linear?
2. Is the relationship positive or negative?
3. Is the relationship strong or weak?
4. Are there any outliers?

:::{.callout-important}
Defining the proper variable type and scale is very important in this exploratory step. Using the wrong type of visualization can easily misconstrue the relationship and send us on a wild goose chase.

We cannot use scatter plots to visualize the relationship between qualitative explanatory variables and a continuous response variable. Instead, we need visualizations that account for the qualitative scale, usually ordinal or nominal. 

There are several ways to visualize the distribution of the response variable across levels of the explanatory variable, such as grouped histograms or box plots, but one type of visualization that shows distribution details is a *violin plot*. A violin plot is a mirrored density plot, where areas with many observations have a larger area under the curve. Using `ggplot2`, we can create such a plot with `geom_violin()`.
:::

@fig-shape-size shows a very sparse relationship that is a bit difficult to interpret. One could say that the relationship appears mostly linear, as an increase in shell size leads to a unidirectional change (increase/positive relationship) in shell shape. However, the relationship is not very strong, as there is a lot of variation in the shape values for each size value. There are also some outliers, which we can see as points that are far from the main point cloud.

We can calculate *Pearson’s correlation coefficient* ($r$) so we don’t have to rely on a subjective interpretation of the strength.^[Or other measures of association strength, e.g., Kendall [@kendall1955rank] or Spearman [@spearman1904].] This coefficient measures the strength of the **linear relationship** between two quantitative variables and is appropriate in this case. A value near 0 indicates no or weak relationship, while values near -1 or +1 indicate a strong negative or positive relationship, respectively.

$$
r = `r cor(nux$shape, nux$size) |> round(3)`
$$

This is an additional indication that the correlation between the two variables is relatively weak, mostly due to the large variation in shape values for each size value. The correlation coefficient is not very high, but it is positive, indicating that larger shells tend to be more elongated.

::: {.callout-important}
If the scatter plot shows a non-linear and non-monotonic (not unidirectional) relationship, the Pearson correlation coefficient will not accurately describe the strength of the relationship. It is easy to rely solely on the correlation coefficient since it is simple to calculate for many variable pairs, but it can often miss relevant information. A visualization enables the identification of complex relationships that often require different modeling approaches.
:::

After gathering information from visualizations and descriptive statistics, the next step in the process is to build the structure of the model. 

$$
\text{shape}_i = \beta_0 + \beta_1 \cdot \text{size}_i + \varepsilon_i
$$ {#eq-snail-simple}

Regardless of whether the model includes one or several explanatory variables, we must always keep in mind the five assumptions presented in @sec-model-assumptions, especially the assumption of linearity. If we have discovered non-linear relationships in the exploratory step, we need to model a more complex relationship than what is shown in @eq-snail-simple.

## Model estimation {#sec-model-estimation}
@eq-snail-simple shows the "true" model based on all observed values in the population, but (nearly) all studies are based on some form of sample. Even a population census during a specific period can be considered a sample in time if the model is intended to be used after the study period ends.

We can denote the estimated model with its *estimated parameters* as:

$$
\hat{Y}_i = b_0 + b_1 \cdot X_{1i}
$$ {#eq-snail-simple-est}

where:
\begin{align*}
  \hat{Y}_i &= \text{estimated value of the response variable for observation } i\\
  b_0 &= \text{estimate of the intercept}\\
  b_1 &= \text{estimate of the slope parameter}
\end{align*}

::: {.callout-note}
Some literature uses $\hat{\beta}$ to denote estimated parameters.
:::

We want this model to best describe the relationship so we first need to define what we mean by "best". If we project two different estimated simple linear models onto a scatter plot of the two variables (@fig-reg-errors), each line will not hit all points exactly, each observation will lie at a certain distance from the regression line. This distance is the observation’s *residual*, denoted by $e_i$, an estimate of $\varepsilon_i$.

```{r}
#| fig-cap: Visualization of regression model residuals
#| fig-height: 3
#| fig-width: 5 
#| label: fig-reg-errors
#| echo: FALSE

tibble(
  b0 = c(-0.126, 0.148),
  b1 = c(0.0038, -0.0051)
  ) |>
  mutate(
    case = 
      as_factor(
        str_c("b[0]==", b0, "~~~b[1]==", b1)
        )
    ) |>
  crossing(
    nux |> 
      select(size, shape)
  ) |> 
  mutate(
    shapehat = b0 + b1 * size,
    res = shape - shapehat
  ) |>
  ggplot() +
  geom_point(aes(x = size, y = shape), color = "steelblue") +
  geom_line(aes(x = size, y = shapehat), color = "goldenrod") +
  geom_segment(aes(x = size, y = shapehat,
                   xend = size, yend = shapehat + res),
               alpha = 0.3) +
  facet_grid(. ~ case, labeller = label_parsed) +
  theme_bw()
```

The vertical lines are the estimated residuals, measuring the deviation from the line and the actual data points. It is intuitively obvious that the first line (with intercept $b_0 = -0.126$ and slope $b_1 = 0.0038$) is better than the second one (intercept $b_0 = 0.148$ and slope $b_1 = -0.0051$). This is because the size of the residuals tend to be larger for the right panel. The model on the left is estimated using *Ordinary Least Squares* (OLS), where the goal is to minimize the model’s total error, whereas the one on the right is just a random line drawn on the graph.  

Mathematically, we calculate $e_i = Y_i - \hat{Y}_i$, where $Y_i$ is the observed value (the point) and $\hat{Y}_i$ is the model’s estimated value (the line). OLS estimates all model parameters so that the total error (**Sum of Squares of Error**, SSE)^[Why the sum of *squared* residuals, and not just their simple sum? This is to prevent very large positive and very large negative deviations canceling each other out in the sum, making it appear as if the total deviation was very small. Squared deviations are always non-negative and therefore do not suffer from this cancellation problem.] for all observations is minimized:

$$
SSE = \sum_{i = 1}^n e_i^2 = \sum_{i = 1}^n (Y_i - \hat{Y}_i)^2
$$ {#eq-sse}

In simple linear regression, we can use simple formulas for the two parameter estimates, $b_0$ and $b_1$, that minimize SSE. However, as soon as we include multiple variables, this becomes significantly more complex. For these complex calculations we instead make use of matrix algebra, which happen behind the scenes in R when using the the function `lm()`.

::: {.callout-note}
Formulas for parameter estimates in simple linear regression are:
\begin{align*}
  b_1 &= \frac{\Sum{n}(X_i - \bar{X})(Y_i - \bar{Y})}{\Sum{n}(X_i - \bar{X})^2}\\
  b_0 &= \bar{Y} - b_1 \cdot \bar{X}
\end{align*}

$b_1$ can also be reformulated as:
$$
\begin{aligned}
 \frac{\Sum{n}(X_i \cdot Y_i) - \frac{\Sum{n}X_i \cdot \Sum{n}Y_i}{n}}{\Sum{n}X_i^2 - \frac{(\Sum{n}X_i)^2}{n}}
\end{aligned}
$$
:::

### Model estimation in R {#sec-model-fit-example}
To fit a linear regression model in R, we use the `lm()` function with the following arguments:

- `formula`: the model structure as a formula object
- `data`: the dataset containing the variables

A formula object is a special format R uses to describe the relationship between variables. Generally, the format is `y ~ x`, where `x` consists of the explanatory variables, for example `shape ~ size`.

:::{.callout-important}
If you have not experienced problems already, using `lm()` requires that we have the correct variable type for all variables as we expect. Make sure that any continuous variable is read as `numeric`!
:::

```{r}
#| code-fold: false

simpleModel <- lm(formula = shape ~ size, data = nux)
```

With `summary()`, we get a detailed output of the model that includes the estimated parameters which are also called *regression coefficients*. When presenting such output, we can use `kable()` or `xtable()` for a cleaner display.

::: {#tip-lm-objects .callout-tip}
To create this clean output of the coefficients, we need to extract a specific part of `summary()` using `coef()`. The documentation for `lm()` provides more information about what can be retrieved from the resulting regression object.

R is an object-oriented programming language, and the `lm()` function returns an object of class "lm", which is a list. It is easy to extract desired parts from that list when needed. There are many functions associated with objects of class "lm":

- `coef()`: Returns regression coefficients  
- `residuals()`: Returns residuals  
- `fitted()`: Returns estimated values ($\hat{Y}$)  
- `summary()`: Returns a summary analysis of the regression model. This function returns an object of class "summary.lm". See `?summary.lm` in the documentation. `coef()` also works on these objects as shown above.  
- `anova()`: Returns the ANOVA table for the model  
- `predict()`: Makes predictions for (new) x-values, i.e., calculates $\hat{Y}$ for given x-values. Can also calculate confidence intervals and prediction intervals for $\hat{Y}$. See `?predict.lm()` for details.  
- `confint()`: Calculates confidence intervals for the regression coefficients  

It is also useful to use `str()` on lm objects. Check `?lm()` under the "Value" section to see the different components of the object. We will make use of a couple of these objects in later chapters.
:::

::: {#fig-reg-summary-bad}
```{r}
summary(simpleModel)
```
Not a very clean output
:::

:::{.callout-note}
`kable()` is a function from the `knitr` package which produces \latex or HTML style formatted tables within R. Alongside `knitr`, we can also customize the design of the table with the help of functions from the `kableExtra` package, for example `kable_styling()` that is used in the below example.

`xtable()` comes from the `xtable` package and is used to create \latex style tables, most often used for non-web resources. 
:::

```{r}
#| tbl-cap: A clean table of the model's estimated parameters
#| tbl-cap-location: top
#| label: tbl-reg-summary-good
#| message: false

require(knitr)
require(kableExtra)

summary(simpleModel) |> 
  coef() |> 
  as_tibble(rownames = NA) |> 
  rownames_to_column() |> 
  rename(
    ` ` = rowname,
    Estimate = Estimate,
    StdError = `Std. Error`,
    `t-value` = `t value`,
    `p-value` = `Pr(>|t|)`
  ) |> 
  kable(
    digits = 4
  ) |> 
  kable_styling("striped")
```

@tbl-reg-summary-good shows the estimated parameters (coefficients). For example, we can see that for each additional size of shell, the shape increases by approximately 0.0041 units on average.

The intercept is only relevant to interpret if the *value range* includes all zeros; if the data covers the region where all explanatory variables take the value 0. In this example, there is no data in that region, which means the intercept value has no meaningful interpretation.

::: {.callout-important}
Even though the interpretation of the intercept may not be meaningful, it **must** be included in the model in order for the OLS estimation to minimize SSE. If the intercept were removed, it would correspond to a line forced to cross the y-axis at $y = 0$, which would result in a model that does not meet our criteria of the "best" model.
:::

## Model diagnostics {#sec-residual-analysis}

After fitting a model based on observations from visualizations and descriptive statistics, we can interpret the estimated relationship between the explanatory variables and the response variable, as we did in @sec-model-fit-example. However, there are two aspects we have yet to  consider:

- we cannot assume these interpretations describe the true relationship since we do not yet know if the model is an appropriate representation,
- these interpretations only describe the collected sample, not the population we want to draw conclusions for.

To assess the appropriateness of the model, we must examine whether it meets the assumptions presented in @sec-model-assumptions through *residual analysis*, and conclusions about the population can be made using statistical inference. We **always** begin by evaluating the model’s appropriateness because the inference methods also rely on these assumptions being met.

Remember @eq-lin-reg? When modeling each individual observation, $\varepsilon_i$ measures the difference between the regression line and the actual observation, which means we can shift the model's assumptions presented in @sec-model-assumptions from $Y|X$ to $\varepsilon$. 
$$
  \varepsilon \overset{\mathrm{iid}}{\sim} N(0, \sigma^2)
$$ {#eq-lin-reg-assumptions}

This reformulation gives us a good starting point to evaluate the suitability of a estimated model.

### Residual analysis {#sec-residual-simple}

Residual analysis involves calculating and visually exploring the residuals from a model against the model assumptions $\varepsilon\overset{iid}{\sim}N(0, \sigma^2)$. Since we estimate error term with residuals, in practice we check if the residuals are independent, normally distributed with mean 0 and constant variance. Residuals can also be used to assess whether the estimated linear model is appropriately structured, i.e. whether the relationship between the response variable and the explanatory variables is linear.

For simplicity, we can extract the residuals as well as the observed and estimated values of the response variable from the estimated model (see @tip-lm-objects).

```{r}
#| code-fold: false

# Create a dataset for visualizations

residualData <- 
  tibble(
    # Extracts the residuals from the model
    residuals = residuals(simpleModel),
    # The observed response values from the data
    y = nux$shape,
    # Extracts the estimated response values from the model
    yHat = fitted(simpleModel)
  )
```

We will visualize these variables in various forms using `ggplot2`, which requires a `data.frame` or `tibble` with data.

#### Normal distribution

We can examine the assumption of normally distributed residuals using a histogram and/or a QQ plot (**q**uantile-**q**uantile plot).

```{r}
#| fig-cap: Distribution of residuals
#| fig-height: 3
#| fig-width: 5 

ggplot(residualData) + 
  aes(x = residuals, y = after_stat(density)) +
  geom_histogram(bins = 10, fill = "steelblue", color = "black") + 
  theme_bw() + 
  labs(x = "Residuals", y = "Density")
```

```{r}
#| fig-cap: Observed residual quantiles compared to theoretical normal quantiles
#| fig-height: 3
#| fig-width: 5 

ggplot(residualData) + 
  # Use standardized residuals
  aes(sample = scale(residuals)) + 
  geom_qq_line() +
  geom_qq(color = "steelblue") +
  theme_bw() + 
  labs(x = "Theoretical Quantiles", y = "Observed Quantiles")
```

In the histogram, we want to see the symmetric, bell-shaped form of the normal distribution centered around 0, which can sometimes be difficult to detect especially with small datasets. The QQ plot shows the observed and theoretical quantiles, where we want the points to follow the drawn line for a "perfect" normal distribution.

:::{.callout-note}
Remember that quantiles is a generalization of a quartile that divides the observed values into equal parts. 

The observed quantiles takes the information from the variable and defines each value as a specific quantile in order from smallest to largest. 

The theoretical quantile assumes that the data is normally distributed with its given mean and standard deviation and for each observation calculates its quantile position within that distribution. We did that "manually" in @sec-continuous-distributions using `pnorm()`.

The QQ plot then maps for each observation the observed quantile against the theoretical and draws a line where we would expect the points to lie if the data were normally distributed. If the points follow close to the line, we can assume that the data is approximately normally distributed.
:::

For this model, we do not see any clear deviations from the expected pattern, which suggests that the residuals are normally distributed. However, we can never be 100% certain that the assumption is met, especially with small datasets.

::: {.callout-important}
We can consider the assumption of normality to be violated if these plots show strong deviations from what we expect. Even when we know a sample is drawn from a normal distribution, the histogram may not always show the shape we are looking for.

```{r}
#| fig-cap: Distribution of a sample from the actual normal distribution
#| fig-height: 5
#| fig-width: 5 
#| echo: false
set.seed(1234)

data <- 
  tibble(
    x = rnorm(30)
  ) 

p1 <- 
  data |> 
  ggplot() + 
  aes(x = x, y = after_stat(density)) +
  geom_histogram(bins = 10, fill = "steelblue", color = "black") + 
  theme_bw() + 
  labs(x = "x", y = "Density")

p2 <- 
  data |> 
  ggplot() + 
  aes(sample = scale(x)) + 
  geom_qq_line() +
  geom_qq(color = "steelblue") +
  theme_bw() + 
  labs(x = "Theoretical Quantiles", y = "Observed Quantiles")

cowplot::plot_grid(p1, p2, ncol = 1)
```

Strong deviations from normality may include, for example, multiple areas of high density:

```{r}
#| fig-cap: Distribution of a sample from another distribution than the normal
#| fig-height: 5
#| fig-width: 5 
#| echo: false
set.seed(1234)

data <- 
  tibble(
    x = runif(30)
  ) 

p1 <- 
  data |> 
  ggplot() + 
  aes(x = x, y = after_stat(density)) +
  geom_histogram(bins = 10, fill = "steelblue", color = "black") + 
  theme_bw() + 
  labs(x = "x", y = "Density")

p2 <- 
  data |> 
  ggplot() + 
  aes(sample = scale(x)) + 
  geom_qq_line() +
  geom_qq(color = "steelblue") +
  theme_bw() + 
  labs(x = "Theoretical Quantiles", y = "Observed Quantiles")

cowplot::plot_grid(p1, p2, ncol = 1)
```

or a highly skewed distribution:

```{r}
#| fig-cap: Distribution of a sample from another distribution than the normal
#| fig-height: 5
#| fig-width: 5
#| echo: false
set.seed(1234)


data <- 
  tibble(
    x = rchisq(30, df = 2)
  ) 

p1 <- 
  data |> 
  ggplot() + 
  aes(x = x, y = after_stat(density)) +
  geom_histogram(bins = 10, fill = "steelblue", color = "black") + 
  theme_bw() + 
  labs(x = "x", y = "Density")

p2 <- 
  data |> 
  ggplot() + 
  aes(sample = scale(x)) + 
  geom_qq_line() +
  geom_qq(color = "steelblue") +
  theme_bw() + 
  labs(x = "Theoretical Quantiles", y = "Observed Quantiles")

cowplot::plot_grid(p1, p2, ncol = 1)
```

These plots suggest that the model is missing an explanatory variable or the data needs to be transformed in some way to meet the assumption.

On top of non-normality, the QQ plot can also indicate if the model deviates from the assumption of linearity. If the QQ plot shows clear symmetrical patterns --- for example if the points curve around the line as in the example below --- it means the model does not meet the assumption of a linear relationship. In this example we are trying to model a sinus function with a linear model.

```{r}
#| echo: false
#| fig-cap: Example of patterns in a QQ plot
#| fig-height: 3
#| fig-width: 5 

data <- 
  tibble(
    x = seq(0, 10, by = 0.1),
    y = sin(x)*2 + rnorm(n = 101, sd = 0.1)
  )

exModel <- lm(y ~ x, data = data)

ggplot(tibble(residuals = resid(exModel))) + 
  # Use standardized residuals
  aes(sample = scale(residuals)) + 
  geom_qq_line() +
  geom_qq(color = "steelblue") +
  theme_bw() + 
  labs(x = "Theoretical Quantiles", y = "Observed Quantiles")


```
:::

#### Constant variance

We can check the assumption of constant variance in the residuals using a scatter plot with residuals on the y-axis and either the estimated values or observed values of the explanatory or response variable on the x-axis. Typically, the estimated values of the response are used so that the x-axis reflects the entire model, but other variables may be useful to visualize to identify potential causes of a violated assumption.

```{r}
#| fig-cap: Spread of residuals against estimated values
#| fig-height: 3
#| fig-width: 5
#| label: fig-ex-eq-var 

ggplot(residualData) + 
  aes(x = yHat, y = residuals) + 
  geom_point(color = "steelblue") + 
  theme_bw() +
  labs(x = "Estimated Values", y = "Residuals") + 
  geom_hline(
    aes(yintercept = 0)
  ) + 
  # Imaginary boundaries
  geom_hline(
    aes(yintercept = -0.05),
    color = "#d9230f",
    linetype = 2
  ) + 
  geom_hline(
    aes(yintercept = 0.05),
    color = "#d9230f",
    linetype = 2
  )
```

To meet the assumption of constant variance, the points at each cross-section of x-values should be evenly spread around the same limits. Think of it as placing an imaginary boundary two parallel lines along the maximum and minimum values of the residuals (the two red dashed lines in @fig-ex-eq-var), and the majority of points should be scattered between them with no big gaps from the point to the boundary. In @fig-ex-eq-var, we see that the variation seem to diminish with larger estimated values as observations fall further from the boundary. Since the dataset is relatively small and only three of the observations ($\hat{Y} > -0.025$) cause the issue we could consider the residuals to meet the assumption of constant variance.

::: {.callout-important}
If the lines covering the maximum and minimum values of the residuals are not parallel, the model does not meet the requirement of constant variance.

```{r}
#| fig-cap: Example of non-constant variance in residuals
#| fig-height: 3
#| fig-width: 5 
#| echo: false

n <- 100

data <- 
  tibble(
    x = seq(1, 10, length.out = n),
    yInc = 3 * x + rnorm(n, sd = 0.5 * x),
    yDec = 3 * x + rnorm(n, sd = 10 / x),
    y2 = x^2 + rnorm(n, sd = 2)
  )

model1 <- lm(yInc ~ x, data = data)
model2 <- lm(yDec ~ x, data = data)
model3 <- lm(y2 ~ x, data = data)

ggplot(tibble(residuals = resid(model1), yHat = fitted(model1))) + 
  aes(x = yHat, y = residuals) + 
  geom_point(color = "steelblue") + 
  theme_bw() +
  labs(x = "Estimated Values", y = "Residuals") + 
  geom_hline(
    aes(yintercept = 0)
  ) + 
  # Imaginary boundaries
  geom_abline(
    slope = 0.35,
    intercept = 0,
    color = "#d9230f",
    linetype = 2
  ) + 
  geom_abline(
    slope = -0.35,
    intercept = 0,
    color = "#d9230f",
    linetype = 2
  ) 

ggplot(tibble(residuals = resid(model2), yHat = fitted(model2))) + 
  aes(x = yHat, y = residuals) + 
  geom_point(color = "steelblue") + 
  theme_bw() +
  labs(x = "Estimated Values", y = "Residuals") + 
  geom_hline(
    aes(yintercept = 0)
  ) + 
  # Imaginary boundaries
  geom_abline(
    slope = -0.15,
    intercept = 6,
    color = "#d9230f",
    linetype = 2
  ) + 
  geom_abline(
    slope = 0.15,
    intercept = -6,
    color = "#d9230f",
    linetype = 2
  )
```

These phenomena usually mean that the entire model or parts of it need to be transformed to meet the assumption of constant variance.

We can also identify problems with linearity in this scatter plot. The figure below shows roughly constant variance in terms of variation across each cross-section of the x-axis, but there is a clear pattern in the residuals. This means the model has not successfully captured the relationship. In this case, it would be appropriate to visualize the residuals against each explanatory variable to identify which ones contribute to the non-linear relationship.

```{r}
#| fig-cap: Pattern in residuals indicating a non-linear relationship
#| fig-height: 3
#| fig-width: 5 
#| echo: false
#| message: false

ggplot(tibble(residuals = resid(model3), yHat = fitted(model3))) + 
  aes(x = yHat, y = residuals) + 
  geom_point(color = "steelblue") + 
  theme_bw() +
  labs(x = "Estimated Values", y = "Residuals") + 
  geom_hline(
    aes(yintercept = 0)
  ) + 
  geom_smooth(aes(y = residuals + 0.45*sd(residuals)), method = "loess", se = FALSE, color = "#d9230f", linetype = 2, linewidth = 0.5) +
  geom_smooth(aes(y = residuals - 0.45*sd(residuals)), method = "loess", se = FALSE, color = "#d9230f", linetype = 2, linewidth = 0.5)
```
:::

#### Independence

Most often we assess this assumption based on the data collection process. Only when we know the data has a time aspect --- such as in time series data or when the same unit has been measured multiple times --- we check that the model has accounted for this dependence properly.

A line plot of residuals in observation order can be used to examine independence, but as mentioned, this visualization is only used in special cases. The line plot should show "randomness," meaning no clear patterns in the residuals.

```{r}
#| fig-cap: Residuals in observation order
#| fig-height: 3
#| fig-width: 5 

ggplot(residualData) + 
  aes(x = 1:nrow(residualData), y = residuals) + 
  geom_line(color = "steelblue") + 
  theme_bw() +
  labs(x = "Obs. index", y = "Residuals") + 
  geom_hline(
    aes(yintercept = 0),
    color = "black"
  )
```

Other examples of data with dependence include:

- We collect data from individuals, but some individuals come from the same family, which may create dependence between them.
- We collect spatial data, such as temperature or rainfall at different locations in Östergötland. It is common for there to be positive correlation (a dependence) between geographically close observations.

### Creating a custom function (`diagnosticPlots`) {#sec-diag-plots}

You will encounter these residual plots many times when conducting regression analyses, so to simplify their generation we can create a custom function that does all of this for us at once. The `cowplot` package provides a function (`plot_grid`) that can combine multiple plots into one.

```{r}
#| fig-cap: Residual plots in a single image
#| fig-height: 3
#| fig-width: 5 
#| label: fig-snail-resid
#| code-fold: false

#' Custom function diagnosticPlots that generate visualizations of residuals
#' @param model A fitted model object of class "lm"
#' @param alpha Defines the opacity of the points (0-1)
#' @param bins  Defines the number of bins in the histogram
#' @param scaleLocation Boolean if a scale location graph should be added

diagnosticPlots <- 
  function(
    model, 
    alpha = 1, 
    bins = 10, 
    scaleLocation = FALSE
  ) {
    if (model |> class() != "lm") {
      stop("model must be an lm object")
    }
    if (alpha < 0 | alpha > 1) {
      stop("alpha must be between 0 and 1")
    }
    if (bins <= 0) {
      stop("bins must be a positive number")
    }
    
    # Summarizes the residuals, observed and fitted values in a tibble
    residualData <- 
      dplyr::tibble(
        residuals = residuals(model),
        # The response variable is the first column in the model's model object
        y = model$model[,1],
        yHat = fitted(model)
      )
    
    # Generates the histogram to assess normality
    p1 <- 
      ggplot2::ggplot(residualData) + 
      ggplot2::aes(x = residuals, y = after_stat(density)) +
      ggplot2::geom_histogram(bins = bins, fill = "steelblue", color = "black") + 
      ggplot2::theme_bw() + 
      ggplot2::labs(x = "Residuals", y = "Density")
    
    # Generates the scatter plot to assess constant variance
    p2 <- 
      ggplot2::ggplot(residualData) + 
      ggplot2::aes(x = yHat, y = residuals) + 
      ggplot2::geom_hline(aes(yintercept = 0)) + 
      ggplot2::geom_point(color = "steelblue", alpha = alpha) + 
      ggplot2::theme_bw() +
      ggplot2::labs(x = "Estimated Values", y = "Residuals")
    
    # Generates the QQ plot to assess normality
    p3 <- 
      ggplot2::ggplot(residualData) + 
      # Use standardized residuals
      ggplot2::aes(sample = scale(residuals)) + 
      ggplot2::geom_qq_line() + 
      ggplot2::geom_qq(color = "steelblue", alpha = alpha) +
      ggplot2::theme_bw() + 
      ggplot2::labs(x= "Theoretical Quantiles", y = "Observed Quantiles")
    
    # If scaleLocation is TRUE, add a scale location plot
    if (scaleLocation) {
      p4 <- 
        ggplot2::ggplot(residualData) + 
        ggplot2::aes(x = yHat, y = sqrt(abs(residuals))) + 
        ggplot2::geom_point(color = "steelblue", alpha = alpha) + 
        ggplot2::theme_bw() +
        ggplot2::labs(x = "Estimated Values", y = expression(sqrt("|Residuals|")))
      
      cowplot::plot_grid(p1, p2, p3, p4, nrow = 2)
      
    } else {
      cowplot::plot_grid(p1, p2, p3, nrow = 2)  
    }
    
}

diagnosticPlots(simpleModel)
```

In summary, @fig-snail-resid shows that the residuals seem to meet the assumptions of normal distribution with mean 0 and constant variance. There are no clear patterns in any plot that suggest otherwise nor that the model fails to capture part of the relationship. A few outliers have been identified, specifically three observations with large estimated values. The conclusion is that the model is a suitable simplification of reality.

### More on diagnostics: Anscombe's quartet {#sec-anscombe}

To further illustrate how diagnostics can be used to visually judge whether the assumptions of linear regression are met, let us take a look at a famous dataset that was designed for precisely this purpose [@Anscombe1973]. The data are built into R (with the name `anscombe`), but are not in the most convenient format:

```{r}
print(anscombe)
```

These are actually four datasets merged into one: `x1` and `y1` are $x$- and $y$-coordinates of the points from the first set, `x2` and `y2` from the second set, and so on. We can use `pivot_longer` to put these data in tidy format:

```{r}
ansLong <- 
  anscombe |>
  pivot_longer(cols = everything(), names_to = c(".value", "set"),
               names_pattern = "(.)(.)")
print(ansLong)
```

We can now visualize each set, along with linear fits:

```{r}
#| message: false
ansLong |>
  ggplot(aes(x = x, y = y, color = set)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  facet_wrap(~ set, nrow = 2, labeller = label_both) +
  theme_bw()
```

The data have been carefully crafted so that the least-squares regression line has an intercept of 3 and a slope of 0.5 for each of the four sets. Furthermore, the p-values are also identical to many decimal places. But this visual representation reveals what would have been much harder to intuit otherwise: that only the first set has a real chance of conforming to the assumptions of linear regression. Performing the regression on just this set and creating diagnostic plots:

```{r}
#| warning: false
#| fig-height: 6
lm(y ~ x, data = filter(ansLong, set == "1")) |> summary()
lm(y ~ x, data = filter(ansLong, set == "1")) |>
  diagnosticPlots(bins = 5)
```

While the number of data points is small, there is otherwise nothing to suggest in these diagnostic plots that there is anything wrong with the regression.

The situation changes for the other three sets. Let us look at set 2:

```{r}
#| warning: false
#| fig-height: 6
lm(y ~ x, data = filter(ansLong, set == "2")) |> summary()
lm(y ~ x, data = filter(ansLong, set == "2")) |>
  diagnosticPlots(bins = 5)
```

Blindly reading off the p-values without considering the diagnostic plots might lead one to take them seriously. This would be wrong however, as the assumptions of the linear regression are clearly not fulfilled. The right diagnostics show that the residuals are not independent, and certainly not homoscedastic (equal variance). The QQ plot and histogram additionally shows that they are not even normally distributed.

In set 3, the trends are driven too much by a single outlier:

```{r}
#| warning: false
#| fig-height: 6
lm(y ~ x, data = filter(ansLong, set == "3")) |> summary()
lm(y ~ x, data = filter(ansLong, set == "3")) |>
  diagnosticPlots(bins = 5)
```

As before, the right diagnostic plots show that the independence of the residuals is violated. Finally, in set 4, the whole regression is based on a single point whose predictor `x` is different from that of the rest:

```{r}
#| warning: false
#| fig-height: 6
lm(y ~ x, data = filter(ansLong, set == "4")) |> summary()
lm(y ~ x, data = filter(ansLong, set == "4")) |>
  diagnosticPlots(bins = 5)
```

Clearly, the assumption that the residual variances are independent of the predictor is heavily violated.

:::{.callout-important}
These examples are there to urge caution when interpreting regression statistics. This problem becomes much more acute when relying on *multiple regression*, where there is more than one predictor variable. Since high-dimensional data cannot be visualized as easily as the datasets above, often the **diagnostic plots are the only way to tell whether the assumptions of regression hold or not.**
:::

## Statistical inference {#sec-inference}

Once we have confirmed that the model is suitable, i.e. that it fulfills its assumptions, we can focus on interpreting the model’s results in relation to the population. In regression analysis, we can perform several types of statistical inference: on the entire model, on groups of parameters, or on individual parameters.

We can start with an *F-test* for the entire model to determine whether at least one slope parameter is significant --- whether the model is worth investigating further --- and then perform individual *t-tests* for each parameter to assess which explanatory variables have a significant effect on the response variable. Since qualitative variables often consist of multiple parameters, these need to be combined to examine the variable’s overall relationship, which we can do with a *partial F-test*.

:::{.callout-note}
In the case of simple linear regression the entire model, groups of parameters and individual parameters are all one and the same test because we only include one explanatory variable (one slope parameter) in the model. Once we start adding more variables to the model these three types of inference differ and we choose the one suitable for the analysis we want to perform.
:::

Before diving into the details of the different tests, we need to present the *ANOVA* table, which is used to break down the variation in the response variable into the model’s components: the explanatory variables and the error term.

### ANOVA {#sec-regression-anova}

**An**alysis **o**f **Va**riance is a collection of methods that calculate the variation of different model components. The goal of a model is to explain the *total variation* in the response variable as effectively as possible. Everything the explanatory variables help to describe is called the *explained variation*, and what the model fails to explain (the remaining error) is the *unexplained variation*.

$$
\underbrace{Y}_\text{total variation} = \underbrace{\beta_0 + \beta_1 \cdot X}_\text{explained variation} + \underbrace{\mathbf{\varepsilon}}_\text{unexplained variation}
$$ {#eq-variation-components}

@eq-variation-components shows that the total variation is the sum of the explained and unexplained variation, which is also reflected in the formulas for each component. Each component is calculated as follows:

$$
  \text{total variation} = SST = \Sum{n}(Y_i - \bar{Y})^2
$$
This corresponds to the numerator for the variance of $Y$. The total variation describes how much variation exist if we were to use the mean of $Y$ as the model.

$$
  \text{unexplained variation} = SSE = \Sum{n}(Y_i - \hat{Y_i})^2
$$

We have previously used SSE as a measure of model error (see @eq-sse), describing the variation of the observed and estimated value.

$$
  \text{explained variation} = SSR = \Sum{n}(\hat{Y_i} - \bar{Y})^2
$$

SSR describes the variation between the model’s estimated values and the mean of $Y$. This can be interpreted as how much more variation the model explains compared to the mean, or simply, how much better the model is at explaining variation in $Y$ than only its mean.

:::{.callout-note}
Adding these all together we can simplify its sum as:
$$
  \Sum{n}(Y_i - \bar{Y})^2 = \Sum{n}(\underbrace{\hat{Y_i}}_\text{positive estimates} - \bar{Y})^2 + \Sum{n}(Y_i - \underbrace{\hat{Y_i}}_\text{negative estimates})^2
$$
where the positive and negative estimates cancel each other out to produce the left hand side.
:::

We can also visualize this relationship in a stacked bar chart. The total height of the bar is SST, while the different segments show how much of the total variation is explained or unexplained in a given model.

```{r}
#| fig-cap: Visualization of the different sources of variation
#| fig-height: 3
#| fig-width: 5 
#| echo: false

anovaTable <- anova(simpleModel) |> 
  rownames_to_column(var = "Source") |> 
  mutate(Source = factor(Source, levels = Source)) |> 
  mutate(
    Source = if_else(Source != "Residuals", "Explained", "Unexplained")
  ) |> 
  group_by(Source) |> 
  summarize(
    `Sum Sq` = sum(`Sum Sq`)
  ) |> 
  ungroup()

ggplot(anovaTable, aes(x = "Total Variation", y = `Sum Sq`, fill = Source)) +
  geom_bar(width = 0.75, stat = "identity", position = "stack", color = "black") +
  theme_minimal() +
  theme(
    panel.grid.major.x = element_blank()
  ) +
  labs(x = "",
       y = "Sum of Squares") +
  scale_fill_manual("Variation", values = c("steelblue", "#d9230f"))
```

#### ANOVA Table {#sec-regression-anova-table}

An ANOVA table is a way to efficiently summarize these components and show additional information, such as *degrees of freedom* ($df$) for each component and *mean squares*.

Degrees of freedom describe how many slope parameters are estimated for each part^[Degrees of freedom actually describe how many pieces of independent information are available for a calculation. Think back to the calculation of sample standard deviation, where the degrees of freedom are $n - 1$, the number of observations minus 1, because we estimate the mean when calculating the standard deviation.] and mean squares show the average variation per degree of freedom, $\frac{SS}{df}$.

```{r}
#| echo: false
#| tbl-cap: Simple ANOVA table
#| tbl-cap-location: top
#| label: tbl-anova-example

anovaTable <- 
  tibble(
    Source = c("Model (Regression)", "Error", "Total"),
    DF = c("$df_R = k$", "$df_E = n - (k + 1)$", "$df_T = n - 1$"),
    `Sum of Squares` = c(
      "$SSR$",
      "$SSE$",
      "$SST$"
    ),
    `Mean Square` = c("$MSR = \\frac{SSR}{df_R}$", "$MSE = \\frac{SSE}{df_E}$", "")
  )

kable(anovaTable, format = "markdown", booktabs = TRUE, escape = FALSE, col.names = c("Source", "DF", "Sum of Squares", "Mean Square"), parse = TRUE) 
```

A simple ANOVA table like @tbl-anova-example shows only the three main components, but different software may display other breakdowns by default. In a multiple linear regression model, it is common to further divide the explained variation, for example into *sequential sums of squares* (@sec-seq-ss).

Using the different sources of variation, we can calculate tests for the whole model or parts of it using various F-tests, while the individual parameter estimates and their associated standard errors can be used in tests for individual slope parameters.

### F-test for the Model

In linear regression, an F-test for the entire model is a good starting point to determine whether at least one slope parameter is significant. We examine the hypotheses:

\begin{align*}
H_0&: \beta_1 = \beta_2 = \beta_3 = \cdots = \beta_k = 0\\
H_a&: \text{At least one of } \beta_j \text{ in } H_0 \text{ is different from } 0
\end{align*}

If at least one slope parameter is significant, it means there is at least one variable that contributes some explained variation, hence the model is better than using just $\bar{Y}$. The test statistic examines the relationship between explained and unexplained variation via their mean squares:

$$
F_{test} = \frac{SSR / k}{SSE / (n - (k+1))} = \frac{MSR}{MSE} 
$$

The test statistic follows an *F-distribution* governed by two degrees of freedom: $df_1$ from the numerator and $df_2$ from the denominator in the calculation, i.e., the model and error degrees of freedom respectively. If $H_0$ is true, the test statistic will be 0, while if $H_a$ is true, the test statistic will be a large positive number. Since both mean squares are positive, the ratio will always be positive, and we can reject $H_0$ if the test statistic is sufficiently far from 0.

```{r}
#| fig-cap: Different F-distributions and their degrees of freedom
#| fig-height: 4
#| fig-width: 7
#| echo: false

# Create a function to generate different F-distributions
generateFdistribution <- function(df1, df2, n = 1000) {
  x <- seq(0, 5, length.out = n)  
  y <- df(x, df1, df2)  
  tibble(x = x, y = y, df1 = df1, df2 = df2)  
}

# Create a list of different degrees of freedom
dfs <- list(c(5, 30), c(10, 100), c(20, 50), c(30, 300))

# Generate data
Fdistributions <- dfs |>
  purrr::map_df(~generateFdistribution(.x[1], .x[2]), .id = "Distribution") |>
  mutate(Distribution = paste0("df1 = ", df1, ", df2 = ", df2))

# Plot the F-distributions using ggplot2
ggplot(Fdistributions) + 
  aes(x = x, y = y, color = Distribution) +
  geom_line(linewidth = 1) +
  labs(
    x = "F-value",
    y = "Density",
    color = "Degrees of Freedom"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    legend.position = "right",
    legend.title = element_text(face = "bold")
  ) +
  scale_color_manual(values = c("steelblue", "#d9230f", "goldenrod", "black"))
```

Using the output from `summary()`, we can find the result of the F-test at the bottom row.

```{r}

summary(simpleModel)

```

`F-statistic: 5.278 on 1 and 36 DF,  p-value: 0.02752` the p-value can be interpreted as we would any hypothesis test, comparing it against the chosen significance level (**chosen before estimating the model!**). Since the p-value is less than 5 percent, we can reject $H_0$ and conclude that at least one of the variables is associated with the response variable.^[If we had made a different decision (not rejecting the null hypothesis), it would not be relevant to continue with the analysis, or at least the remaining analysis should focus on investigating why a linear regression model we expect to show a relationship based on pairwise scatter plots does not show it collectively.]

### t-test for individual parameters

Even though the F-test for the model and the individual t-test for the slope is the same test in a simple linear regression, this is not the case for a multiple regression model. As such it is not appropriate to look at the ANOVA table when we want to examine individual parameters. Instead, we should use the coefficient table from the `summary()` output.

Formally, the hypotheses are:

$$
\begin{aligned}
H_0 &: \beta_j = 0\\
H_a &: \beta_j \ne 0
\end{aligned}
$$

where $j$ is one of the slope parameters in the model.

The test statistic is calculated from the estimated slope parameter and its standard error:

$$
t_{test} = \frac{b_j - 0}{s_{b_j}}
$$

The test statistic follows a t-distribution under $H_0$ with $n - (k + 1)$ degrees of freedom.

In R, t-tests are included in the coefficient table, which we can extract from the `summary()` object using `coef()`.

```{r}
#| tbl-cap: Coefficient table for a model with associated t-tests for individual parameters
#| tbl-cap-location: top
#| label: tbl-coef-example

summary(simpleModel) |> 
  coef() |> 
  round(4) |> 
  kable(format = "markdown",
        col.names = 
          c(
            "Variable", 
            "Estimate", 
            "Std. Error", 
            "t-value", 
            "p-value"
          ), 
        parse = TRUE) |> 
  kable_styling("striped")
```

In @tbl-coef-example, we see that the p-value for the slope is less than 5 percent which means we can reject $H_0$ at the 5% significance level, meaning the variable has a significant effect on the response variable.

## Simple evaluation metrics {#sec-r-square}

Just because a model is appropriate, meets model assumptions, and contains significant parameters does not mean it is the best model that can explain the response or even that it is a good one. With the help of various evaluation metrics, we can get an overview of how good the model is.

The *coefficient of determination* ($R^2$) describes what proportion of the total variation is explained by the model’s explanatory variables. With this description, we can calculate $R^2$ as:

$$
R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}
$$
Given that this is a proportion of the total variation its values range from 0 (model is useless at explaining the response) to 1 (model perfectly explains the response) and is interpreted in percent.

The coefficient of determination is called *multiple R-squared* in R and can be found at the second to last line in the `summary()` output. `Multiple R-squared:  0.1279` is interpreted as approximately 13 percent of the total variation in snail shape is explained by the snail size, which is a very low value.

:::{.callout-important}
Good models typically have an $R^2$ value above 0.5, meaning that at least half of the total variation is explained by the model. However, this is not a strict rule and depends on the context of the analysis. In some fields, such as social sciences, an $R^2$ value of 0.3 or even lower may be considered acceptable.

Whenever we get a low coefficient, we should consider the model structure and whether it is appropriate for the data. We can also consider adding more explanatory variables to the model, transforming existing variables, or using a different type of model altogether.

Sometimes there is just too much variation in the data that cannot be explained by any measured variable, and the best model we can produce is still a poor one. In these cases, we should be careful not to add too many variables or transformations that do not significantly improve the model's predictive power.
:::

## Predictions {#sec-predictions}

We can also use the estimated model to *predict* new values of the response variable for new observations. From the estimated regression line, we substitute each variable with the observation’s actual value and get a simple sum that describes the response variable’s value on the line.

Predictions involve estimating the value of $Y$ given observed values of $X$ using the estimated regression line. These predictions will fall along the line, which further motivates that the model needs to be appropriate and good. We don’t want a prediction in one area to be more accurate than a prediction in another, or for the regression line to generally be a poor representation of the response variable. The model is based on a specific *domain*, the observed values of $X$, and predictions should be made within this domain. 

There are certain cases, such as in time series analysis, where predictions are made outside the domain, but there is a time dependency that enables these *extrapolations*. In "normal" regression, we should avoid extrapolating the regression line outside the domain.

### Mean of Y for given $X$

If we are interested in the average value of the response variable for all new observations with given values of $X$, we can estimate $\mu_{Y|X_0}$ where $X_0$ contains values for the new observation. We use the estimated regression model and calculate a point prediction of the response variable with the help of `predict()` which substitutes the values of $X_0$ into the regression equation:

```{r}

x0 <- 
  tibble(
    size = 23
  )

predict(
  simpleModel, 
  newdata = x0,
  interval = "confidence"
  )

```

The interval estimate for a mean becomes a confidence interval because we are making inference of **all** possible new observations with the shell size of 23 units. 

### Individual prediction of Y for given $X$

If we are instead interested in a single value of $Y$ with given values of $X$, we can estimate $Y_{X_0}$. Since the prediction uses the same estimated model the point estimate will be the same as the mean prediction, but the interval estimate will be wider because it accounts for the additional uncertainty of predicting a single value rather than the mean of all possible values. We can use `predict()` with the `interval` argument set to `"prediction"`:

```{r}
predict(
  simpleModel, 
  newdata = x0,
  interval = "prediction"
  )
```

The interval estimate of an individual observation becomes a prediction interval because we only account for one new observation with the shell size of 23 units.

## Exercises {#sec-exercises-regression}

The file [`plant_growth_rate.csv`](https://raw.githubusercontent.com/dysordys/intro-R-and-stats/main/data/plant_growth_rate.zip) contains individual plant growth data (mm/week), as a function of soil moisture content. 

1. Investigate the type and scale of the variables.
2. Summarize descriptive statistics for each variable.
3. Visualize the distribution of each variable.
4. Create a scatter plot showing the relationship between the two variables in the sample. Interpret it based on the four key pieces of information a scatter plot can provide.
5. Summarize your observations and justify whether or not you think a simple linear regression model is suitable.
6. Fit a linear regression model using `lm()` and interpret the regression coefficients.
7. Create diagnostic plots to check whether the assumptions of the model are satisfied.
8. Perform an F-test for the model and interpret the result. Do the same for a t-test for the slope parameter.
9. Summarize your findings along with the coefficient of determination and determine whether or not the model is suitable for predicting plant growth based on soil moisture content.

It is difficult to measure the height of a tree. By contrast, the diameter at breast height (DBH) is easy to measure. Can one infer the height of a tree by measuring its DBH? The built-in dataset `trees` contains DBH data (labeled `Girth`), as well as measured height and timber volume of 31 felled black cherry trees. You can ignore timber volume, and focus instead on how well DBH predicts tree height. 

1. Repeat steps 1-9 from the previous exercise using the `trees` dataset.
2. Predict the height of a tree with a DBH of 20 cm, and interpret a confidence interval for the mean height of all trees with that DBH.

